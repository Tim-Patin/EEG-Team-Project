{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "Fp1     1\n",
       "Fp2     2\n",
       "F3      3\n",
       "F4      4\n",
       "C3      5\n",
       "       ..\n",
       "PO8    63\n",
       "Fpz    64\n",
       "CPz    65\n",
       "POz    66\n",
       "Oz     67\n",
       "Name: urchan, Length: 66, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv(\"Channels.csv\"))\n",
    "\n",
    "channels_dict=df.set_index('labels')['urchan']\n",
    "channels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cupy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using CUDA device 0\n",
      "Enabling CUDA with 10.83 GB available memory\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"data\")\n",
    "\n",
    "data_all = []\n",
    "labels_all = []\n",
    "\n",
    "os.environ['MNE_USE_CUDA'] = 'true' \n",
    "mne.utils.set_config('MNE_USE_CUDA', 'true') \n",
    "mne.cuda.init_cuda(ignore_config=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18080\\2397413495.py:55: RuntimeWarning: Using n_components=15 (resulting in n_components_=15) may lead to an unstable mixing matrix estimation because the ratio between the largest (59) and smallest (8.6e-06) variances is too large (> 1e6); consider setting n_components=0.999999 or an integer <= 8\n",
      "  ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 2.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.5s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.4s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.5s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.4s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.5s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.5s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.4s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.4s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.6s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 2.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.6s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.5s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.6s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.6s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.8s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.4s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Using CUDA for FFT resampling\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "Final dataset shape: (1937, 2500, 66) (trials, time points, channels)\n",
      "Labels shape: (1937,)\n"
     ]
    }
   ],
   "source": [
    "# MIN_POINTS = 100  #Minimum number of time points per trial\n",
    "# TARGET_SAMPLING_RATE = 250\n",
    "# window_size=2500\n",
    "# overlap=0.5\n",
    "# for file_path in data_dir.glob(\"*.set\"):\n",
    "#         file_name = file_path.stem\n",
    "#         if \"PREP\" not in file_name:\n",
    "#             number = int(file_name.split('_')[0])\n",
    "#             label = 1 if number % 2 == 1 else 0\n",
    "            \n",
    "#             # Load and preprocess the raw data\n",
    "#             raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "#             raw.set_eeg_reference('average', projection=True)\n",
    "#             raw.resample(TARGET_SAMPLING_RATE, npad=\"auto\")\n",
    "#             raw.filter(1., 30., fir_design='firwin', n_jobs='cuda')\n",
    "            \n",
    "#             ica = mne.preprocessing.ICA(n_components=15, random_state=22, max_iter=1000, method='picard')\n",
    "#             ica.fit(raw)\n",
    "#             raw = ica.apply(raw)\n",
    "            \n",
    "#             data = raw.get_data().T  # (n_times, n_channels)\n",
    "            \n",
    "#             if data.shape[0] >= MIN_POINTS:\n",
    "#                 # Create overlapping windows\n",
    "#                 stride = int(window_size * (1 - overlap))\n",
    "#                 for start in range(0, data.shape[0] - window_size + 1, stride):\n",
    "#                     window = data[start:start + window_size, :]\n",
    "#                     data_all.append(window)\n",
    "#                     labels_all.append(label)\n",
    "    \n",
    "# data_all = np.stack(data_all)  # (n_windows, window_size, n_channels)\n",
    "# labels_all = np.array(labels_all)\n",
    "\n",
    "# # Check the shapes of the final dataset\n",
    "# print(\"Final dataset shape:\",data_all.shape,\"(trials, time points, channels)\")\n",
    "# print(\"Labels shape:\",labels_all.shape)\n",
    "\n",
    "TARGET_SAMPLING_RATE = 250\n",
    "window_size = 2500\n",
    "overlap = 0.5 #50% overlap\n",
    "\n",
    "for file_path in data_dir.glob(\"*.set\"):\n",
    "        file_name = file_path.stem\n",
    "        if \"PREP\" not in file_name:\n",
    "            number = int(file_name.split('_')[0])\n",
    "            label = 1 if number % 2 == 1 else 0\n",
    "            \n",
    "            # Load and preprocess the raw data\n",
    "            raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "            raw.set_eeg_reference('average', projection=True)\n",
    "            raw.resample(TARGET_SAMPLING_RATE, npad=\"auto\",method=\"fft\",n_jobs='cuda')\n",
    "            raw.filter(1., 30., fir_design='firwin', n_jobs='cuda')\n",
    "            \n",
    "            ica = mne.preprocessing.ICA(n_components=15, random_state=22, max_iter=1000, method='picard')\n",
    "            ica.fit(raw)\n",
    "            raw = ica.apply(raw)\n",
    "            \n",
    "            data = raw.get_data().T  # (n_times, n_channels)\n",
    "            \n",
    "            #Only create windows if there's enough data for at least one window\n",
    "            if data.shape[0] >= window_size:\n",
    "                stride = int(window_size * (1 - overlap))\n",
    "                for start in range(0, data.shape[0] - window_size + 1, stride):\n",
    "                    window = data[start:start + window_size, :]\n",
    "                    data_all.append(window)\n",
    "                    labels_all.append(label)\n",
    "            else:\n",
    "                #Skip other trials\n",
    "                print(\"Skipping trial:\",file_name,\" - too short:\",data.shape[0],\" points\")\n",
    "\n",
    "data_all = np.stack(data_all)\n",
    "labels_all = np.array(labels_all)\n",
    "\n",
    "print(\"Final dataset shape:\",data_all.shape,\"(trials, time points, channels)\")\n",
    "print(\"Labels shape:\",labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_all, labels_all, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "X = data_all.reshape(data_all.shape[0], -1)\n",
    "    \n",
    "    # Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_all, test_size=0.2, \n",
    "                                                       random_state=42, stratify=labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, criterion, print_freq=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_index, (data, target) in enumerate(train_dataloader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_index % print_freq == 0:\n",
    "            print(f\"Batch {batch_index}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return train_loss / len(train_dataloader)\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (data, target) in enumerate(test_dataloader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "def train_model(model, train_dataloader, test_dataloader, optimizer, criterion, num_epochs):\n",
    "    loss_results = []\n",
    "    accuracy_results = []\n",
    "    train_loss_results=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "        test_loss, test_accuracy = test(model, test_dataloader, criterion)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | Train loss: {train_loss:.5f} |\",\n",
    "            f\"Test loss: {test_loss:.5f} | Test accuracy: {test_accuracy:.5f}\"\n",
    "        )\n",
    "        train_loss_results.append([epoch + 1, train_loss])\n",
    "        accuracy_results.append([epoch + 1, test_accuracy])\n",
    "        loss_results.append([epoch + 1, test_loss])\n",
    "    return train_loss_results,accuracy_results, loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.7808\n",
      "Batch 10, Loss: 0.6890\n",
      "Batch 20, Loss: 0.7364\n",
      "Batch 30, Loss: 0.5510\n",
      "Batch 40, Loss: 0.6240\n",
      "Batch 50, Loss: 0.6301\n",
      "Batch 60, Loss: 0.5547\n",
      "Batch 70, Loss: 0.7919\n",
      "Batch 80, Loss: 0.6775\n",
      "Batch 90, Loss: 0.5474\n",
      "Epoch: 1 | Train loss: 0.74208 | Test loss: 0.37336 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.6317\n",
      "Batch 10, Loss: 0.6203\n",
      "Batch 20, Loss: 0.6162\n",
      "Batch 30, Loss: 0.3599\n",
      "Batch 40, Loss: 0.6192\n",
      "Batch 50, Loss: 0.4959\n",
      "Batch 60, Loss: 0.6048\n",
      "Batch 70, Loss: 0.3501\n",
      "Batch 80, Loss: 0.5822\n",
      "Batch 90, Loss: 0.4576\n",
      "Epoch: 2 | Train loss: 0.59366 | Test loss: 0.71642 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.5527\n",
      "Batch 10, Loss: 0.5275\n",
      "Batch 20, Loss: 0.5061\n",
      "Batch 30, Loss: 0.4691\n",
      "Batch 40, Loss: 0.5096\n",
      "Batch 50, Loss: 0.6592\n",
      "Batch 60, Loss: 0.4286\n",
      "Batch 70, Loss: 0.4728\n",
      "Batch 80, Loss: 0.4303\n",
      "Batch 90, Loss: 0.5857\n",
      "Epoch: 3 | Train loss: 0.51983 | Test loss: 1.30517 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3585\n",
      "Batch 10, Loss: 0.4353\n",
      "Batch 20, Loss: 0.4193\n",
      "Batch 30, Loss: 0.6615\n",
      "Batch 40, Loss: 0.5333\n",
      "Batch 50, Loss: 0.5427\n",
      "Batch 60, Loss: 0.5646\n",
      "Batch 70, Loss: 0.5635\n",
      "Batch 80, Loss: 0.5422\n",
      "Batch 90, Loss: 0.5837\n",
      "Epoch: 4 | Train loss: 0.51627 | Test loss: 2.05718 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.5022\n",
      "Batch 10, Loss: 0.4518\n",
      "Batch 20, Loss: 0.4379\n",
      "Batch 30, Loss: 0.4595\n",
      "Batch 40, Loss: 0.5221\n",
      "Batch 50, Loss: 0.3264\n",
      "Batch 60, Loss: 0.3778\n",
      "Batch 70, Loss: 0.6040\n",
      "Batch 80, Loss: 0.4742\n",
      "Batch 90, Loss: 0.4175\n",
      "Epoch: 5 | Train loss: 0.45117 | Test loss: 0.04764 | Test accuracy: 0.54639\n",
      "Batch 0, Loss: 0.3042\n",
      "Batch 10, Loss: 0.4171\n",
      "Batch 20, Loss: 0.3785\n",
      "Batch 30, Loss: 0.3975\n",
      "Batch 40, Loss: 0.3754\n",
      "Batch 50, Loss: 0.2232\n",
      "Batch 60, Loss: 0.8138\n",
      "Batch 70, Loss: 0.3572\n",
      "Batch 80, Loss: 0.5045\n",
      "Batch 90, Loss: 0.3708\n",
      "Epoch: 6 | Train loss: 0.40249 | Test loss: 3.57590 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.4709\n",
      "Batch 10, Loss: 0.2717\n",
      "Batch 20, Loss: 0.3151\n",
      "Batch 30, Loss: 0.4176\n",
      "Batch 40, Loss: 0.2157\n",
      "Batch 50, Loss: 0.4658\n",
      "Batch 60, Loss: 0.3622\n",
      "Batch 70, Loss: 0.4500\n",
      "Batch 80, Loss: 0.4518\n",
      "Batch 90, Loss: 0.7964\n",
      "Epoch: 7 | Train loss: 0.38387 | Test loss: 1.11267 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.7681\n",
      "Batch 10, Loss: 0.5593\n",
      "Batch 20, Loss: 0.5152\n",
      "Batch 30, Loss: 0.2785\n",
      "Batch 40, Loss: 0.3896\n",
      "Batch 50, Loss: 0.2030\n",
      "Batch 60, Loss: 0.5056\n",
      "Batch 70, Loss: 0.2957\n",
      "Batch 80, Loss: 0.4012\n",
      "Batch 90, Loss: 0.4890\n",
      "Epoch: 8 | Train loss: 0.38920 | Test loss: 0.05875 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.3018\n",
      "Batch 10, Loss: 0.2202\n",
      "Batch 20, Loss: 0.6861\n",
      "Batch 30, Loss: 0.4423\n",
      "Batch 40, Loss: 0.1835\n",
      "Batch 50, Loss: 0.5421\n",
      "Batch 60, Loss: 0.3403\n",
      "Batch 70, Loss: 0.1388\n",
      "Batch 80, Loss: 0.4179\n",
      "Batch 90, Loss: 0.2882\n",
      "Epoch: 9 | Train loss: 0.34917 | Test loss: 9.35109 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1228\n",
      "Batch 10, Loss: 0.3842\n",
      "Batch 20, Loss: 0.3011\n",
      "Batch 30, Loss: 0.3928\n",
      "Batch 40, Loss: 0.3134\n",
      "Batch 50, Loss: 0.2732\n",
      "Batch 60, Loss: 0.1436\n",
      "Batch 70, Loss: 0.4690\n",
      "Batch 80, Loss: 0.5136\n",
      "Batch 90, Loss: 0.2903\n",
      "Epoch: 10 | Train loss: 0.29885 | Test loss: 3.43167 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3831\n",
      "Batch 10, Loss: 0.2734\n",
      "Batch 20, Loss: 0.3897\n",
      "Batch 30, Loss: 0.2588\n",
      "Batch 40, Loss: 0.2102\n",
      "Batch 50, Loss: 0.3668\n",
      "Batch 60, Loss: 0.3122\n",
      "Batch 70, Loss: 0.3235\n",
      "Batch 80, Loss: 0.3113\n",
      "Batch 90, Loss: 0.2857\n",
      "Epoch: 11 | Train loss: 0.30054 | Test loss: 1.89110 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1780\n",
      "Batch 10, Loss: 0.1189\n",
      "Batch 20, Loss: 0.3375\n",
      "Batch 30, Loss: 0.2049\n",
      "Batch 40, Loss: 0.2966\n",
      "Batch 50, Loss: 0.1418\n",
      "Batch 60, Loss: 0.4495\n",
      "Batch 70, Loss: 0.5776\n",
      "Batch 80, Loss: 0.1228\n",
      "Batch 90, Loss: 0.4892\n",
      "Epoch: 12 | Train loss: 0.28671 | Test loss: 5.95897 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2049\n",
      "Batch 10, Loss: 0.4654\n",
      "Batch 20, Loss: 0.1926\n",
      "Batch 30, Loss: 0.4163\n",
      "Batch 40, Loss: 0.5242\n",
      "Batch 50, Loss: 0.2455\n",
      "Batch 60, Loss: 0.5167\n",
      "Batch 70, Loss: 0.1620\n",
      "Batch 80, Loss: 0.0234\n",
      "Batch 90, Loss: 0.3589\n",
      "Epoch: 13 | Train loss: 0.26819 | Test loss: 2.80126 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2012\n",
      "Batch 10, Loss: 0.2755\n",
      "Batch 20, Loss: 0.3383\n",
      "Batch 30, Loss: 0.1749\n",
      "Batch 40, Loss: 0.2049\n",
      "Batch 50, Loss: 0.0824\n",
      "Batch 60, Loss: 0.3734\n",
      "Batch 70, Loss: 0.4684\n",
      "Batch 80, Loss: 0.1897\n",
      "Batch 90, Loss: 0.4188\n",
      "Epoch: 14 | Train loss: 0.28746 | Test loss: 3.91667 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3435\n",
      "Batch 10, Loss: 0.0721\n",
      "Batch 20, Loss: 0.1920\n",
      "Batch 30, Loss: 0.2710\n",
      "Batch 40, Loss: 0.3252\n",
      "Batch 50, Loss: 0.1429\n",
      "Batch 60, Loss: 0.2238\n",
      "Batch 70, Loss: 0.3201\n",
      "Batch 80, Loss: 0.1359\n",
      "Batch 90, Loss: 0.2228\n",
      "Epoch: 15 | Train loss: 0.27815 | Test loss: 4.70684 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1703\n",
      "Batch 10, Loss: 0.3106\n",
      "Batch 20, Loss: 0.4560\n",
      "Batch 30, Loss: 0.3613\n",
      "Batch 40, Loss: 0.4418\n",
      "Batch 50, Loss: 0.1571\n",
      "Batch 60, Loss: 0.0468\n",
      "Batch 70, Loss: 0.2314\n",
      "Batch 80, Loss: 0.2667\n",
      "Batch 90, Loss: 0.1069\n",
      "Epoch: 16 | Train loss: 0.26889 | Test loss: 10.82288 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1339\n",
      "Batch 10, Loss: 0.2918\n",
      "Batch 20, Loss: 0.2606\n",
      "Batch 30, Loss: 0.4656\n",
      "Batch 40, Loss: 0.1569\n",
      "Batch 50, Loss: 0.0938\n",
      "Batch 60, Loss: 0.3610\n",
      "Batch 70, Loss: 0.1128\n",
      "Batch 80, Loss: 0.1274\n",
      "Batch 90, Loss: 0.2994\n",
      "Epoch: 17 | Train loss: 0.28567 | Test loss: 7.06005 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2945\n",
      "Batch 10, Loss: 0.3344\n",
      "Batch 20, Loss: 0.4350\n",
      "Batch 30, Loss: 0.1810\n",
      "Batch 40, Loss: 0.0378\n",
      "Batch 50, Loss: 0.2894\n",
      "Batch 60, Loss: 0.2725\n",
      "Batch 70, Loss: 0.0904\n",
      "Batch 80, Loss: 0.5495\n",
      "Batch 90, Loss: 0.1423\n",
      "Epoch: 18 | Train loss: 0.24996 | Test loss: 12.06216 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.7201\n",
      "Batch 10, Loss: 0.3951\n",
      "Batch 20, Loss: 0.0635\n",
      "Batch 30, Loss: 0.3264\n",
      "Batch 40, Loss: 0.0184\n",
      "Batch 50, Loss: 0.2001\n",
      "Batch 60, Loss: 0.0519\n",
      "Batch 70, Loss: 0.5984\n",
      "Batch 80, Loss: 0.1280\n",
      "Batch 90, Loss: 0.4910\n",
      "Epoch: 19 | Train loss: 0.24656 | Test loss: 4.45330 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3893\n",
      "Batch 10, Loss: 0.2040\n",
      "Batch 20, Loss: 0.1369\n",
      "Batch 30, Loss: 0.0620\n",
      "Batch 40, Loss: 0.2530\n",
      "Batch 50, Loss: 0.0269\n",
      "Batch 60, Loss: 0.0943\n",
      "Batch 70, Loss: 0.1366\n",
      "Batch 80, Loss: 0.1188\n",
      "Batch 90, Loss: 0.5014\n",
      "Epoch: 20 | Train loss: 0.20276 | Test loss: 0.42550 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 1.8608\n",
      "Batch 10, Loss: 0.2150\n",
      "Batch 20, Loss: 0.1874\n",
      "Batch 30, Loss: 0.1378\n",
      "Batch 40, Loss: 0.1499\n",
      "Batch 50, Loss: 0.1368\n",
      "Batch 60, Loss: 0.1997\n",
      "Batch 70, Loss: 0.4090\n",
      "Batch 80, Loss: 0.0493\n",
      "Batch 90, Loss: 0.4418\n",
      "Epoch: 21 | Train loss: 0.23971 | Test loss: 14.62955 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0506\n",
      "Batch 10, Loss: 0.1883\n",
      "Batch 20, Loss: 0.0342\n",
      "Batch 30, Loss: 0.2940\n",
      "Batch 40, Loss: 0.2054\n",
      "Batch 50, Loss: 0.0814\n",
      "Batch 60, Loss: 0.1769\n",
      "Batch 70, Loss: 0.4334\n",
      "Batch 80, Loss: 0.1985\n",
      "Batch 90, Loss: 0.3473\n",
      "Epoch: 22 | Train loss: 0.21374 | Test loss: 0.08889 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0922\n",
      "Batch 10, Loss: 0.1275\n",
      "Batch 20, Loss: 0.3667\n",
      "Batch 30, Loss: 0.3379\n",
      "Batch 40, Loss: 0.3141\n",
      "Batch 50, Loss: 0.4897\n",
      "Batch 60, Loss: 0.7823\n",
      "Batch 70, Loss: 0.3193\n",
      "Batch 80, Loss: 0.1234\n",
      "Batch 90, Loss: 0.0841\n",
      "Epoch: 23 | Train loss: 0.23624 | Test loss: 0.04496 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.1490\n",
      "Batch 10, Loss: 0.0135\n",
      "Batch 20, Loss: 0.2214\n",
      "Batch 30, Loss: 0.2034\n",
      "Batch 40, Loss: 0.1823\n",
      "Batch 50, Loss: 0.5828\n",
      "Batch 60, Loss: 0.0882\n",
      "Batch 70, Loss: 0.5383\n",
      "Batch 80, Loss: 0.0784\n",
      "Batch 90, Loss: 0.1598\n",
      "Epoch: 24 | Train loss: 0.20812 | Test loss: 1.69184 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1431\n",
      "Batch 10, Loss: 0.0470\n",
      "Batch 20, Loss: 0.0381\n",
      "Batch 30, Loss: 0.3566\n",
      "Batch 40, Loss: 0.2237\n",
      "Batch 50, Loss: 0.1682\n",
      "Batch 60, Loss: 0.0344\n",
      "Batch 70, Loss: 0.0736\n",
      "Batch 80, Loss: 0.1688\n",
      "Batch 90, Loss: 0.2378\n",
      "Epoch: 25 | Train loss: 0.20271 | Test loss: 1.14372 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.4132\n",
      "Batch 10, Loss: 0.3246\n",
      "Batch 20, Loss: 0.0657\n",
      "Batch 30, Loss: 0.1365\n",
      "Batch 40, Loss: 0.0462\n",
      "Batch 50, Loss: 0.0376\n",
      "Batch 60, Loss: 0.0433\n",
      "Batch 70, Loss: 0.4888\n",
      "Batch 80, Loss: 0.3574\n",
      "Batch 90, Loss: 0.0266\n",
      "Epoch: 26 | Train loss: 0.19645 | Test loss: 0.89796 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.4707\n",
      "Batch 10, Loss: 0.0692\n",
      "Batch 20, Loss: 0.0410\n",
      "Batch 30, Loss: 0.1219\n",
      "Batch 40, Loss: 0.0564\n",
      "Batch 50, Loss: 0.2601\n",
      "Batch 60, Loss: 0.0955\n",
      "Batch 70, Loss: 0.0965\n",
      "Batch 80, Loss: 0.3036\n",
      "Batch 90, Loss: 0.0748\n",
      "Epoch: 27 | Train loss: 0.20751 | Test loss: 7.41892 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1957\n",
      "Batch 10, Loss: 0.0395\n",
      "Batch 20, Loss: 0.1135\n",
      "Batch 30, Loss: 0.1213\n",
      "Batch 40, Loss: 0.3247\n",
      "Batch 50, Loss: 0.4004\n",
      "Batch 60, Loss: 0.0646\n",
      "Batch 70, Loss: 0.0675\n",
      "Batch 80, Loss: 0.0762\n",
      "Batch 90, Loss: 0.4374\n",
      "Epoch: 28 | Train loss: 0.19865 | Test loss: 6.18008 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.4306\n",
      "Batch 10, Loss: 0.6597\n",
      "Batch 20, Loss: 0.1128\n",
      "Batch 30, Loss: 0.3123\n",
      "Batch 40, Loss: 0.0382\n",
      "Batch 50, Loss: 0.0300\n",
      "Batch 60, Loss: 0.0784\n",
      "Batch 70, Loss: 0.1610\n",
      "Batch 80, Loss: 0.2125\n",
      "Batch 90, Loss: 0.3261\n",
      "Epoch: 29 | Train loss: 0.16583 | Test loss: 9.08575 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1255\n",
      "Batch 10, Loss: 0.3273\n",
      "Batch 20, Loss: 0.2264\n",
      "Batch 30, Loss: 0.3144\n",
      "Batch 40, Loss: 0.1873\n",
      "Batch 50, Loss: 0.4801\n",
      "Batch 60, Loss: 0.3263\n",
      "Batch 70, Loss: 0.0488\n",
      "Batch 80, Loss: 0.1140\n",
      "Batch 90, Loss: 0.2450\n",
      "Epoch: 30 | Train loss: 0.20866 | Test loss: 3.25433 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1104\n",
      "Batch 10, Loss: 0.0886\n",
      "Batch 20, Loss: 0.0688\n",
      "Batch 30, Loss: 0.1061\n",
      "Batch 40, Loss: 0.0416\n",
      "Batch 50, Loss: 0.0507\n",
      "Batch 60, Loss: 0.1163\n",
      "Batch 70, Loss: 0.0689\n",
      "Batch 80, Loss: 0.0465\n",
      "Batch 90, Loss: 0.0799\n",
      "Epoch: 31 | Train loss: 0.15175 | Test loss: 6.97708 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0122\n",
      "Batch 10, Loss: 0.1694\n",
      "Batch 20, Loss: 0.2047\n",
      "Batch 30, Loss: 0.3389\n",
      "Batch 40, Loss: 0.2817\n",
      "Batch 50, Loss: 0.0225\n",
      "Batch 60, Loss: 0.0351\n",
      "Batch 70, Loss: 0.2988\n",
      "Batch 80, Loss: 0.1299\n",
      "Batch 90, Loss: 0.1306\n",
      "Epoch: 32 | Train loss: 0.17435 | Test loss: 7.83776 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1262\n",
      "Batch 10, Loss: 0.1433\n",
      "Batch 20, Loss: 0.0908\n",
      "Batch 30, Loss: 0.1417\n",
      "Batch 40, Loss: 0.1777\n",
      "Batch 50, Loss: 0.1075\n",
      "Batch 60, Loss: 0.0561\n",
      "Batch 70, Loss: 0.0465\n",
      "Batch 80, Loss: 0.0723\n",
      "Batch 90, Loss: 0.0331\n",
      "Epoch: 33 | Train loss: 0.13963 | Test loss: 0.40719 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0450\n",
      "Batch 10, Loss: 0.0102\n",
      "Batch 20, Loss: 0.1121\n",
      "Batch 30, Loss: 0.4548\n",
      "Batch 40, Loss: 0.1950\n",
      "Batch 50, Loss: 0.3276\n",
      "Batch 60, Loss: 0.2628\n",
      "Batch 70, Loss: 0.0178\n",
      "Batch 80, Loss: 0.0912\n",
      "Batch 90, Loss: 0.4005\n",
      "Epoch: 34 | Train loss: 0.18162 | Test loss: 2.52721 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0378\n",
      "Batch 10, Loss: 0.0516\n",
      "Batch 20, Loss: 0.4783\n",
      "Batch 30, Loss: 0.2642\n",
      "Batch 40, Loss: 0.1497\n",
      "Batch 50, Loss: 0.0054\n",
      "Batch 60, Loss: 0.1211\n",
      "Batch 70, Loss: 0.0078\n",
      "Batch 80, Loss: 0.1154\n",
      "Batch 90, Loss: 0.0807\n",
      "Epoch: 35 | Train loss: 0.13008 | Test loss: 6.03585 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0977\n",
      "Batch 10, Loss: 0.0109\n",
      "Batch 20, Loss: 0.0781\n",
      "Batch 30, Loss: 0.2297\n",
      "Batch 40, Loss: 0.2549\n",
      "Batch 50, Loss: 0.1096\n",
      "Batch 60, Loss: 0.3429\n",
      "Batch 70, Loss: 0.2632\n",
      "Batch 80, Loss: 0.0440\n",
      "Batch 90, Loss: 0.5649\n",
      "Epoch: 36 | Train loss: 0.16704 | Test loss: 1.76684 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.7696\n",
      "Batch 10, Loss: 0.5401\n",
      "Batch 20, Loss: 0.0785\n",
      "Batch 30, Loss: 0.1029\n",
      "Batch 40, Loss: 0.3499\n",
      "Batch 50, Loss: 0.0326\n",
      "Batch 60, Loss: 0.0436\n",
      "Batch 70, Loss: 0.0895\n",
      "Batch 80, Loss: 0.1245\n",
      "Batch 90, Loss: 0.2396\n",
      "Epoch: 37 | Train loss: 0.17768 | Test loss: 4.30931 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3163\n",
      "Batch 10, Loss: 0.0777\n",
      "Batch 20, Loss: 0.1064\n",
      "Batch 30, Loss: 0.0681\n",
      "Batch 40, Loss: 0.0493\n",
      "Batch 50, Loss: 0.1127\n",
      "Batch 60, Loss: 0.2108\n",
      "Batch 70, Loss: 0.3091\n",
      "Batch 80, Loss: 0.0740\n",
      "Batch 90, Loss: 0.0229\n",
      "Epoch: 38 | Train loss: 0.16926 | Test loss: 2.44094 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.1333\n",
      "Batch 10, Loss: 0.1870\n",
      "Batch 20, Loss: 0.1876\n",
      "Batch 30, Loss: 0.0296\n",
      "Batch 40, Loss: 0.0732\n",
      "Batch 50, Loss: 0.0253\n",
      "Batch 60, Loss: 0.4397\n",
      "Batch 70, Loss: 0.3404\n",
      "Batch 80, Loss: 0.0200\n",
      "Batch 90, Loss: 0.0539\n",
      "Epoch: 39 | Train loss: 0.20875 | Test loss: 0.19707 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.2191\n",
      "Batch 10, Loss: 0.0248\n",
      "Batch 20, Loss: 0.1106\n",
      "Batch 30, Loss: 0.2102\n",
      "Batch 40, Loss: 0.0586\n",
      "Batch 50, Loss: 0.1759\n",
      "Batch 60, Loss: 0.6767\n",
      "Batch 70, Loss: 0.0677\n",
      "Batch 80, Loss: 0.0380\n",
      "Batch 90, Loss: 0.0879\n",
      "Epoch: 40 | Train loss: 0.17185 | Test loss: 2.25855 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1316\n",
      "Batch 10, Loss: 0.0077\n",
      "Batch 20, Loss: 0.0033\n",
      "Batch 30, Loss: 0.0491\n",
      "Batch 40, Loss: 0.0538\n",
      "Batch 50, Loss: 0.2493\n",
      "Batch 60, Loss: 0.1747\n",
      "Batch 70, Loss: 0.0384\n",
      "Batch 80, Loss: 0.4508\n",
      "Batch 90, Loss: 0.0581\n",
      "Epoch: 41 | Train loss: 0.17836 | Test loss: 2.48495 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3561\n",
      "Batch 10, Loss: 0.0577\n",
      "Batch 20, Loss: 0.1862\n",
      "Batch 30, Loss: 0.1332\n",
      "Batch 40, Loss: 0.0621\n",
      "Batch 50, Loss: 0.0227\n",
      "Batch 60, Loss: 0.2583\n",
      "Batch 70, Loss: 0.0781\n",
      "Batch 80, Loss: 0.0595\n",
      "Batch 90, Loss: 0.1415\n",
      "Epoch: 42 | Train loss: 0.15806 | Test loss: 0.66546 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2048\n",
      "Batch 10, Loss: 0.0763\n",
      "Batch 20, Loss: 0.0537\n",
      "Batch 30, Loss: 0.2840\n",
      "Batch 40, Loss: 0.0619\n",
      "Batch 50, Loss: 0.3298\n",
      "Batch 60, Loss: 0.0545\n",
      "Batch 70, Loss: 0.1088\n",
      "Batch 80, Loss: 0.0636\n",
      "Batch 90, Loss: 0.1622\n",
      "Epoch: 43 | Train loss: 0.16246 | Test loss: 0.23755 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0923\n",
      "Batch 10, Loss: 0.0405\n",
      "Batch 20, Loss: 0.0246\n",
      "Batch 30, Loss: 0.0527\n",
      "Batch 40, Loss: 0.1382\n",
      "Batch 50, Loss: 0.1077\n",
      "Batch 60, Loss: 0.0128\n",
      "Batch 70, Loss: 0.3130\n",
      "Batch 80, Loss: 0.0544\n",
      "Batch 90, Loss: 0.1410\n",
      "Epoch: 44 | Train loss: 0.14150 | Test loss: 2.12034 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2038\n",
      "Batch 10, Loss: 0.1731\n",
      "Batch 20, Loss: 0.0249\n",
      "Batch 30, Loss: 0.2404\n",
      "Batch 40, Loss: 0.1475\n",
      "Batch 50, Loss: 0.2005\n",
      "Batch 60, Loss: 0.2513\n",
      "Batch 70, Loss: 0.0096\n",
      "Batch 80, Loss: 0.1081\n",
      "Batch 90, Loss: 0.0295\n",
      "Epoch: 45 | Train loss: 0.12162 | Test loss: 1.98301 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0122\n",
      "Batch 10, Loss: 0.0185\n",
      "Batch 20, Loss: 0.1969\n",
      "Batch 30, Loss: 0.4247\n",
      "Batch 40, Loss: 0.1708\n",
      "Batch 50, Loss: 0.0393\n",
      "Batch 60, Loss: 0.0874\n",
      "Batch 70, Loss: 0.1387\n",
      "Batch 80, Loss: 0.2451\n",
      "Batch 90, Loss: 0.0419\n",
      "Epoch: 46 | Train loss: 0.15478 | Test loss: 1.77853 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2855\n",
      "Batch 10, Loss: 0.0149\n",
      "Batch 20, Loss: 0.2341\n",
      "Batch 30, Loss: 0.0301\n",
      "Batch 40, Loss: 0.2764\n",
      "Batch 50, Loss: 0.0342\n",
      "Batch 60, Loss: 0.0889\n",
      "Batch 70, Loss: 0.0355\n",
      "Batch 80, Loss: 0.4475\n",
      "Batch 90, Loss: 0.1205\n",
      "Epoch: 47 | Train loss: 0.13985 | Test loss: 0.94190 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0153\n",
      "Batch 10, Loss: 0.0308\n",
      "Batch 20, Loss: 0.1941\n",
      "Batch 30, Loss: 0.1777\n",
      "Batch 40, Loss: 0.1846\n",
      "Batch 50, Loss: 0.1475\n",
      "Batch 60, Loss: 0.0255\n",
      "Batch 70, Loss: 0.0555\n",
      "Batch 80, Loss: 0.0461\n",
      "Batch 90, Loss: 0.6849\n",
      "Epoch: 48 | Train loss: 0.13746 | Test loss: 2.31127 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2895\n",
      "Batch 10, Loss: 0.0356\n",
      "Batch 20, Loss: 0.1974\n",
      "Batch 30, Loss: 0.1672\n",
      "Batch 40, Loss: 0.0996\n",
      "Batch 50, Loss: 0.0804\n",
      "Batch 60, Loss: 0.0114\n",
      "Batch 70, Loss: 0.0342\n",
      "Batch 80, Loss: 0.0654\n",
      "Batch 90, Loss: 0.0260\n",
      "Epoch: 49 | Train loss: 0.13708 | Test loss: 2.11713 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0702\n",
      "Batch 10, Loss: 0.3072\n",
      "Batch 20, Loss: 0.0298\n",
      "Batch 30, Loss: 1.1121\n",
      "Batch 40, Loss: 0.1006\n",
      "Batch 50, Loss: 0.2361\n",
      "Batch 60, Loss: 0.0234\n",
      "Batch 70, Loss: 0.2489\n",
      "Batch 80, Loss: 0.0233\n",
      "Batch 90, Loss: 0.3538\n",
      "Epoch: 50 | Train loss: 0.14410 | Test loss: 1.57821 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0434\n",
      "Batch 10, Loss: 0.0731\n",
      "Batch 20, Loss: 0.0536\n",
      "Batch 30, Loss: 0.0319\n",
      "Batch 40, Loss: 0.2226\n",
      "Batch 50, Loss: 0.2157\n",
      "Batch 60, Loss: 0.1598\n",
      "Batch 70, Loss: 0.0364\n",
      "Batch 80, Loss: 0.0755\n",
      "Batch 90, Loss: 0.3562\n",
      "Epoch: 51 | Train loss: 0.19751 | Test loss: 2.34244 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1006\n",
      "Batch 10, Loss: 0.0604\n",
      "Batch 20, Loss: 0.0711\n",
      "Batch 30, Loss: 0.1589\n",
      "Batch 40, Loss: 0.1314\n",
      "Batch 50, Loss: 0.0652\n",
      "Batch 60, Loss: 0.4038\n",
      "Batch 70, Loss: 0.7396\n",
      "Batch 80, Loss: 0.3303\n",
      "Batch 90, Loss: 0.2073\n",
      "Epoch: 52 | Train loss: 0.14927 | Test loss: 0.39313 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0674\n",
      "Batch 10, Loss: 0.2238\n",
      "Batch 20, Loss: 0.1743\n",
      "Batch 30, Loss: 0.0912\n",
      "Batch 40, Loss: 0.3106\n",
      "Batch 50, Loss: 0.2085\n",
      "Batch 60, Loss: 0.1178\n",
      "Batch 70, Loss: 0.1147\n",
      "Batch 80, Loss: 0.0601\n",
      "Batch 90, Loss: 0.2611\n",
      "Epoch: 53 | Train loss: 0.14613 | Test loss: 0.04302 | Test accuracy: 0.65979\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.2067\n",
      "Batch 20, Loss: 0.1407\n",
      "Batch 30, Loss: 0.0552\n",
      "Batch 40, Loss: 0.1383\n",
      "Batch 50, Loss: 0.0955\n",
      "Batch 60, Loss: 0.0841\n",
      "Batch 70, Loss: 0.0736\n",
      "Batch 80, Loss: 0.1377\n",
      "Batch 90, Loss: 0.1310\n",
      "Epoch: 54 | Train loss: 0.16083 | Test loss: 7.42673 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0469\n",
      "Batch 10, Loss: 0.0840\n",
      "Batch 20, Loss: 0.0057\n",
      "Batch 30, Loss: 0.0298\n",
      "Batch 40, Loss: 0.2603\n",
      "Batch 50, Loss: 0.0116\n",
      "Batch 60, Loss: 0.1783\n",
      "Batch 70, Loss: 0.2973\n",
      "Batch 80, Loss: 0.0383\n",
      "Batch 90, Loss: 0.0217\n",
      "Epoch: 55 | Train loss: 0.13105 | Test loss: 0.05136 | Test accuracy: 0.54639\n",
      "Batch 0, Loss: 0.3596\n",
      "Batch 10, Loss: 0.0536\n",
      "Batch 20, Loss: 0.2072\n",
      "Batch 30, Loss: 0.0860\n",
      "Batch 40, Loss: 0.5508\n",
      "Batch 50, Loss: 0.0302\n",
      "Batch 60, Loss: 0.0779\n",
      "Batch 70, Loss: 0.0514\n",
      "Batch 80, Loss: 0.3424\n",
      "Batch 90, Loss: 0.0578\n",
      "Epoch: 56 | Train loss: 0.14054 | Test loss: 0.07100 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0759\n",
      "Batch 10, Loss: 0.0177\n",
      "Batch 20, Loss: 0.1033\n",
      "Batch 30, Loss: 0.4256\n",
      "Batch 40, Loss: 0.0154\n",
      "Batch 50, Loss: 0.0728\n",
      "Batch 60, Loss: 0.0875\n",
      "Batch 70, Loss: 0.1569\n",
      "Batch 80, Loss: 0.2122\n",
      "Batch 90, Loss: 0.0220\n",
      "Epoch: 57 | Train loss: 0.13181 | Test loss: 7.26068 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0251\n",
      "Batch 10, Loss: 0.2543\n",
      "Batch 20, Loss: 0.1808\n",
      "Batch 30, Loss: 0.0378\n",
      "Batch 40, Loss: 0.0304\n",
      "Batch 50, Loss: 0.2221\n",
      "Batch 60, Loss: 0.1596\n",
      "Batch 70, Loss: 0.1030\n",
      "Batch 80, Loss: 0.0824\n",
      "Batch 90, Loss: 0.0462\n",
      "Epoch: 58 | Train loss: 0.10880 | Test loss: 0.48611 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0002\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0127\n",
      "Batch 30, Loss: 0.0247\n",
      "Batch 40, Loss: 0.0197\n",
      "Batch 50, Loss: 0.3646\n",
      "Batch 60, Loss: 0.1661\n",
      "Batch 70, Loss: 0.0284\n",
      "Batch 80, Loss: 0.0840\n",
      "Batch 90, Loss: 0.0421\n",
      "Epoch: 59 | Train loss: 0.12837 | Test loss: 6.13848 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0691\n",
      "Batch 10, Loss: 0.0545\n",
      "Batch 20, Loss: 0.0229\n",
      "Batch 30, Loss: 0.3056\n",
      "Batch 40, Loss: 0.1307\n",
      "Batch 50, Loss: 0.2733\n",
      "Batch 60, Loss: 0.1865\n",
      "Batch 70, Loss: 0.1509\n",
      "Batch 80, Loss: 0.1463\n",
      "Batch 90, Loss: 0.0771\n",
      "Epoch: 60 | Train loss: 0.19826 | Test loss: 2.42868 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2226\n",
      "Batch 10, Loss: 0.4612\n",
      "Batch 20, Loss: 0.0508\n",
      "Batch 30, Loss: 0.0452\n",
      "Batch 40, Loss: 0.4179\n",
      "Batch 50, Loss: 0.1159\n",
      "Batch 60, Loss: 0.0437\n",
      "Batch 70, Loss: 0.0645\n",
      "Batch 80, Loss: 0.5881\n",
      "Batch 90, Loss: 0.1462\n",
      "Epoch: 61 | Train loss: 0.16921 | Test loss: 4.60593 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0277\n",
      "Batch 10, Loss: 0.0566\n",
      "Batch 20, Loss: 0.2142\n",
      "Batch 30, Loss: 0.0228\n",
      "Batch 40, Loss: 0.0362\n",
      "Batch 50, Loss: 0.0350\n",
      "Batch 60, Loss: 0.0250\n",
      "Batch 70, Loss: 0.2801\n",
      "Batch 80, Loss: 0.0297\n",
      "Batch 90, Loss: 0.0190\n",
      "Epoch: 62 | Train loss: 0.13326 | Test loss: 2.95477 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1486\n",
      "Batch 10, Loss: 0.0545\n",
      "Batch 20, Loss: 0.0261\n",
      "Batch 30, Loss: 0.1747\n",
      "Batch 40, Loss: 0.1175\n",
      "Batch 50, Loss: 0.0412\n",
      "Batch 60, Loss: 0.0865\n",
      "Batch 70, Loss: 0.0106\n",
      "Batch 80, Loss: 0.0276\n",
      "Batch 90, Loss: 0.0964\n",
      "Epoch: 63 | Train loss: 0.10002 | Test loss: 5.82413 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1142\n",
      "Batch 10, Loss: 0.0491\n",
      "Batch 20, Loss: 0.0467\n",
      "Batch 30, Loss: 0.1033\n",
      "Batch 40, Loss: 0.0192\n",
      "Batch 50, Loss: 0.0204\n",
      "Batch 60, Loss: 0.0876\n",
      "Batch 70, Loss: 0.0357\n",
      "Batch 80, Loss: 0.0412\n",
      "Batch 90, Loss: 0.1121\n",
      "Epoch: 64 | Train loss: 0.13825 | Test loss: 0.04452 | Test accuracy: 0.49485\n",
      "Batch 0, Loss: 0.0239\n",
      "Batch 10, Loss: 0.1940\n",
      "Batch 20, Loss: 0.1104\n",
      "Batch 30, Loss: 0.0186\n",
      "Batch 40, Loss: 0.4813\n",
      "Batch 50, Loss: 0.0846\n",
      "Batch 60, Loss: 0.1370\n",
      "Batch 70, Loss: 0.0011\n",
      "Batch 80, Loss: 0.2772\n",
      "Batch 90, Loss: 0.0092\n",
      "Epoch: 65 | Train loss: 0.10385 | Test loss: 3.88196 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0060\n",
      "Batch 10, Loss: 0.0346\n",
      "Batch 20, Loss: 0.0247\n",
      "Batch 30, Loss: 0.0615\n",
      "Batch 40, Loss: 0.0489\n",
      "Batch 50, Loss: 0.2204\n",
      "Batch 60, Loss: 0.3906\n",
      "Batch 70, Loss: 0.5007\n",
      "Batch 80, Loss: 0.0113\n",
      "Batch 90, Loss: 0.0733\n",
      "Epoch: 66 | Train loss: 0.10527 | Test loss: 0.04739 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1933\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0473\n",
      "Batch 30, Loss: 0.0560\n",
      "Batch 40, Loss: 0.0132\n",
      "Batch 50, Loss: 0.0072\n",
      "Batch 60, Loss: 0.0274\n",
      "Batch 70, Loss: 0.1270\n",
      "Batch 80, Loss: 0.0419\n",
      "Batch 90, Loss: 0.2593\n",
      "Epoch: 67 | Train loss: 0.13584 | Test loss: 0.09969 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.1443\n",
      "Batch 10, Loss: 0.2827\n",
      "Batch 20, Loss: 0.0500\n",
      "Batch 30, Loss: 0.1359\n",
      "Batch 40, Loss: 0.0256\n",
      "Batch 50, Loss: 0.0894\n",
      "Batch 60, Loss: 0.2086\n",
      "Batch 70, Loss: 0.1061\n",
      "Batch 80, Loss: 0.1296\n",
      "Batch 90, Loss: 0.4130\n",
      "Epoch: 68 | Train loss: 0.13759 | Test loss: 1.00929 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2932\n",
      "Batch 10, Loss: 0.2118\n",
      "Batch 20, Loss: 0.1725\n",
      "Batch 30, Loss: 0.1554\n",
      "Batch 40, Loss: 0.0046\n",
      "Batch 50, Loss: 0.0951\n",
      "Batch 60, Loss: 0.9780\n",
      "Batch 70, Loss: 0.0872\n",
      "Batch 80, Loss: 0.0735\n",
      "Batch 90, Loss: 0.1669\n",
      "Epoch: 69 | Train loss: 0.17918 | Test loss: 4.98365 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0038\n",
      "Batch 10, Loss: 0.1528\n",
      "Batch 20, Loss: 0.1936\n",
      "Batch 30, Loss: 0.0424\n",
      "Batch 40, Loss: 0.2166\n",
      "Batch 50, Loss: 0.1486\n",
      "Batch 60, Loss: 0.0901\n",
      "Batch 70, Loss: 0.0368\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.2850\n",
      "Epoch: 70 | Train loss: 0.12720 | Test loss: 2.94088 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0094\n",
      "Batch 10, Loss: 0.1341\n",
      "Batch 20, Loss: 0.4280\n",
      "Batch 30, Loss: 1.0493\n",
      "Batch 40, Loss: 0.1770\n",
      "Batch 50, Loss: 0.1982\n",
      "Batch 60, Loss: 0.0121\n",
      "Batch 70, Loss: 0.0295\n",
      "Batch 80, Loss: 0.1464\n",
      "Batch 90, Loss: 0.0067\n",
      "Epoch: 71 | Train loss: 0.11123 | Test loss: 1.84771 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1346\n",
      "Batch 10, Loss: 0.0446\n",
      "Batch 20, Loss: 0.7207\n",
      "Batch 30, Loss: 0.0440\n",
      "Batch 40, Loss: 0.1227\n",
      "Batch 50, Loss: 0.1112\n",
      "Batch 60, Loss: 0.0565\n",
      "Batch 70, Loss: 0.0080\n",
      "Batch 80, Loss: 0.0174\n",
      "Batch 90, Loss: 0.0780\n",
      "Epoch: 72 | Train loss: 0.10327 | Test loss: 5.35770 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.6067\n",
      "Batch 10, Loss: 0.0072\n",
      "Batch 20, Loss: 0.0756\n",
      "Batch 30, Loss: 0.0407\n",
      "Batch 40, Loss: 0.0046\n",
      "Batch 50, Loss: 0.1831\n",
      "Batch 60, Loss: 0.0825\n",
      "Batch 70, Loss: 0.0157\n",
      "Batch 80, Loss: 0.1648\n",
      "Batch 90, Loss: 0.5741\n",
      "Epoch: 73 | Train loss: 0.14404 | Test loss: 0.82199 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1717\n",
      "Batch 10, Loss: 0.0295\n",
      "Batch 20, Loss: 0.2052\n",
      "Batch 30, Loss: 0.0790\n",
      "Batch 40, Loss: 0.0567\n",
      "Batch 50, Loss: 0.1208\n",
      "Batch 60, Loss: 0.0950\n",
      "Batch 70, Loss: 0.2211\n",
      "Batch 80, Loss: 0.1113\n",
      "Batch 90, Loss: 0.0166\n",
      "Epoch: 74 | Train loss: 0.15996 | Test loss: 0.22432 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0449\n",
      "Batch 10, Loss: 0.1319\n",
      "Batch 20, Loss: 0.0523\n",
      "Batch 30, Loss: 0.0272\n",
      "Batch 40, Loss: 0.0746\n",
      "Batch 50, Loss: 0.4084\n",
      "Batch 60, Loss: 0.1541\n",
      "Batch 70, Loss: 0.0086\n",
      "Batch 80, Loss: 0.1103\n",
      "Batch 90, Loss: 0.0201\n",
      "Epoch: 75 | Train loss: 0.11897 | Test loss: 2.57746 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0233\n",
      "Batch 10, Loss: 0.1548\n",
      "Batch 20, Loss: 0.2529\n",
      "Batch 30, Loss: 0.0112\n",
      "Batch 40, Loss: 0.1565\n",
      "Batch 50, Loss: 0.1580\n",
      "Batch 60, Loss: 0.1143\n",
      "Batch 70, Loss: 0.0252\n",
      "Batch 80, Loss: 0.0701\n",
      "Batch 90, Loss: 0.0957\n",
      "Epoch: 76 | Train loss: 0.10154 | Test loss: 2.08040 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1714\n",
      "Batch 10, Loss: 0.0586\n",
      "Batch 20, Loss: 0.1774\n",
      "Batch 30, Loss: 0.0912\n",
      "Batch 40, Loss: 0.1045\n",
      "Batch 50, Loss: 0.1153\n",
      "Batch 60, Loss: 0.6659\n",
      "Batch 70, Loss: 0.1570\n",
      "Batch 80, Loss: 0.0073\n",
      "Batch 90, Loss: 0.0972\n",
      "Epoch: 77 | Train loss: 0.13798 | Test loss: 0.06784 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.0137\n",
      "Batch 10, Loss: 0.1315\n",
      "Batch 20, Loss: 0.0653\n",
      "Batch 30, Loss: 0.0012\n",
      "Batch 40, Loss: 0.0087\n",
      "Batch 50, Loss: 0.0336\n",
      "Batch 60, Loss: 0.0092\n",
      "Batch 70, Loss: 0.0628\n",
      "Batch 80, Loss: 0.0114\n",
      "Batch 90, Loss: 0.2235\n",
      "Epoch: 78 | Train loss: 0.08592 | Test loss: 1.37520 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1248\n",
      "Batch 10, Loss: 0.0157\n",
      "Batch 20, Loss: 0.0438\n",
      "Batch 30, Loss: 0.0407\n",
      "Batch 40, Loss: 0.2270\n",
      "Batch 50, Loss: 0.0313\n",
      "Batch 60, Loss: 0.0674\n",
      "Batch 70, Loss: 0.0543\n",
      "Batch 80, Loss: 0.0295\n",
      "Batch 90, Loss: 0.0138\n",
      "Epoch: 79 | Train loss: 0.09073 | Test loss: 6.50982 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0972\n",
      "Batch 10, Loss: 0.0115\n",
      "Batch 20, Loss: 0.5557\n",
      "Batch 30, Loss: 0.0195\n",
      "Batch 40, Loss: 0.0068\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0499\n",
      "Batch 70, Loss: 0.0702\n",
      "Batch 80, Loss: 0.0120\n",
      "Batch 90, Loss: 0.0125\n",
      "Epoch: 80 | Train loss: 0.13274 | Test loss: 1.19586 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0086\n",
      "Batch 10, Loss: 0.0549\n",
      "Batch 20, Loss: 0.4658\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.1466\n",
      "Batch 50, Loss: 0.2176\n",
      "Batch 60, Loss: 0.3732\n",
      "Batch 70, Loss: 0.0597\n",
      "Batch 80, Loss: 0.1706\n",
      "Batch 90, Loss: 0.1266\n",
      "Epoch: 81 | Train loss: 0.11205 | Test loss: 0.84945 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0139\n",
      "Batch 10, Loss: 0.0473\n",
      "Batch 20, Loss: 0.0674\n",
      "Batch 30, Loss: 0.2053\n",
      "Batch 40, Loss: 0.0128\n",
      "Batch 50, Loss: 0.0826\n",
      "Batch 60, Loss: 0.1666\n",
      "Batch 70, Loss: 0.0555\n",
      "Batch 80, Loss: 0.0576\n",
      "Batch 90, Loss: 0.0360\n",
      "Epoch: 82 | Train loss: 0.12319 | Test loss: 0.14120 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0124\n",
      "Batch 10, Loss: 0.3158\n",
      "Batch 20, Loss: 0.2928\n",
      "Batch 30, Loss: 0.0967\n",
      "Batch 40, Loss: 0.0518\n",
      "Batch 50, Loss: 0.0536\n",
      "Batch 60, Loss: 0.0928\n",
      "Batch 70, Loss: 0.0432\n",
      "Batch 80, Loss: 0.0291\n",
      "Batch 90, Loss: 0.1156\n",
      "Epoch: 83 | Train loss: 0.10164 | Test loss: 2.31146 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0207\n",
      "Batch 10, Loss: 0.0815\n",
      "Batch 20, Loss: 0.7154\n",
      "Batch 30, Loss: 0.4366\n",
      "Batch 40, Loss: 0.0691\n",
      "Batch 50, Loss: 0.0941\n",
      "Batch 60, Loss: 0.4870\n",
      "Batch 70, Loss: 0.0414\n",
      "Batch 80, Loss: 0.0845\n",
      "Batch 90, Loss: 0.1209\n",
      "Epoch: 84 | Train loss: 0.14722 | Test loss: 0.96398 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.2441\n",
      "Batch 20, Loss: 0.6838\n",
      "Batch 30, Loss: 0.1557\n",
      "Batch 40, Loss: 0.0296\n",
      "Batch 50, Loss: 0.0199\n",
      "Batch 60, Loss: 0.2296\n",
      "Batch 70, Loss: 0.1446\n",
      "Batch 80, Loss: 0.1305\n",
      "Batch 90, Loss: 0.0429\n",
      "Epoch: 85 | Train loss: 0.13571 | Test loss: 0.19894 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0986\n",
      "Batch 10, Loss: 0.0008\n",
      "Batch 20, Loss: 0.0123\n",
      "Batch 30, Loss: 0.0352\n",
      "Batch 40, Loss: 0.0296\n",
      "Batch 50, Loss: 0.0325\n",
      "Batch 60, Loss: 0.0050\n",
      "Batch 70, Loss: 0.0519\n",
      "Batch 80, Loss: 0.1184\n",
      "Batch 90, Loss: 0.0158\n",
      "Epoch: 86 | Train loss: 0.09683 | Test loss: 2.00828 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0144\n",
      "Batch 10, Loss: 0.1014\n",
      "Batch 20, Loss: 0.0686\n",
      "Batch 30, Loss: 0.1407\n",
      "Batch 40, Loss: 0.0225\n",
      "Batch 50, Loss: 0.1330\n",
      "Batch 60, Loss: 0.0949\n",
      "Batch 70, Loss: 0.3189\n",
      "Batch 80, Loss: 0.2825\n",
      "Batch 90, Loss: 0.0166\n",
      "Epoch: 87 | Train loss: 0.11826 | Test loss: 0.53086 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0292\n",
      "Batch 10, Loss: 0.0241\n",
      "Batch 20, Loss: 0.4359\n",
      "Batch 30, Loss: 0.0043\n",
      "Batch 40, Loss: 0.0134\n",
      "Batch 50, Loss: 0.0073\n",
      "Batch 60, Loss: 0.0194\n",
      "Batch 70, Loss: 0.1524\n",
      "Batch 80, Loss: 0.0605\n",
      "Batch 90, Loss: 0.1881\n",
      "Epoch: 88 | Train loss: 0.09401 | Test loss: 0.04621 | Test accuracy: 0.67784\n",
      "Batch 0, Loss: 0.0819\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0315\n",
      "Batch 30, Loss: 0.3424\n",
      "Batch 40, Loss: 0.0281\n",
      "Batch 50, Loss: 0.0323\n",
      "Batch 60, Loss: 0.0268\n",
      "Batch 70, Loss: 0.2031\n",
      "Batch 80, Loss: 0.0118\n",
      "Batch 90, Loss: 0.1242\n",
      "Epoch: 89 | Train loss: 0.10718 | Test loss: 0.04469 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0700\n",
      "Batch 10, Loss: 0.0116\n",
      "Batch 20, Loss: 0.0425\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.0227\n",
      "Batch 50, Loss: 0.1370\n",
      "Batch 60, Loss: 0.4267\n",
      "Batch 70, Loss: 0.1979\n",
      "Batch 80, Loss: 0.1096\n",
      "Batch 90, Loss: 0.1401\n",
      "Epoch: 90 | Train loss: 0.09387 | Test loss: 1.83758 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1002\n",
      "Batch 10, Loss: 0.0171\n",
      "Batch 20, Loss: 0.1721\n",
      "Batch 30, Loss: 0.1214\n",
      "Batch 40, Loss: 0.1320\n",
      "Batch 50, Loss: 0.0196\n",
      "Batch 60, Loss: 0.0129\n",
      "Batch 70, Loss: 0.5231\n",
      "Batch 80, Loss: 0.2846\n",
      "Batch 90, Loss: 0.0751\n",
      "Epoch: 91 | Train loss: 0.11557 | Test loss: 0.04622 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0313\n",
      "Batch 10, Loss: 0.3890\n",
      "Batch 20, Loss: 0.0120\n",
      "Batch 30, Loss: 0.1169\n",
      "Batch 40, Loss: 0.0283\n",
      "Batch 50, Loss: 0.0881\n",
      "Batch 60, Loss: 0.0090\n",
      "Batch 70, Loss: 0.0816\n",
      "Batch 80, Loss: 0.1246\n",
      "Batch 90, Loss: 0.0129\n",
      "Epoch: 92 | Train loss: 0.09123 | Test loss: 6.10017 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0187\n",
      "Batch 10, Loss: 0.1228\n",
      "Batch 20, Loss: 0.0453\n",
      "Batch 30, Loss: 0.0073\n",
      "Batch 40, Loss: 0.0217\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.0917\n",
      "Batch 70, Loss: 0.0609\n",
      "Batch 80, Loss: 0.0219\n",
      "Batch 90, Loss: 0.0131\n",
      "Epoch: 93 | Train loss: 0.09420 | Test loss: 5.15371 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0438\n",
      "Batch 10, Loss: 0.0520\n",
      "Batch 20, Loss: 0.0311\n",
      "Batch 30, Loss: 0.0127\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0082\n",
      "Batch 60, Loss: 0.0454\n",
      "Batch 70, Loss: 0.0255\n",
      "Batch 80, Loss: 0.0321\n",
      "Batch 90, Loss: 0.4181\n",
      "Epoch: 94 | Train loss: 0.08476 | Test loss: 3.39166 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0152\n",
      "Batch 10, Loss: 0.0283\n",
      "Batch 20, Loss: 0.0188\n",
      "Batch 30, Loss: 0.0572\n",
      "Batch 40, Loss: 0.0426\n",
      "Batch 50, Loss: 0.0464\n",
      "Batch 60, Loss: 0.0091\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.0505\n",
      "Batch 90, Loss: 0.0390\n",
      "Epoch: 95 | Train loss: 0.10485 | Test loss: 1.00862 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0239\n",
      "Batch 10, Loss: 0.0209\n",
      "Batch 20, Loss: 0.0207\n",
      "Batch 30, Loss: 0.1134\n",
      "Batch 40, Loss: 0.0554\n",
      "Batch 50, Loss: 0.0798\n",
      "Batch 60, Loss: 0.0136\n",
      "Batch 70, Loss: 0.0162\n",
      "Batch 80, Loss: 0.4707\n",
      "Batch 90, Loss: 0.0481\n",
      "Epoch: 96 | Train loss: 0.08814 | Test loss: 0.25058 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0270\n",
      "Batch 10, Loss: 0.0318\n",
      "Batch 20, Loss: 0.1902\n",
      "Batch 30, Loss: 0.0311\n",
      "Batch 40, Loss: 0.1553\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0090\n",
      "Batch 70, Loss: 0.2258\n",
      "Batch 80, Loss: 0.0828\n",
      "Batch 90, Loss: 0.0325\n",
      "Epoch: 97 | Train loss: 0.08260 | Test loss: 5.83160 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0442\n",
      "Batch 10, Loss: 0.0088\n",
      "Batch 20, Loss: 0.0983\n",
      "Batch 30, Loss: 0.0683\n",
      "Batch 40, Loss: 0.0220\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0104\n",
      "Batch 70, Loss: 0.3423\n",
      "Batch 80, Loss: 0.0302\n",
      "Batch 90, Loss: 0.0760\n",
      "Epoch: 98 | Train loss: 0.07397 | Test loss: 6.75589 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2528\n",
      "Batch 10, Loss: 0.0973\n",
      "Batch 20, Loss: 0.0174\n",
      "Batch 30, Loss: 0.0190\n",
      "Batch 40, Loss: 0.0594\n",
      "Batch 50, Loss: 0.0810\n",
      "Batch 60, Loss: 0.4345\n",
      "Batch 70, Loss: 0.0788\n",
      "Batch 80, Loss: 0.1323\n",
      "Batch 90, Loss: 0.0412\n",
      "Epoch: 99 | Train loss: 0.10442 | Test loss: 2.37862 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0176\n",
      "Batch 10, Loss: 0.0345\n",
      "Batch 20, Loss: 0.1149\n",
      "Batch 30, Loss: 0.2628\n",
      "Batch 40, Loss: 0.1767\n",
      "Batch 50, Loss: 0.0580\n",
      "Batch 60, Loss: 0.0923\n",
      "Batch 70, Loss: 0.0961\n",
      "Batch 80, Loss: 0.0477\n",
      "Batch 90, Loss: 0.0109\n",
      "Epoch: 100 | Train loss: 0.12747 | Test loss: 1.21114 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0503\n",
      "Batch 10, Loss: 0.0198\n",
      "Batch 20, Loss: 0.0298\n",
      "Batch 30, Loss: 0.5433\n",
      "Batch 40, Loss: 0.2193\n",
      "Batch 50, Loss: 0.2737\n",
      "Batch 60, Loss: 0.2654\n",
      "Batch 70, Loss: 0.0172\n",
      "Batch 80, Loss: 0.0460\n",
      "Batch 90, Loss: 0.0139\n",
      "Epoch: 101 | Train loss: 0.10307 | Test loss: 1.95482 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0031\n",
      "Batch 10, Loss: 0.0358\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.2367\n",
      "Batch 40, Loss: 0.3143\n",
      "Batch 50, Loss: 0.1225\n",
      "Batch 60, Loss: 0.2837\n",
      "Batch 70, Loss: 0.0349\n",
      "Batch 80, Loss: 0.0842\n",
      "Batch 90, Loss: 0.0261\n",
      "Epoch: 102 | Train loss: 0.15591 | Test loss: 2.61511 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0845\n",
      "Batch 10, Loss: 0.0075\n",
      "Batch 20, Loss: 0.0611\n",
      "Batch 30, Loss: 0.0531\n",
      "Batch 40, Loss: 0.1149\n",
      "Batch 50, Loss: 0.0345\n",
      "Batch 60, Loss: 0.0154\n",
      "Batch 70, Loss: 0.0162\n",
      "Batch 80, Loss: 0.0152\n",
      "Batch 90, Loss: 0.0497\n",
      "Epoch: 103 | Train loss: 0.07285 | Test loss: 4.65429 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0552\n",
      "Batch 10, Loss: 0.0366\n",
      "Batch 20, Loss: 0.0131\n",
      "Batch 30, Loss: 0.1988\n",
      "Batch 40, Loss: 0.0033\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0201\n",
      "Batch 70, Loss: 0.0259\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.1114\n",
      "Epoch: 104 | Train loss: 0.10007 | Test loss: 2.88919 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0732\n",
      "Batch 10, Loss: 0.0252\n",
      "Batch 20, Loss: 0.0758\n",
      "Batch 30, Loss: 0.0221\n",
      "Batch 40, Loss: 0.1208\n",
      "Batch 50, Loss: 0.0358\n",
      "Batch 60, Loss: 0.0116\n",
      "Batch 70, Loss: 0.0238\n",
      "Batch 80, Loss: 0.0067\n",
      "Batch 90, Loss: 0.0485\n",
      "Epoch: 105 | Train loss: 0.06825 | Test loss: 0.27642 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0626\n",
      "Batch 10, Loss: 0.0202\n",
      "Batch 20, Loss: 0.0014\n",
      "Batch 30, Loss: 0.0113\n",
      "Batch 40, Loss: 0.0945\n",
      "Batch 50, Loss: 0.0009\n",
      "Batch 60, Loss: 0.0118\n",
      "Batch 70, Loss: 0.0214\n",
      "Batch 80, Loss: 0.0840\n",
      "Batch 90, Loss: 0.0666\n",
      "Epoch: 106 | Train loss: 0.06134 | Test loss: 0.20043 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.3626\n",
      "Batch 10, Loss: 0.1457\n",
      "Batch 20, Loss: 0.0020\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.3254\n",
      "Batch 50, Loss: 0.3363\n",
      "Batch 60, Loss: 0.0682\n",
      "Batch 70, Loss: 0.1587\n",
      "Batch 80, Loss: 0.0765\n",
      "Batch 90, Loss: 0.1964\n",
      "Epoch: 107 | Train loss: 0.13212 | Test loss: 4.02423 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1504\n",
      "Batch 10, Loss: 0.0588\n",
      "Batch 20, Loss: 0.0104\n",
      "Batch 30, Loss: 0.0292\n",
      "Batch 40, Loss: 0.0567\n",
      "Batch 50, Loss: 0.0483\n",
      "Batch 60, Loss: 0.0709\n",
      "Batch 70, Loss: 0.0015\n",
      "Batch 80, Loss: 0.0768\n",
      "Batch 90, Loss: 0.8404\n",
      "Epoch: 108 | Train loss: 0.08972 | Test loss: 2.49745 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0148\n",
      "Batch 10, Loss: 0.0432\n",
      "Batch 20, Loss: 0.0271\n",
      "Batch 30, Loss: 0.0288\n",
      "Batch 40, Loss: 0.0221\n",
      "Batch 50, Loss: 0.0952\n",
      "Batch 60, Loss: 0.0270\n",
      "Batch 70, Loss: 0.2394\n",
      "Batch 80, Loss: 0.0775\n",
      "Batch 90, Loss: 0.2051\n",
      "Epoch: 109 | Train loss: 0.10145 | Test loss: 0.59726 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1409\n",
      "Batch 10, Loss: 0.0394\n",
      "Batch 20, Loss: 0.0327\n",
      "Batch 30, Loss: 0.0142\n",
      "Batch 40, Loss: 0.7450\n",
      "Batch 50, Loss: 0.0454\n",
      "Batch 60, Loss: 0.3297\n",
      "Batch 70, Loss: 0.1594\n",
      "Batch 80, Loss: 0.0468\n",
      "Batch 90, Loss: 0.0821\n",
      "Epoch: 110 | Train loss: 0.12745 | Test loss: 0.96248 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0123\n",
      "Batch 10, Loss: 0.0187\n",
      "Batch 20, Loss: 0.0087\n",
      "Batch 30, Loss: 0.0062\n",
      "Batch 40, Loss: 0.0171\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.0267\n",
      "Batch 70, Loss: 0.1361\n",
      "Batch 80, Loss: 0.0494\n",
      "Batch 90, Loss: 0.0888\n",
      "Epoch: 111 | Train loss: 0.07327 | Test loss: 0.05285 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1417\n",
      "Batch 10, Loss: 0.1316\n",
      "Batch 20, Loss: 0.1405\n",
      "Batch 30, Loss: 0.1726\n",
      "Batch 40, Loss: 0.2460\n",
      "Batch 50, Loss: 0.0074\n",
      "Batch 60, Loss: 0.3048\n",
      "Batch 70, Loss: 0.0062\n",
      "Batch 80, Loss: 0.0336\n",
      "Batch 90, Loss: 0.0531\n",
      "Epoch: 112 | Train loss: 0.06736 | Test loss: 0.12957 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0005\n",
      "Batch 10, Loss: 0.0036\n",
      "Batch 20, Loss: 0.2026\n",
      "Batch 30, Loss: 0.0617\n",
      "Batch 40, Loss: 0.0021\n",
      "Batch 50, Loss: 0.0931\n",
      "Batch 60, Loss: 0.0241\n",
      "Batch 70, Loss: 0.0241\n",
      "Batch 80, Loss: 0.1405\n",
      "Batch 90, Loss: 0.0125\n",
      "Epoch: 113 | Train loss: 0.07344 | Test loss: 0.04330 | Test accuracy: 0.54897\n",
      "Batch 0, Loss: 0.0748\n",
      "Batch 10, Loss: 0.1002\n",
      "Batch 20, Loss: 0.0330\n",
      "Batch 30, Loss: 0.0878\n",
      "Batch 40, Loss: 0.0892\n",
      "Batch 50, Loss: 0.0118\n",
      "Batch 60, Loss: 0.0077\n",
      "Batch 70, Loss: 0.0173\n",
      "Batch 80, Loss: 0.2830\n",
      "Batch 90, Loss: 0.0152\n",
      "Epoch: 114 | Train loss: 0.12085 | Test loss: 2.19399 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0276\n",
      "Batch 10, Loss: 0.0272\n",
      "Batch 20, Loss: 0.0309\n",
      "Batch 30, Loss: 0.0186\n",
      "Batch 40, Loss: 0.1744\n",
      "Batch 50, Loss: 0.0648\n",
      "Batch 60, Loss: 0.0330\n",
      "Batch 70, Loss: 0.0120\n",
      "Batch 80, Loss: 0.0277\n",
      "Batch 90, Loss: 0.0339\n",
      "Epoch: 115 | Train loss: 0.05731 | Test loss: 1.10338 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1951\n",
      "Batch 10, Loss: 0.0162\n",
      "Batch 20, Loss: 0.0436\n",
      "Batch 30, Loss: 0.0300\n",
      "Batch 40, Loss: 0.3054\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0268\n",
      "Batch 70, Loss: 0.0222\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.0762\n",
      "Epoch: 116 | Train loss: 0.08502 | Test loss: 0.74753 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0330\n",
      "Batch 10, Loss: 0.4998\n",
      "Batch 20, Loss: 0.0203\n",
      "Batch 30, Loss: 0.1699\n",
      "Batch 40, Loss: 0.2408\n",
      "Batch 50, Loss: 0.0071\n",
      "Batch 60, Loss: 0.0337\n",
      "Batch 70, Loss: 0.0729\n",
      "Batch 80, Loss: 0.1328\n",
      "Batch 90, Loss: 0.0523\n",
      "Epoch: 117 | Train loss: 0.10919 | Test loss: 2.71209 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0298\n",
      "Batch 10, Loss: 0.0093\n",
      "Batch 20, Loss: 0.1454\n",
      "Batch 30, Loss: 0.0097\n",
      "Batch 40, Loss: 0.0237\n",
      "Batch 50, Loss: 0.0299\n",
      "Batch 60, Loss: 0.0359\n",
      "Batch 70, Loss: 0.0120\n",
      "Batch 80, Loss: 0.0045\n",
      "Batch 90, Loss: 0.0751\n",
      "Epoch: 118 | Train loss: 0.05094 | Test loss: 1.00025 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0038\n",
      "Batch 10, Loss: 0.0600\n",
      "Batch 20, Loss: 0.0278\n",
      "Batch 30, Loss: 0.0475\n",
      "Batch 40, Loss: 0.0127\n",
      "Batch 50, Loss: 0.0656\n",
      "Batch 60, Loss: 0.0206\n",
      "Batch 70, Loss: 0.3729\n",
      "Batch 80, Loss: 0.0059\n",
      "Batch 90, Loss: 0.0389\n",
      "Epoch: 119 | Train loss: 0.10431 | Test loss: 0.42654 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2967\n",
      "Batch 10, Loss: 0.0233\n",
      "Batch 20, Loss: 0.2578\n",
      "Batch 30, Loss: 0.0403\n",
      "Batch 40, Loss: 0.0114\n",
      "Batch 50, Loss: 0.0066\n",
      "Batch 60, Loss: 0.0302\n",
      "Batch 70, Loss: 0.0358\n",
      "Batch 80, Loss: 0.0560\n",
      "Batch 90, Loss: 0.0158\n",
      "Epoch: 120 | Train loss: 0.08323 | Test loss: 0.35739 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0063\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.0250\n",
      "Batch 30, Loss: 0.0182\n",
      "Batch 40, Loss: 0.2687\n",
      "Batch 50, Loss: 0.1350\n",
      "Batch 60, Loss: 0.0732\n",
      "Batch 70, Loss: 0.0893\n",
      "Batch 80, Loss: 0.0130\n",
      "Batch 90, Loss: 0.0173\n",
      "Epoch: 121 | Train loss: 0.09817 | Test loss: 1.04242 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0646\n",
      "Batch 10, Loss: 0.0012\n",
      "Batch 20, Loss: 0.2956\n",
      "Batch 30, Loss: 0.0393\n",
      "Batch 40, Loss: 0.1921\n",
      "Batch 50, Loss: 0.0160\n",
      "Batch 60, Loss: 0.0386\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0394\n",
      "Batch 90, Loss: 0.0176\n",
      "Epoch: 122 | Train loss: 0.11587 | Test loss: 0.20581 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0085\n",
      "Batch 10, Loss: 0.0165\n",
      "Batch 20, Loss: 0.0422\n",
      "Batch 30, Loss: 0.0184\n",
      "Batch 40, Loss: 0.0744\n",
      "Batch 50, Loss: 0.1091\n",
      "Batch 60, Loss: 0.0538\n",
      "Batch 70, Loss: 0.0076\n",
      "Batch 80, Loss: 0.0566\n",
      "Batch 90, Loss: 0.0254\n",
      "Epoch: 123 | Train loss: 0.09113 | Test loss: 0.16438 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0203\n",
      "Batch 10, Loss: 0.1030\n",
      "Batch 20, Loss: 0.0124\n",
      "Batch 30, Loss: 0.0861\n",
      "Batch 40, Loss: 0.1238\n",
      "Batch 50, Loss: 0.0573\n",
      "Batch 60, Loss: 0.0254\n",
      "Batch 70, Loss: 0.1744\n",
      "Batch 80, Loss: 0.0804\n",
      "Batch 90, Loss: 0.1217\n",
      "Epoch: 124 | Train loss: 0.06360 | Test loss: 0.75804 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0060\n",
      "Batch 10, Loss: 0.0134\n",
      "Batch 20, Loss: 0.0014\n",
      "Batch 30, Loss: 0.0346\n",
      "Batch 40, Loss: 0.0681\n",
      "Batch 50, Loss: 0.0093\n",
      "Batch 60, Loss: 0.0257\n",
      "Batch 70, Loss: 0.0122\n",
      "Batch 80, Loss: 0.0611\n",
      "Batch 90, Loss: 0.2141\n",
      "Epoch: 125 | Train loss: 0.09071 | Test loss: 0.05078 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0170\n",
      "Batch 10, Loss: 0.0442\n",
      "Batch 20, Loss: 0.1425\n",
      "Batch 30, Loss: 0.0176\n",
      "Batch 40, Loss: 0.5669\n",
      "Batch 50, Loss: 0.0681\n",
      "Batch 60, Loss: 0.0517\n",
      "Batch 70, Loss: 0.1641\n",
      "Batch 80, Loss: 0.1434\n",
      "Batch 90, Loss: 0.0843\n",
      "Epoch: 126 | Train loss: 0.10567 | Test loss: 0.87704 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1345\n",
      "Batch 10, Loss: 0.0568\n",
      "Batch 20, Loss: 0.2473\n",
      "Batch 30, Loss: 0.6040\n",
      "Batch 40, Loss: 0.0264\n",
      "Batch 50, Loss: 0.3497\n",
      "Batch 60, Loss: 0.0161\n",
      "Batch 70, Loss: 0.3373\n",
      "Batch 80, Loss: 0.0264\n",
      "Batch 90, Loss: 0.0288\n",
      "Epoch: 127 | Train loss: 0.09156 | Test loss: 2.71534 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2701\n",
      "Batch 10, Loss: 0.0173\n",
      "Batch 20, Loss: 0.1983\n",
      "Batch 30, Loss: 0.0062\n",
      "Batch 40, Loss: 0.0073\n",
      "Batch 50, Loss: 0.2621\n",
      "Batch 60, Loss: 0.0181\n",
      "Batch 70, Loss: 0.1284\n",
      "Batch 80, Loss: 0.1267\n",
      "Batch 90, Loss: 0.0109\n",
      "Epoch: 128 | Train loss: 0.07185 | Test loss: 0.38714 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0114\n",
      "Batch 10, Loss: 0.1054\n",
      "Batch 20, Loss: 0.0086\n",
      "Batch 30, Loss: 0.1227\n",
      "Batch 40, Loss: 0.0391\n",
      "Batch 50, Loss: 0.0022\n",
      "Batch 60, Loss: 0.1805\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0055\n",
      "Batch 90, Loss: 0.1946\n",
      "Epoch: 129 | Train loss: 0.07599 | Test loss: 0.73864 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0311\n",
      "Batch 10, Loss: 0.0798\n",
      "Batch 20, Loss: 0.1812\n",
      "Batch 30, Loss: 0.0059\n",
      "Batch 40, Loss: 0.1495\n",
      "Batch 50, Loss: 0.0129\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.1871\n",
      "Batch 80, Loss: 0.1852\n",
      "Batch 90, Loss: 0.0064\n",
      "Epoch: 130 | Train loss: 0.06675 | Test loss: 1.01195 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0058\n",
      "Batch 10, Loss: 0.2168\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.1279\n",
      "Batch 40, Loss: 0.0787\n",
      "Batch 50, Loss: 0.0959\n",
      "Batch 60, Loss: 0.3977\n",
      "Batch 70, Loss: 0.1426\n",
      "Batch 80, Loss: 0.0149\n",
      "Batch 90, Loss: 0.0219\n",
      "Epoch: 131 | Train loss: 0.08171 | Test loss: 0.04507 | Test accuracy: 0.50000\n",
      "Batch 0, Loss: 0.0072\n",
      "Batch 10, Loss: 0.0245\n",
      "Batch 20, Loss: 0.3336\n",
      "Batch 30, Loss: 0.0391\n",
      "Batch 40, Loss: 0.0266\n",
      "Batch 50, Loss: 0.1877\n",
      "Batch 60, Loss: 0.0329\n",
      "Batch 70, Loss: 0.1230\n",
      "Batch 80, Loss: 0.0406\n",
      "Batch 90, Loss: 0.2487\n",
      "Epoch: 132 | Train loss: 0.08335 | Test loss: 0.65792 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2591\n",
      "Batch 10, Loss: 0.0021\n",
      "Batch 20, Loss: 0.0575\n",
      "Batch 30, Loss: 0.0737\n",
      "Batch 40, Loss: 0.0120\n",
      "Batch 50, Loss: 0.0786\n",
      "Batch 60, Loss: 0.0054\n",
      "Batch 70, Loss: 0.0774\n",
      "Batch 80, Loss: 0.0668\n",
      "Batch 90, Loss: 0.0104\n",
      "Epoch: 133 | Train loss: 0.07474 | Test loss: 0.62515 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0637\n",
      "Batch 10, Loss: 0.0388\n",
      "Batch 20, Loss: 0.2991\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.0827\n",
      "Batch 50, Loss: 0.1289\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0762\n",
      "Batch 80, Loss: 0.0201\n",
      "Batch 90, Loss: 0.0265\n",
      "Epoch: 134 | Train loss: 0.06628 | Test loss: 2.92964 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2127\n",
      "Batch 10, Loss: 0.0320\n",
      "Batch 20, Loss: 0.0105\n",
      "Batch 30, Loss: 0.0683\n",
      "Batch 40, Loss: 0.0289\n",
      "Batch 50, Loss: 0.1893\n",
      "Batch 60, Loss: 0.1921\n",
      "Batch 70, Loss: 0.0812\n",
      "Batch 80, Loss: 0.0146\n",
      "Batch 90, Loss: 0.0241\n",
      "Epoch: 135 | Train loss: 0.08423 | Test loss: 0.07386 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.6859\n",
      "Batch 10, Loss: 0.1941\n",
      "Batch 20, Loss: 0.1083\n",
      "Batch 30, Loss: 0.0407\n",
      "Batch 40, Loss: 0.0152\n",
      "Batch 50, Loss: 0.0146\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0271\n",
      "Batch 90, Loss: 0.0204\n",
      "Epoch: 136 | Train loss: 0.07151 | Test loss: 0.23442 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0078\n",
      "Batch 10, Loss: 0.0289\n",
      "Batch 20, Loss: 0.0199\n",
      "Batch 30, Loss: 0.0094\n",
      "Batch 40, Loss: 0.0197\n",
      "Batch 50, Loss: 0.0144\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0068\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 137 | Train loss: 0.05762 | Test loss: 0.41333 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1321\n",
      "Batch 10, Loss: 0.0474\n",
      "Batch 20, Loss: 0.1686\n",
      "Batch 30, Loss: 0.0069\n",
      "Batch 40, Loss: 0.8645\n",
      "Batch 50, Loss: 0.3449\n",
      "Batch 60, Loss: 0.0907\n",
      "Batch 70, Loss: 0.1168\n",
      "Batch 80, Loss: 0.1171\n",
      "Batch 90, Loss: 0.0044\n",
      "Epoch: 138 | Train loss: 0.10768 | Test loss: 0.79454 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0145\n",
      "Batch 10, Loss: 0.1340\n",
      "Batch 20, Loss: 0.0187\n",
      "Batch 30, Loss: 0.0387\n",
      "Batch 40, Loss: 0.0035\n",
      "Batch 50, Loss: 0.0259\n",
      "Batch 60, Loss: 0.1744\n",
      "Batch 70, Loss: 0.0503\n",
      "Batch 80, Loss: 0.2796\n",
      "Batch 90, Loss: 0.0257\n",
      "Epoch: 139 | Train loss: 0.07825 | Test loss: 0.58046 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0965\n",
      "Batch 20, Loss: 0.0163\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.0311\n",
      "Batch 50, Loss: 0.0521\n",
      "Batch 60, Loss: 0.0223\n",
      "Batch 70, Loss: 0.0227\n",
      "Batch 80, Loss: 0.0181\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 140 | Train loss: 0.06293 | Test loss: 0.27900 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0162\n",
      "Batch 10, Loss: 0.0057\n",
      "Batch 20, Loss: 0.0216\n",
      "Batch 30, Loss: 0.1399\n",
      "Batch 40, Loss: 0.0050\n",
      "Batch 50, Loss: 0.0070\n",
      "Batch 60, Loss: 0.0611\n",
      "Batch 70, Loss: 0.1115\n",
      "Batch 80, Loss: 0.0032\n",
      "Batch 90, Loss: 0.1021\n",
      "Epoch: 141 | Train loss: 0.08177 | Test loss: 0.10548 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0046\n",
      "Batch 10, Loss: 0.0063\n",
      "Batch 20, Loss: 0.0511\n",
      "Batch 30, Loss: 0.0224\n",
      "Batch 40, Loss: 0.0328\n",
      "Batch 50, Loss: 0.0890\n",
      "Batch 60, Loss: 0.2386\n",
      "Batch 70, Loss: 0.2688\n",
      "Batch 80, Loss: 0.0064\n",
      "Batch 90, Loss: 0.2421\n",
      "Epoch: 142 | Train loss: 0.08663 | Test loss: 0.20784 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 1.1320\n",
      "Batch 10, Loss: 0.0209\n",
      "Batch 20, Loss: 0.0463\n",
      "Batch 30, Loss: 0.0821\n",
      "Batch 40, Loss: 0.0194\n",
      "Batch 50, Loss: 0.0132\n",
      "Batch 60, Loss: 0.0430\n",
      "Batch 70, Loss: 0.0046\n",
      "Batch 80, Loss: 0.0398\n",
      "Batch 90, Loss: 0.0601\n",
      "Epoch: 143 | Train loss: 0.14771 | Test loss: 0.06885 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0152\n",
      "Batch 10, Loss: 0.0036\n",
      "Batch 20, Loss: 0.1534\n",
      "Batch 30, Loss: 0.0325\n",
      "Batch 40, Loss: 0.1218\n",
      "Batch 50, Loss: 0.0185\n",
      "Batch 60, Loss: 0.6287\n",
      "Batch 70, Loss: 0.0054\n",
      "Batch 80, Loss: 0.0687\n",
      "Batch 90, Loss: 0.0046\n",
      "Epoch: 144 | Train loss: 0.08257 | Test loss: 0.34745 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0027\n",
      "Batch 10, Loss: 0.2753\n",
      "Batch 20, Loss: 0.0951\n",
      "Batch 30, Loss: 0.0018\n",
      "Batch 40, Loss: 0.0311\n",
      "Batch 50, Loss: 0.0120\n",
      "Batch 60, Loss: 0.0683\n",
      "Batch 70, Loss: 0.0072\n",
      "Batch 80, Loss: 0.0121\n",
      "Batch 90, Loss: 0.0130\n",
      "Epoch: 145 | Train loss: 0.06020 | Test loss: 0.05644 | Test accuracy: 0.54639\n",
      "Batch 0, Loss: 0.0236\n",
      "Batch 10, Loss: 0.0674\n",
      "Batch 20, Loss: 0.0050\n",
      "Batch 30, Loss: 0.0235\n",
      "Batch 40, Loss: 0.0173\n",
      "Batch 50, Loss: 0.1529\n",
      "Batch 60, Loss: 0.0184\n",
      "Batch 70, Loss: 0.0136\n",
      "Batch 80, Loss: 0.0239\n",
      "Batch 90, Loss: 0.0472\n",
      "Epoch: 146 | Train loss: 0.06376 | Test loss: 0.13234 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0147\n",
      "Batch 10, Loss: 0.0276\n",
      "Batch 20, Loss: 0.0144\n",
      "Batch 30, Loss: 0.0709\n",
      "Batch 40, Loss: 0.1170\n",
      "Batch 50, Loss: 0.0168\n",
      "Batch 60, Loss: 0.2030\n",
      "Batch 70, Loss: 0.1261\n",
      "Batch 80, Loss: 0.0255\n",
      "Batch 90, Loss: 0.0247\n",
      "Epoch: 147 | Train loss: 0.06533 | Test loss: 0.13194 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0151\n",
      "Batch 10, Loss: 0.0167\n",
      "Batch 20, Loss: 0.0517\n",
      "Batch 30, Loss: 0.0291\n",
      "Batch 40, Loss: 0.1392\n",
      "Batch 50, Loss: 0.0231\n",
      "Batch 60, Loss: 0.0141\n",
      "Batch 70, Loss: 0.2389\n",
      "Batch 80, Loss: 0.0057\n",
      "Batch 90, Loss: 0.0715\n",
      "Epoch: 148 | Train loss: 0.08829 | Test loss: 0.05330 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0738\n",
      "Batch 10, Loss: 0.0484\n",
      "Batch 20, Loss: 0.0135\n",
      "Batch 30, Loss: 0.0007\n",
      "Batch 40, Loss: 0.1580\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.0361\n",
      "Batch 70, Loss: 0.0058\n",
      "Batch 80, Loss: 0.0231\n",
      "Batch 90, Loss: 0.0211\n",
      "Epoch: 149 | Train loss: 0.06561 | Test loss: 0.06519 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0017\n",
      "Batch 10, Loss: 0.0029\n",
      "Batch 20, Loss: 0.0365\n",
      "Batch 30, Loss: 0.0181\n",
      "Batch 40, Loss: 0.0105\n",
      "Batch 50, Loss: 0.0146\n",
      "Batch 60, Loss: 0.0153\n",
      "Batch 70, Loss: 0.0186\n",
      "Batch 80, Loss: 0.1399\n",
      "Batch 90, Loss: 0.0137\n",
      "Epoch: 150 | Train loss: 0.04764 | Test loss: 1.59616 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0085\n",
      "Batch 10, Loss: 0.0205\n",
      "Batch 20, Loss: 0.0916\n",
      "Batch 30, Loss: 0.0459\n",
      "Batch 40, Loss: 0.0551\n",
      "Batch 50, Loss: 0.0035\n",
      "Batch 60, Loss: 0.0274\n",
      "Batch 70, Loss: 0.0313\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0236\n",
      "Epoch: 151 | Train loss: 0.06829 | Test loss: 0.66335 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1314\n",
      "Batch 10, Loss: 0.0225\n",
      "Batch 20, Loss: 0.0241\n",
      "Batch 30, Loss: 0.0298\n",
      "Batch 40, Loss: 0.2122\n",
      "Batch 50, Loss: 0.2796\n",
      "Batch 60, Loss: 0.1027\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0171\n",
      "Batch 90, Loss: 0.0025\n",
      "Epoch: 152 | Train loss: 0.09559 | Test loss: 0.39270 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1391\n",
      "Batch 10, Loss: 0.0229\n",
      "Batch 20, Loss: 0.0011\n",
      "Batch 30, Loss: 0.0039\n",
      "Batch 40, Loss: 0.0033\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0343\n",
      "Batch 70, Loss: 0.1090\n",
      "Batch 80, Loss: 0.5977\n",
      "Batch 90, Loss: 0.0272\n",
      "Epoch: 153 | Train loss: 0.07425 | Test loss: 0.07849 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0675\n",
      "Batch 10, Loss: 0.0643\n",
      "Batch 20, Loss: 0.0139\n",
      "Batch 30, Loss: 0.0239\n",
      "Batch 40, Loss: 0.0611\n",
      "Batch 50, Loss: 0.0007\n",
      "Batch 60, Loss: 0.0091\n",
      "Batch 70, Loss: 0.0077\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.1417\n",
      "Epoch: 154 | Train loss: 0.05438 | Test loss: 0.04645 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0742\n",
      "Batch 10, Loss: 0.0068\n",
      "Batch 20, Loss: 0.0036\n",
      "Batch 30, Loss: 0.9585\n",
      "Batch 40, Loss: 0.0079\n",
      "Batch 50, Loss: 0.1332\n",
      "Batch 60, Loss: 0.0975\n",
      "Batch 70, Loss: 0.0088\n",
      "Batch 80, Loss: 0.0395\n",
      "Batch 90, Loss: 0.0969\n",
      "Epoch: 155 | Train loss: 0.07438 | Test loss: 0.20267 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0086\n",
      "Batch 10, Loss: 0.0089\n",
      "Batch 20, Loss: 0.0218\n",
      "Batch 30, Loss: 0.0389\n",
      "Batch 40, Loss: 0.0134\n",
      "Batch 50, Loss: 0.1788\n",
      "Batch 60, Loss: 0.0252\n",
      "Batch 70, Loss: 0.0117\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0006\n",
      "Epoch: 156 | Train loss: 0.06382 | Test loss: 0.21315 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0294\n",
      "Batch 10, Loss: 0.0138\n",
      "Batch 20, Loss: 0.2530\n",
      "Batch 30, Loss: 0.0331\n",
      "Batch 40, Loss: 0.0449\n",
      "Batch 50, Loss: 0.0513\n",
      "Batch 60, Loss: 0.0220\n",
      "Batch 70, Loss: 0.0066\n",
      "Batch 80, Loss: 0.0131\n",
      "Batch 90, Loss: 0.0738\n",
      "Epoch: 157 | Train loss: 0.06423 | Test loss: 0.19969 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0891\n",
      "Batch 20, Loss: 0.0021\n",
      "Batch 30, Loss: 0.0097\n",
      "Batch 40, Loss: 0.0090\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.1362\n",
      "Batch 70, Loss: 0.2938\n",
      "Batch 80, Loss: 0.4317\n",
      "Batch 90, Loss: 0.0717\n",
      "Epoch: 158 | Train loss: 0.06914 | Test loss: 0.05345 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0266\n",
      "Batch 10, Loss: 0.0223\n",
      "Batch 20, Loss: 0.0173\n",
      "Batch 30, Loss: 0.0077\n",
      "Batch 40, Loss: 0.1752\n",
      "Batch 50, Loss: 0.0168\n",
      "Batch 60, Loss: 0.0710\n",
      "Batch 70, Loss: 0.0442\n",
      "Batch 80, Loss: 0.0041\n",
      "Batch 90, Loss: 0.0014\n",
      "Epoch: 159 | Train loss: 0.05545 | Test loss: 0.42635 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0337\n",
      "Batch 10, Loss: 0.0001\n",
      "Batch 20, Loss: 0.7356\n",
      "Batch 30, Loss: 0.0276\n",
      "Batch 40, Loss: 0.0107\n",
      "Batch 50, Loss: 0.0051\n",
      "Batch 60, Loss: 0.0175\n",
      "Batch 70, Loss: 0.0580\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.0283\n",
      "Epoch: 160 | Train loss: 0.06784 | Test loss: 0.22238 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0520\n",
      "Batch 10, Loss: 0.0137\n",
      "Batch 20, Loss: 0.0207\n",
      "Batch 30, Loss: 0.0575\n",
      "Batch 40, Loss: 0.1788\n",
      "Batch 50, Loss: 0.0075\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.0023\n",
      "Batch 80, Loss: 0.0132\n",
      "Batch 90, Loss: 0.0370\n",
      "Epoch: 161 | Train loss: 0.07340 | Test loss: 0.07571 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.1811\n",
      "Batch 10, Loss: 0.0614\n",
      "Batch 20, Loss: 0.0336\n",
      "Batch 30, Loss: 0.0335\n",
      "Batch 40, Loss: 0.0418\n",
      "Batch 50, Loss: 0.0399\n",
      "Batch 60, Loss: 0.0185\n",
      "Batch 70, Loss: 0.0155\n",
      "Batch 80, Loss: 0.0439\n",
      "Batch 90, Loss: 0.0743\n",
      "Epoch: 162 | Train loss: 0.06582 | Test loss: 0.13073 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0093\n",
      "Batch 10, Loss: 0.0127\n",
      "Batch 20, Loss: 0.0055\n",
      "Batch 30, Loss: 0.0260\n",
      "Batch 40, Loss: 0.0612\n",
      "Batch 50, Loss: 0.3408\n",
      "Batch 60, Loss: 0.0141\n",
      "Batch 70, Loss: 0.0942\n",
      "Batch 80, Loss: 0.1283\n",
      "Batch 90, Loss: 0.0762\n",
      "Epoch: 163 | Train loss: 0.06764 | Test loss: 0.15478 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0032\n",
      "Batch 10, Loss: 0.0150\n",
      "Batch 20, Loss: 0.0268\n",
      "Batch 30, Loss: 0.4992\n",
      "Batch 40, Loss: 0.1251\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0122\n",
      "Batch 70, Loss: 0.0090\n",
      "Batch 80, Loss: 0.0167\n",
      "Batch 90, Loss: 0.0500\n",
      "Epoch: 164 | Train loss: 0.06339 | Test loss: 0.40103 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0308\n",
      "Batch 10, Loss: 0.0144\n",
      "Batch 20, Loss: 0.1093\n",
      "Batch 30, Loss: 0.2272\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0773\n",
      "Batch 60, Loss: 0.0218\n",
      "Batch 70, Loss: 0.0085\n",
      "Batch 80, Loss: 0.0085\n",
      "Batch 90, Loss: 0.1336\n",
      "Epoch: 165 | Train loss: 0.04670 | Test loss: 0.06266 | Test accuracy: 0.54897\n",
      "Batch 0, Loss: 0.0688\n",
      "Batch 10, Loss: 0.0835\n",
      "Batch 20, Loss: 0.0131\n",
      "Batch 30, Loss: 0.0658\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0429\n",
      "Batch 60, Loss: 0.0626\n",
      "Batch 70, Loss: 0.2545\n",
      "Batch 80, Loss: 0.0006\n",
      "Batch 90, Loss: 0.3708\n",
      "Epoch: 166 | Train loss: 0.05127 | Test loss: 0.12915 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0134\n",
      "Batch 10, Loss: 0.0243\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0423\n",
      "Batch 40, Loss: 0.0596\n",
      "Batch 50, Loss: 0.1306\n",
      "Batch 60, Loss: 0.0336\n",
      "Batch 70, Loss: 0.0201\n",
      "Batch 80, Loss: 0.0221\n",
      "Batch 90, Loss: 0.0061\n",
      "Epoch: 167 | Train loss: 0.08014 | Test loss: 2.51325 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1169\n",
      "Batch 10, Loss: 0.0570\n",
      "Batch 20, Loss: 0.0045\n",
      "Batch 30, Loss: 0.0215\n",
      "Batch 40, Loss: 0.0249\n",
      "Batch 50, Loss: 0.0337\n",
      "Batch 60, Loss: 0.0121\n",
      "Batch 70, Loss: 0.0057\n",
      "Batch 80, Loss: 0.0101\n",
      "Batch 90, Loss: 0.0939\n",
      "Epoch: 168 | Train loss: 0.08173 | Test loss: 0.31074 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0289\n",
      "Batch 20, Loss: 0.0668\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0646\n",
      "Batch 50, Loss: 0.0508\n",
      "Batch 60, Loss: 0.0710\n",
      "Batch 70, Loss: 0.0123\n",
      "Batch 80, Loss: 0.2682\n",
      "Batch 90, Loss: 0.0715\n",
      "Epoch: 169 | Train loss: 0.09115 | Test loss: 0.96991 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0123\n",
      "Batch 10, Loss: 0.1613\n",
      "Batch 20, Loss: 0.0518\n",
      "Batch 30, Loss: 0.0228\n",
      "Batch 40, Loss: 0.0142\n",
      "Batch 50, Loss: 0.0140\n",
      "Batch 60, Loss: 0.0052\n",
      "Batch 70, Loss: 0.0064\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0030\n",
      "Epoch: 170 | Train loss: 0.06199 | Test loss: 0.27788 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1873\n",
      "Batch 10, Loss: 0.0018\n",
      "Batch 20, Loss: 0.1964\n",
      "Batch 30, Loss: 0.0085\n",
      "Batch 40, Loss: 0.0070\n",
      "Batch 50, Loss: 0.0323\n",
      "Batch 60, Loss: 0.0182\n",
      "Batch 70, Loss: 0.0093\n",
      "Batch 80, Loss: 0.0210\n",
      "Batch 90, Loss: 0.0314\n",
      "Epoch: 171 | Train loss: 0.06287 | Test loss: 0.05654 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0451\n",
      "Batch 10, Loss: 0.2641\n",
      "Batch 20, Loss: 0.0317\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.0164\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0260\n",
      "Batch 70, Loss: 0.0420\n",
      "Batch 80, Loss: 0.0941\n",
      "Batch 90, Loss: 0.0040\n",
      "Epoch: 172 | Train loss: 0.03998 | Test loss: 0.11973 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0023\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0914\n",
      "Batch 40, Loss: 0.0077\n",
      "Batch 50, Loss: 0.0370\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0209\n",
      "Batch 80, Loss: 0.3246\n",
      "Batch 90, Loss: 0.3400\n",
      "Epoch: 173 | Train loss: 0.04846 | Test loss: 0.24351 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0021\n",
      "Batch 10, Loss: 0.0108\n",
      "Batch 20, Loss: 0.0027\n",
      "Batch 30, Loss: 0.1063\n",
      "Batch 40, Loss: 0.0049\n",
      "Batch 50, Loss: 0.0101\n",
      "Batch 60, Loss: 0.0007\n",
      "Batch 70, Loss: 0.0278\n",
      "Batch 80, Loss: 0.0003\n",
      "Batch 90, Loss: 0.0408\n",
      "Epoch: 174 | Train loss: 0.03433 | Test loss: 0.20024 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0545\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.0072\n",
      "Batch 30, Loss: 0.0111\n",
      "Batch 40, Loss: 0.0206\n",
      "Batch 50, Loss: 0.0003\n",
      "Batch 60, Loss: 0.0134\n",
      "Batch 70, Loss: 0.0424\n",
      "Batch 80, Loss: 0.0090\n",
      "Batch 90, Loss: 0.1137\n",
      "Epoch: 175 | Train loss: 0.02948 | Test loss: 0.17064 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0509\n",
      "Batch 10, Loss: 0.2895\n",
      "Batch 20, Loss: 0.0325\n",
      "Batch 30, Loss: 0.0194\n",
      "Batch 40, Loss: 0.0019\n",
      "Batch 50, Loss: 0.0079\n",
      "Batch 60, Loss: 0.0329\n",
      "Batch 70, Loss: 0.1821\n",
      "Batch 80, Loss: 0.1357\n",
      "Batch 90, Loss: 0.3905\n",
      "Epoch: 176 | Train loss: 0.05267 | Test loss: 0.19758 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0090\n",
      "Batch 10, Loss: 0.0159\n",
      "Batch 20, Loss: 0.0187\n",
      "Batch 30, Loss: 0.0127\n",
      "Batch 40, Loss: 0.0092\n",
      "Batch 50, Loss: 0.0971\n",
      "Batch 60, Loss: 0.0668\n",
      "Batch 70, Loss: 0.2581\n",
      "Batch 80, Loss: 0.0285\n",
      "Batch 90, Loss: 0.0118\n",
      "Epoch: 177 | Train loss: 0.07420 | Test loss: 0.77613 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0840\n",
      "Batch 10, Loss: 0.0435\n",
      "Batch 20, Loss: 0.0886\n",
      "Batch 30, Loss: 0.0555\n",
      "Batch 40, Loss: 0.1386\n",
      "Batch 50, Loss: 0.0090\n",
      "Batch 60, Loss: 0.0531\n",
      "Batch 70, Loss: 0.0309\n",
      "Batch 80, Loss: 0.0195\n",
      "Batch 90, Loss: 0.0119\n",
      "Epoch: 178 | Train loss: 0.09413 | Test loss: 0.72914 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0039\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.1870\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.2340\n",
      "Batch 50, Loss: 0.0492\n",
      "Batch 60, Loss: 0.0425\n",
      "Batch 70, Loss: 0.1924\n",
      "Batch 80, Loss: 0.0106\n",
      "Batch 90, Loss: 0.4838\n",
      "Epoch: 179 | Train loss: 0.08298 | Test loss: 1.37141 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0277\n",
      "Batch 10, Loss: 0.0557\n",
      "Batch 20, Loss: 0.0007\n",
      "Batch 30, Loss: 0.3700\n",
      "Batch 40, Loss: 0.0709\n",
      "Batch 50, Loss: 0.0408\n",
      "Batch 60, Loss: 0.0222\n",
      "Batch 70, Loss: 0.0240\n",
      "Batch 80, Loss: 0.3350\n",
      "Batch 90, Loss: 0.0337\n",
      "Epoch: 180 | Train loss: 0.08694 | Test loss: 0.15334 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1767\n",
      "Batch 10, Loss: 0.0281\n",
      "Batch 20, Loss: 0.0293\n",
      "Batch 30, Loss: 0.1266\n",
      "Batch 40, Loss: 0.0169\n",
      "Batch 50, Loss: 0.0056\n",
      "Batch 60, Loss: 0.0417\n",
      "Batch 70, Loss: 0.0055\n",
      "Batch 80, Loss: 0.0345\n",
      "Batch 90, Loss: 0.2099\n",
      "Epoch: 181 | Train loss: 0.07955 | Test loss: 0.31285 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0234\n",
      "Batch 10, Loss: 0.2410\n",
      "Batch 20, Loss: 0.0048\n",
      "Batch 30, Loss: 0.0373\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.0246\n",
      "Batch 60, Loss: 0.3020\n",
      "Batch 70, Loss: 0.5399\n",
      "Batch 80, Loss: 0.0078\n",
      "Batch 90, Loss: 0.0123\n",
      "Epoch: 182 | Train loss: 0.05685 | Test loss: 0.09585 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.1208\n",
      "Batch 10, Loss: 0.0110\n",
      "Batch 20, Loss: 0.0834\n",
      "Batch 30, Loss: 0.0356\n",
      "Batch 40, Loss: 0.0239\n",
      "Batch 50, Loss: 0.0074\n",
      "Batch 60, Loss: 0.0103\n",
      "Batch 70, Loss: 0.2747\n",
      "Batch 80, Loss: 0.0144\n",
      "Batch 90, Loss: 0.0943\n",
      "Epoch: 183 | Train loss: 0.06727 | Test loss: 0.13765 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0696\n",
      "Batch 10, Loss: 0.0174\n",
      "Batch 20, Loss: 0.0283\n",
      "Batch 30, Loss: 0.0281\n",
      "Batch 40, Loss: 0.1605\n",
      "Batch 50, Loss: 0.0590\n",
      "Batch 60, Loss: 0.1075\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0185\n",
      "Batch 90, Loss: 0.0091\n",
      "Epoch: 184 | Train loss: 0.05603 | Test loss: 0.07956 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0031\n",
      "Batch 10, Loss: 0.1025\n",
      "Batch 20, Loss: 0.0180\n",
      "Batch 30, Loss: 0.0555\n",
      "Batch 40, Loss: 0.3597\n",
      "Batch 50, Loss: 0.2038\n",
      "Batch 60, Loss: 0.0229\n",
      "Batch 70, Loss: 0.0743\n",
      "Batch 80, Loss: 0.0160\n",
      "Batch 90, Loss: 0.0134\n",
      "Epoch: 185 | Train loss: 0.09741 | Test loss: 0.31357 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3180\n",
      "Batch 10, Loss: 0.0238\n",
      "Batch 20, Loss: 0.1537\n",
      "Batch 30, Loss: 0.0254\n",
      "Batch 40, Loss: 0.0121\n",
      "Batch 50, Loss: 0.0101\n",
      "Batch 60, Loss: 0.2201\n",
      "Batch 70, Loss: 0.0242\n",
      "Batch 80, Loss: 0.0177\n",
      "Batch 90, Loss: 0.1806\n",
      "Epoch: 186 | Train loss: 0.08690 | Test loss: 0.04407 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.0995\n",
      "Batch 10, Loss: 0.0167\n",
      "Batch 20, Loss: 0.0005\n",
      "Batch 30, Loss: 0.0168\n",
      "Batch 40, Loss: 0.0694\n",
      "Batch 50, Loss: 0.1828\n",
      "Batch 60, Loss: 0.0025\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0339\n",
      "Batch 90, Loss: 0.0543\n",
      "Epoch: 187 | Train loss: 0.05157 | Test loss: 0.04225 | Test accuracy: 0.50000\n",
      "Batch 0, Loss: 0.0282\n",
      "Batch 10, Loss: 0.0191\n",
      "Batch 20, Loss: 0.0283\n",
      "Batch 30, Loss: 0.0011\n",
      "Batch 40, Loss: 0.2450\n",
      "Batch 50, Loss: 0.0062\n",
      "Batch 60, Loss: 0.0018\n",
      "Batch 70, Loss: 0.3315\n",
      "Batch 80, Loss: 0.0417\n",
      "Batch 90, Loss: 0.0718\n",
      "Epoch: 188 | Train loss: 0.09512 | Test loss: 0.31659 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0283\n",
      "Batch 10, Loss: 0.0629\n",
      "Batch 20, Loss: 0.3314\n",
      "Batch 30, Loss: 0.0038\n",
      "Batch 40, Loss: 0.0057\n",
      "Batch 50, Loss: 0.0129\n",
      "Batch 60, Loss: 0.0579\n",
      "Batch 70, Loss: 0.0031\n",
      "Batch 80, Loss: 0.0838\n",
      "Batch 90, Loss: 0.1930\n",
      "Epoch: 189 | Train loss: 0.08204 | Test loss: 0.11189 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0071\n",
      "Batch 10, Loss: 0.0032\n",
      "Batch 20, Loss: 0.0629\n",
      "Batch 30, Loss: 0.0080\n",
      "Batch 40, Loss: 0.0043\n",
      "Batch 50, Loss: 0.0348\n",
      "Batch 60, Loss: 0.0658\n",
      "Batch 70, Loss: 0.0072\n",
      "Batch 80, Loss: 0.0242\n",
      "Batch 90, Loss: 0.0293\n",
      "Epoch: 190 | Train loss: 0.05724 | Test loss: 0.21106 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0444\n",
      "Batch 10, Loss: 0.0165\n",
      "Batch 20, Loss: 0.3705\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.0348\n",
      "Batch 50, Loss: 0.0887\n",
      "Batch 60, Loss: 0.0885\n",
      "Batch 70, Loss: 0.1680\n",
      "Batch 80, Loss: 0.0292\n",
      "Batch 90, Loss: 0.1946\n",
      "Epoch: 191 | Train loss: 0.08490 | Test loss: 0.08898 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1509\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0074\n",
      "Batch 30, Loss: 0.0153\n",
      "Batch 40, Loss: 0.0739\n",
      "Batch 50, Loss: 0.0274\n",
      "Batch 60, Loss: 0.0380\n",
      "Batch 70, Loss: 0.0079\n",
      "Batch 80, Loss: 0.2593\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 192 | Train loss: 0.04624 | Test loss: 0.05212 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0177\n",
      "Batch 10, Loss: 0.0597\n",
      "Batch 20, Loss: 0.0029\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0339\n",
      "Batch 50, Loss: 0.0077\n",
      "Batch 60, Loss: 0.0637\n",
      "Batch 70, Loss: 0.0401\n",
      "Batch 80, Loss: 0.0165\n",
      "Batch 90, Loss: 0.0309\n",
      "Epoch: 193 | Train loss: 0.04480 | Test loss: 0.41843 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1647\n",
      "Batch 10, Loss: 0.0249\n",
      "Batch 20, Loss: 0.0057\n",
      "Batch 30, Loss: 0.0072\n",
      "Batch 40, Loss: 0.0329\n",
      "Batch 50, Loss: 0.0522\n",
      "Batch 60, Loss: 0.0867\n",
      "Batch 70, Loss: 0.0099\n",
      "Batch 80, Loss: 0.0012\n",
      "Batch 90, Loss: 0.0209\n",
      "Epoch: 194 | Train loss: 0.03966 | Test loss: 0.12914 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0132\n",
      "Batch 10, Loss: 0.3028\n",
      "Batch 20, Loss: 0.0437\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.0141\n",
      "Batch 50, Loss: 0.0335\n",
      "Batch 60, Loss: 0.0044\n",
      "Batch 70, Loss: 0.3635\n",
      "Batch 80, Loss: 0.0080\n",
      "Batch 90, Loss: 0.0133\n",
      "Epoch: 195 | Train loss: 0.06647 | Test loss: 0.14729 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0358\n",
      "Batch 10, Loss: 0.0361\n",
      "Batch 20, Loss: 0.0430\n",
      "Batch 30, Loss: 0.0059\n",
      "Batch 40, Loss: 0.0089\n",
      "Batch 50, Loss: 0.0342\n",
      "Batch 60, Loss: 0.0327\n",
      "Batch 70, Loss: 0.0291\n",
      "Batch 80, Loss: 0.0122\n",
      "Batch 90, Loss: 0.0919\n",
      "Epoch: 196 | Train loss: 0.06660 | Test loss: 1.06279 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0473\n",
      "Batch 10, Loss: 0.0068\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0900\n",
      "Batch 50, Loss: 0.0288\n",
      "Batch 60, Loss: 0.0369\n",
      "Batch 70, Loss: 0.0004\n",
      "Batch 80, Loss: 0.0246\n",
      "Batch 90, Loss: 0.0248\n",
      "Epoch: 197 | Train loss: 0.05245 | Test loss: 0.21223 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0615\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.2839\n",
      "Batch 30, Loss: 0.1100\n",
      "Batch 40, Loss: 0.1792\n",
      "Batch 50, Loss: 0.0131\n",
      "Batch 60, Loss: 0.0107\n",
      "Batch 70, Loss: 0.0721\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0220\n",
      "Epoch: 198 | Train loss: 0.05438 | Test loss: 0.47100 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0124\n",
      "Batch 10, Loss: 0.0207\n",
      "Batch 20, Loss: 0.0657\n",
      "Batch 30, Loss: 0.1228\n",
      "Batch 40, Loss: 0.0069\n",
      "Batch 50, Loss: 0.0185\n",
      "Batch 60, Loss: 0.2193\n",
      "Batch 70, Loss: 0.2039\n",
      "Batch 80, Loss: 0.0285\n",
      "Batch 90, Loss: 0.0521\n",
      "Epoch: 199 | Train loss: 0.07734 | Test loss: 0.24531 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0085\n",
      "Batch 10, Loss: 0.0224\n",
      "Batch 20, Loss: 0.8472\n",
      "Batch 30, Loss: 0.0263\n",
      "Batch 40, Loss: 0.0079\n",
      "Batch 50, Loss: 0.0481\n",
      "Batch 60, Loss: 0.2242\n",
      "Batch 70, Loss: 0.0312\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.1067\n",
      "Epoch: 200 | Train loss: 0.06000 | Test loss: 0.16546 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0040\n",
      "Batch 30, Loss: 0.0188\n",
      "Batch 40, Loss: 0.0011\n",
      "Batch 50, Loss: 0.0145\n",
      "Batch 60, Loss: 0.1945\n",
      "Batch 70, Loss: 0.0079\n",
      "Batch 80, Loss: 0.0074\n",
      "Batch 90, Loss: 0.0101\n",
      "Epoch: 201 | Train loss: 0.04976 | Test loss: 1.94560 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0048\n",
      "Batch 10, Loss: 0.0883\n",
      "Batch 20, Loss: 0.0545\n",
      "Batch 30, Loss: 0.2860\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.1848\n",
      "Batch 60, Loss: 0.0124\n",
      "Batch 70, Loss: 0.0020\n",
      "Batch 80, Loss: 0.0484\n",
      "Batch 90, Loss: 0.0097\n",
      "Epoch: 202 | Train loss: 0.06405 | Test loss: 0.26609 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0362\n",
      "Batch 10, Loss: 0.0143\n",
      "Batch 20, Loss: 0.0273\n",
      "Batch 30, Loss: 0.1136\n",
      "Batch 40, Loss: 0.0004\n",
      "Batch 50, Loss: 0.0022\n",
      "Batch 60, Loss: 0.1714\n",
      "Batch 70, Loss: 0.0266\n",
      "Batch 80, Loss: 0.0584\n",
      "Batch 90, Loss: 0.0218\n",
      "Epoch: 203 | Train loss: 0.04467 | Test loss: 0.09969 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0734\n",
      "Batch 10, Loss: 0.0100\n",
      "Batch 20, Loss: 0.1188\n",
      "Batch 30, Loss: 0.0033\n",
      "Batch 40, Loss: 0.0029\n",
      "Batch 50, Loss: 0.1002\n",
      "Batch 60, Loss: 0.0253\n",
      "Batch 70, Loss: 0.1751\n",
      "Batch 80, Loss: 0.1762\n",
      "Batch 90, Loss: 0.0114\n",
      "Epoch: 204 | Train loss: 0.08997 | Test loss: 0.07654 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.1563\n",
      "Batch 10, Loss: 0.0460\n",
      "Batch 20, Loss: 0.0249\n",
      "Batch 30, Loss: 0.0114\n",
      "Batch 40, Loss: 0.0460\n",
      "Batch 50, Loss: 0.0409\n",
      "Batch 60, Loss: 0.0076\n",
      "Batch 70, Loss: 0.0592\n",
      "Batch 80, Loss: 0.0017\n",
      "Batch 90, Loss: 0.0286\n",
      "Epoch: 205 | Train loss: 0.05224 | Test loss: 0.03985 | Test accuracy: 0.64175\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0307\n",
      "Batch 20, Loss: 0.0177\n",
      "Batch 30, Loss: 0.1086\n",
      "Batch 40, Loss: 0.2403\n",
      "Batch 50, Loss: 0.0036\n",
      "Batch 60, Loss: 0.0195\n",
      "Batch 70, Loss: 0.0056\n",
      "Batch 80, Loss: 0.0301\n",
      "Batch 90, Loss: 0.0164\n",
      "Epoch: 206 | Train loss: 0.04734 | Test loss: 0.20337 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0762\n",
      "Batch 10, Loss: 0.0436\n",
      "Batch 20, Loss: 0.0200\n",
      "Batch 30, Loss: 0.0781\n",
      "Batch 40, Loss: 0.0069\n",
      "Batch 50, Loss: 0.0083\n",
      "Batch 60, Loss: 0.0382\n",
      "Batch 70, Loss: 0.0327\n",
      "Batch 80, Loss: 0.0482\n",
      "Batch 90, Loss: 0.0089\n",
      "Epoch: 207 | Train loss: 0.05314 | Test loss: 0.04650 | Test accuracy: 0.54639\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.0099\n",
      "Batch 20, Loss: 0.0059\n",
      "Batch 30, Loss: 0.0133\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0403\n",
      "Batch 60, Loss: 0.0256\n",
      "Batch 70, Loss: 0.0212\n",
      "Batch 80, Loss: 0.0277\n",
      "Batch 90, Loss: 0.0128\n",
      "Epoch: 208 | Train loss: 0.05681 | Test loss: 0.06914 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0489\n",
      "Batch 10, Loss: 0.0394\n",
      "Batch 20, Loss: 0.0219\n",
      "Batch 30, Loss: 0.0193\n",
      "Batch 40, Loss: 0.0498\n",
      "Batch 50, Loss: 0.0927\n",
      "Batch 60, Loss: 0.0071\n",
      "Batch 70, Loss: 0.0040\n",
      "Batch 80, Loss: 0.0824\n",
      "Batch 90, Loss: 0.3053\n",
      "Epoch: 209 | Train loss: 0.05024 | Test loss: 0.26713 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0039\n",
      "Batch 10, Loss: 0.0352\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.2108\n",
      "Batch 40, Loss: 0.0170\n",
      "Batch 50, Loss: 0.0123\n",
      "Batch 60, Loss: 0.0220\n",
      "Batch 70, Loss: 0.0126\n",
      "Batch 80, Loss: 0.0342\n",
      "Batch 90, Loss: 0.0752\n",
      "Epoch: 210 | Train loss: 0.09014 | Test loss: 0.80225 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1722\n",
      "Batch 10, Loss: 0.0267\n",
      "Batch 20, Loss: 0.0176\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.1639\n",
      "Batch 50, Loss: 0.0098\n",
      "Batch 60, Loss: 0.0408\n",
      "Batch 70, Loss: 0.0083\n",
      "Batch 80, Loss: 0.0120\n",
      "Batch 90, Loss: 0.2235\n",
      "Epoch: 211 | Train loss: 0.09044 | Test loss: 0.29128 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0689\n",
      "Batch 10, Loss: 0.1046\n",
      "Batch 20, Loss: 0.0087\n",
      "Batch 30, Loss: 0.0399\n",
      "Batch 40, Loss: 0.0744\n",
      "Batch 50, Loss: 0.1444\n",
      "Batch 60, Loss: 0.0634\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.0310\n",
      "Batch 90, Loss: 0.0052\n",
      "Epoch: 212 | Train loss: 0.05468 | Test loss: 0.44357 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.0039\n",
      "Batch 20, Loss: 0.0390\n",
      "Batch 30, Loss: 0.0563\n",
      "Batch 40, Loss: 0.0401\n",
      "Batch 50, Loss: 0.4199\n",
      "Batch 60, Loss: 0.0271\n",
      "Batch 70, Loss: 0.0073\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0139\n",
      "Epoch: 213 | Train loss: 0.04530 | Test loss: 0.36690 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0097\n",
      "Batch 10, Loss: 0.0290\n",
      "Batch 20, Loss: 0.0066\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0332\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0369\n",
      "Batch 70, Loss: 0.0172\n",
      "Batch 80, Loss: 0.0318\n",
      "Batch 90, Loss: 0.0188\n",
      "Epoch: 214 | Train loss: 0.03888 | Test loss: 0.04713 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0104\n",
      "Batch 10, Loss: 0.0394\n",
      "Batch 20, Loss: 0.7227\n",
      "Batch 30, Loss: 0.0152\n",
      "Batch 40, Loss: 0.0086\n",
      "Batch 50, Loss: 1.3145\n",
      "Batch 60, Loss: 0.0169\n",
      "Batch 70, Loss: 0.2576\n",
      "Batch 80, Loss: 0.0206\n",
      "Batch 90, Loss: 0.3512\n",
      "Epoch: 215 | Train loss: 0.10619 | Test loss: 0.21435 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0140\n",
      "Batch 10, Loss: 0.0327\n",
      "Batch 20, Loss: 0.2572\n",
      "Batch 30, Loss: 0.0099\n",
      "Batch 40, Loss: 0.1398\n",
      "Batch 50, Loss: 0.2028\n",
      "Batch 60, Loss: 0.0081\n",
      "Batch 70, Loss: 0.4231\n",
      "Batch 80, Loss: 0.0425\n",
      "Batch 90, Loss: 0.3210\n",
      "Epoch: 216 | Train loss: 0.08017 | Test loss: 0.04068 | Test accuracy: 0.60309\n",
      "Batch 0, Loss: 0.0294\n",
      "Batch 10, Loss: 0.1885\n",
      "Batch 20, Loss: 0.0233\n",
      "Batch 30, Loss: 0.0689\n",
      "Batch 40, Loss: 0.0431\n",
      "Batch 50, Loss: 0.0041\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0170\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0469\n",
      "Epoch: 217 | Train loss: 0.07778 | Test loss: 0.11304 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0590\n",
      "Batch 10, Loss: 0.0701\n",
      "Batch 20, Loss: 0.0085\n",
      "Batch 30, Loss: 0.0160\n",
      "Batch 40, Loss: 0.0346\n",
      "Batch 50, Loss: 0.0141\n",
      "Batch 60, Loss: 0.0383\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0341\n",
      "Batch 90, Loss: 0.0485\n",
      "Epoch: 218 | Train loss: 0.08776 | Test loss: 0.04908 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.1739\n",
      "Batch 10, Loss: 0.0031\n",
      "Batch 20, Loss: 0.0805\n",
      "Batch 30, Loss: 0.1201\n",
      "Batch 40, Loss: 0.0140\n",
      "Batch 50, Loss: 0.0284\n",
      "Batch 60, Loss: 0.0073\n",
      "Batch 70, Loss: 0.0571\n",
      "Batch 80, Loss: 0.0383\n",
      "Batch 90, Loss: 0.1982\n",
      "Epoch: 219 | Train loss: 0.07020 | Test loss: 0.30070 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0304\n",
      "Batch 10, Loss: 0.0050\n",
      "Batch 20, Loss: 0.0048\n",
      "Batch 30, Loss: 0.0035\n",
      "Batch 40, Loss: 0.0107\n",
      "Batch 50, Loss: 0.0122\n",
      "Batch 60, Loss: 0.0196\n",
      "Batch 70, Loss: 0.0799\n",
      "Batch 80, Loss: 0.2061\n",
      "Batch 90, Loss: 0.0112\n",
      "Epoch: 220 | Train loss: 0.04720 | Test loss: 0.28322 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0088\n",
      "Batch 10, Loss: 0.0051\n",
      "Batch 20, Loss: 0.0208\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0011\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0081\n",
      "Batch 70, Loss: 0.0125\n",
      "Batch 80, Loss: 0.0065\n",
      "Batch 90, Loss: 0.0070\n",
      "Epoch: 221 | Train loss: 0.02946 | Test loss: 0.06920 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0651\n",
      "Batch 10, Loss: 0.0395\n",
      "Batch 20, Loss: 0.0045\n",
      "Batch 30, Loss: 0.0161\n",
      "Batch 40, Loss: 0.3455\n",
      "Batch 50, Loss: 0.0261\n",
      "Batch 60, Loss: 0.0012\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0696\n",
      "Batch 90, Loss: 0.0049\n",
      "Epoch: 222 | Train loss: 0.04201 | Test loss: 0.08177 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0188\n",
      "Batch 10, Loss: 0.0285\n",
      "Batch 20, Loss: 0.0037\n",
      "Batch 30, Loss: 0.0114\n",
      "Batch 40, Loss: 0.1174\n",
      "Batch 50, Loss: 0.0265\n",
      "Batch 60, Loss: 0.0193\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0143\n",
      "Batch 90, Loss: 0.2309\n",
      "Epoch: 223 | Train loss: 0.04301 | Test loss: 0.17300 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0118\n",
      "Batch 10, Loss: 0.1616\n",
      "Batch 20, Loss: 0.0540\n",
      "Batch 30, Loss: 0.1505\n",
      "Batch 40, Loss: 0.0031\n",
      "Batch 50, Loss: 0.1738\n",
      "Batch 60, Loss: 0.0457\n",
      "Batch 70, Loss: 0.0352\n",
      "Batch 80, Loss: 0.3227\n",
      "Batch 90, Loss: 0.2178\n",
      "Epoch: 224 | Train loss: 0.08044 | Test loss: 0.04829 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.2283\n",
      "Batch 10, Loss: 0.0358\n",
      "Batch 20, Loss: 0.0021\n",
      "Batch 30, Loss: 0.2045\n",
      "Batch 40, Loss: 0.7358\n",
      "Batch 50, Loss: 0.0272\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.1163\n",
      "Batch 80, Loss: 0.1771\n",
      "Batch 90, Loss: 0.0107\n",
      "Epoch: 225 | Train loss: 0.06549 | Test loss: 0.09286 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0240\n",
      "Batch 10, Loss: 0.0460\n",
      "Batch 20, Loss: 0.0100\n",
      "Batch 30, Loss: 0.0085\n",
      "Batch 40, Loss: 0.0450\n",
      "Batch 50, Loss: 0.0132\n",
      "Batch 60, Loss: 0.0217\n",
      "Batch 70, Loss: 0.0128\n",
      "Batch 80, Loss: 0.0091\n",
      "Batch 90, Loss: 0.5676\n",
      "Epoch: 226 | Train loss: 0.04301 | Test loss: 2.37792 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0400\n",
      "Batch 10, Loss: 0.0483\n",
      "Batch 20, Loss: 0.3205\n",
      "Batch 30, Loss: 0.0348\n",
      "Batch 40, Loss: 0.2185\n",
      "Batch 50, Loss: 0.0053\n",
      "Batch 60, Loss: 0.4675\n",
      "Batch 70, Loss: 0.0066\n",
      "Batch 80, Loss: 0.0588\n",
      "Batch 90, Loss: 0.0134\n",
      "Epoch: 227 | Train loss: 0.05957 | Test loss: 0.07696 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.1497\n",
      "Batch 10, Loss: 0.2560\n",
      "Batch 20, Loss: 0.0787\n",
      "Batch 30, Loss: 0.0069\n",
      "Batch 40, Loss: 0.0040\n",
      "Batch 50, Loss: 0.0117\n",
      "Batch 60, Loss: 0.2011\n",
      "Batch 70, Loss: 0.0309\n",
      "Batch 80, Loss: 0.0389\n",
      "Batch 90, Loss: 0.0010\n",
      "Epoch: 228 | Train loss: 0.05678 | Test loss: 0.05096 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0162\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.0046\n",
      "Batch 40, Loss: 0.0148\n",
      "Batch 50, Loss: 0.0586\n",
      "Batch 60, Loss: 0.0016\n",
      "Batch 70, Loss: 0.0787\n",
      "Batch 80, Loss: 0.2032\n",
      "Batch 90, Loss: 0.0161\n",
      "Epoch: 229 | Train loss: 0.04604 | Test loss: 0.06686 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.1046\n",
      "Batch 10, Loss: 0.0467\n",
      "Batch 20, Loss: 0.0030\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.2037\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0083\n",
      "Batch 70, Loss: 0.0399\n",
      "Batch 80, Loss: 0.0518\n",
      "Batch 90, Loss: 0.0057\n",
      "Epoch: 230 | Train loss: 0.03689 | Test loss: 0.09046 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0106\n",
      "Batch 10, Loss: 0.0056\n",
      "Batch 20, Loss: 0.0430\n",
      "Batch 30, Loss: 0.0318\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.0201\n",
      "Batch 70, Loss: 0.0124\n",
      "Batch 80, Loss: 0.0257\n",
      "Batch 90, Loss: 0.0519\n",
      "Epoch: 231 | Train loss: 0.04011 | Test loss: 0.37976 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0645\n",
      "Batch 10, Loss: 0.0018\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.0690\n",
      "Batch 40, Loss: 0.0415\n",
      "Batch 50, Loss: 0.0638\n",
      "Batch 60, Loss: 0.0181\n",
      "Batch 70, Loss: 0.0445\n",
      "Batch 80, Loss: 0.0255\n",
      "Batch 90, Loss: 0.0077\n",
      "Epoch: 232 | Train loss: 0.08430 | Test loss: 0.05817 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0240\n",
      "Batch 10, Loss: 0.1261\n",
      "Batch 20, Loss: 0.0115\n",
      "Batch 30, Loss: 0.0199\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0226\n",
      "Batch 60, Loss: 0.0114\n",
      "Batch 70, Loss: 0.0065\n",
      "Batch 80, Loss: 0.0201\n",
      "Batch 90, Loss: 0.2650\n",
      "Epoch: 233 | Train loss: 0.04413 | Test loss: 0.14277 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0001\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0460\n",
      "Batch 30, Loss: 0.0142\n",
      "Batch 40, Loss: 0.0139\n",
      "Batch 50, Loss: 0.0267\n",
      "Batch 60, Loss: 0.0233\n",
      "Batch 70, Loss: 0.0015\n",
      "Batch 80, Loss: 0.5070\n",
      "Batch 90, Loss: 0.1638\n",
      "Epoch: 234 | Train loss: 0.06711 | Test loss: 0.06254 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.3221\n",
      "Batch 10, Loss: 0.0109\n",
      "Batch 20, Loss: 0.0462\n",
      "Batch 30, Loss: 0.0583\n",
      "Batch 40, Loss: 0.1145\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0254\n",
      "Batch 70, Loss: 0.0041\n",
      "Batch 80, Loss: 0.0940\n",
      "Batch 90, Loss: 0.2291\n",
      "Epoch: 235 | Train loss: 0.06393 | Test loss: 0.09886 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0080\n",
      "Batch 20, Loss: 0.1734\n",
      "Batch 30, Loss: 0.1494\n",
      "Batch 40, Loss: 0.0413\n",
      "Batch 50, Loss: 0.0682\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0114\n",
      "Batch 80, Loss: 0.0131\n",
      "Batch 90, Loss: 0.0013\n",
      "Epoch: 236 | Train loss: 0.05408 | Test loss: 0.11985 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0905\n",
      "Batch 10, Loss: 0.0052\n",
      "Batch 20, Loss: 0.3308\n",
      "Batch 30, Loss: 0.0393\n",
      "Batch 40, Loss: 0.0112\n",
      "Batch 50, Loss: 0.0792\n",
      "Batch 60, Loss: 0.0187\n",
      "Batch 70, Loss: 0.0334\n",
      "Batch 80, Loss: 0.0388\n",
      "Batch 90, Loss: 0.1180\n",
      "Epoch: 237 | Train loss: 0.05112 | Test loss: 0.04790 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0120\n",
      "Batch 10, Loss: 0.0003\n",
      "Batch 20, Loss: 0.2844\n",
      "Batch 30, Loss: 0.0310\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.0009\n",
      "Batch 60, Loss: 0.0110\n",
      "Batch 70, Loss: 0.0035\n",
      "Batch 80, Loss: 0.0007\n",
      "Batch 90, Loss: 0.0016\n",
      "Epoch: 238 | Train loss: 0.03010 | Test loss: 0.04071 | Test accuracy: 0.66237\n",
      "Batch 0, Loss: 0.0017\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0135\n",
      "Batch 30, Loss: 0.0140\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.2650\n",
      "Batch 70, Loss: 0.0082\n",
      "Batch 80, Loss: 0.0155\n",
      "Batch 90, Loss: 0.0036\n",
      "Epoch: 239 | Train loss: 0.04368 | Test loss: 1.34682 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0133\n",
      "Batch 10, Loss: 0.0709\n",
      "Batch 20, Loss: 0.0147\n",
      "Batch 30, Loss: 0.0038\n",
      "Batch 40, Loss: 0.1151\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0135\n",
      "Batch 70, Loss: 0.1782\n",
      "Batch 80, Loss: 0.0220\n",
      "Batch 90, Loss: 0.0604\n",
      "Epoch: 240 | Train loss: 0.06678 | Test loss: 0.09574 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0202\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.0309\n",
      "Batch 30, Loss: 0.0858\n",
      "Batch 40, Loss: 0.0174\n",
      "Batch 50, Loss: 0.0146\n",
      "Batch 60, Loss: 0.0090\n",
      "Batch 70, Loss: 0.0212\n",
      "Batch 80, Loss: 0.0054\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 241 | Train loss: 0.05733 | Test loss: 0.06099 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.1472\n",
      "Batch 10, Loss: 0.0143\n",
      "Batch 20, Loss: 0.3706\n",
      "Batch 30, Loss: 0.0181\n",
      "Batch 40, Loss: 0.3232\n",
      "Batch 50, Loss: 0.0066\n",
      "Batch 60, Loss: 0.1057\n",
      "Batch 70, Loss: 0.0588\n",
      "Batch 80, Loss: 0.0278\n",
      "Batch 90, Loss: 0.0445\n",
      "Epoch: 242 | Train loss: 0.04698 | Test loss: 0.90150 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1119\n",
      "Batch 10, Loss: 0.0885\n",
      "Batch 20, Loss: 0.0178\n",
      "Batch 30, Loss: 0.2151\n",
      "Batch 40, Loss: 0.0752\n",
      "Batch 50, Loss: 0.0040\n",
      "Batch 60, Loss: 0.0438\n",
      "Batch 70, Loss: 0.0420\n",
      "Batch 80, Loss: 0.0942\n",
      "Batch 90, Loss: 0.2600\n",
      "Epoch: 243 | Train loss: 0.05731 | Test loss: 0.72486 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0011\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.0056\n",
      "Batch 30, Loss: 0.0119\n",
      "Batch 40, Loss: 0.1060\n",
      "Batch 50, Loss: 0.0650\n",
      "Batch 60, Loss: 0.1760\n",
      "Batch 70, Loss: 0.0124\n",
      "Batch 80, Loss: 0.0283\n",
      "Batch 90, Loss: 0.0137\n",
      "Epoch: 244 | Train loss: 0.08087 | Test loss: 0.04119 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0398\n",
      "Batch 10, Loss: 0.0263\n",
      "Batch 20, Loss: 0.0291\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0215\n",
      "Batch 50, Loss: 0.0097\n",
      "Batch 60, Loss: 0.0922\n",
      "Batch 70, Loss: 0.0081\n",
      "Batch 80, Loss: 0.0021\n",
      "Batch 90, Loss: 0.0127\n",
      "Epoch: 245 | Train loss: 0.06089 | Test loss: 0.12183 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.7504\n",
      "Batch 10, Loss: 0.1674\n",
      "Batch 20, Loss: 0.0534\n",
      "Batch 30, Loss: 0.5367\n",
      "Batch 40, Loss: 0.0124\n",
      "Batch 50, Loss: 0.0686\n",
      "Batch 60, Loss: 0.0066\n",
      "Batch 70, Loss: 0.1005\n",
      "Batch 80, Loss: 0.0167\n",
      "Batch 90, Loss: 0.0353\n",
      "Epoch: 246 | Train loss: 0.08581 | Test loss: 0.10042 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0108\n",
      "Batch 10, Loss: 0.0127\n",
      "Batch 20, Loss: 0.1684\n",
      "Batch 30, Loss: 0.0509\n",
      "Batch 40, Loss: 0.1305\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0022\n",
      "Batch 70, Loss: 0.0322\n",
      "Batch 80, Loss: 0.0020\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 247 | Train loss: 0.04392 | Test loss: 0.10091 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0011\n",
      "Batch 10, Loss: 0.0094\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.0242\n",
      "Batch 40, Loss: 0.0516\n",
      "Batch 50, Loss: 0.0066\n",
      "Batch 60, Loss: 0.0024\n",
      "Batch 70, Loss: 0.0092\n",
      "Batch 80, Loss: 0.0018\n",
      "Batch 90, Loss: 0.0136\n",
      "Epoch: 248 | Train loss: 0.04456 | Test loss: 0.04608 | Test accuracy: 0.49227\n",
      "Batch 0, Loss: 0.0065\n",
      "Batch 10, Loss: 0.0615\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0069\n",
      "Batch 40, Loss: 0.4854\n",
      "Batch 50, Loss: 0.2026\n",
      "Batch 60, Loss: 0.2377\n",
      "Batch 70, Loss: 0.0491\n",
      "Batch 80, Loss: 0.0196\n",
      "Batch 90, Loss: 0.0163\n",
      "Epoch: 249 | Train loss: 0.07087 | Test loss: 0.06539 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0078\n",
      "Batch 10, Loss: 0.0292\n",
      "Batch 20, Loss: 0.0365\n",
      "Batch 30, Loss: 0.0269\n",
      "Batch 40, Loss: 0.0084\n",
      "Batch 50, Loss: 0.0090\n",
      "Batch 60, Loss: 0.2009\n",
      "Batch 70, Loss: 0.0092\n",
      "Batch 80, Loss: 0.0302\n",
      "Batch 90, Loss: 0.0444\n",
      "Epoch: 250 | Train loss: 0.07668 | Test loss: 0.06252 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0215\n",
      "Batch 10, Loss: 0.0053\n",
      "Batch 20, Loss: 0.0588\n",
      "Batch 30, Loss: 0.4429\n",
      "Batch 40, Loss: 0.0459\n",
      "Batch 50, Loss: 0.1443\n",
      "Batch 60, Loss: 0.0108\n",
      "Batch 70, Loss: 0.0351\n",
      "Batch 80, Loss: 0.0315\n",
      "Batch 90, Loss: 0.0192\n",
      "Epoch: 251 | Train loss: 0.08187 | Test loss: 1.39637 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2657\n",
      "Batch 10, Loss: 0.0108\n",
      "Batch 20, Loss: 0.0274\n",
      "Batch 30, Loss: 0.0080\n",
      "Batch 40, Loss: 0.0083\n",
      "Batch 50, Loss: 0.0796\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0022\n",
      "Batch 80, Loss: 0.0072\n",
      "Batch 90, Loss: 0.1899\n",
      "Epoch: 252 | Train loss: 0.06347 | Test loss: 0.05129 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0183\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.0035\n",
      "Batch 40, Loss: 0.0249\n",
      "Batch 50, Loss: 0.0133\n",
      "Batch 60, Loss: 0.0587\n",
      "Batch 70, Loss: 0.0505\n",
      "Batch 80, Loss: 0.0061\n",
      "Batch 90, Loss: 0.0048\n",
      "Epoch: 253 | Train loss: 0.04399 | Test loss: 0.04924 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0789\n",
      "Batch 10, Loss: 0.0182\n",
      "Batch 20, Loss: 0.0216\n",
      "Batch 30, Loss: 0.0751\n",
      "Batch 40, Loss: 0.0064\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0102\n",
      "Batch 70, Loss: 0.0071\n",
      "Batch 80, Loss: 0.0042\n",
      "Batch 90, Loss: 0.1385\n",
      "Epoch: 254 | Train loss: 0.02323 | Test loss: 0.08126 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0015\n",
      "Batch 10, Loss: 0.0256\n",
      "Batch 20, Loss: 0.0023\n",
      "Batch 30, Loss: 0.0150\n",
      "Batch 40, Loss: 0.0291\n",
      "Batch 50, Loss: 0.0268\n",
      "Batch 60, Loss: 0.0055\n",
      "Batch 70, Loss: 0.0508\n",
      "Batch 80, Loss: 0.0171\n",
      "Batch 90, Loss: 0.0460\n",
      "Epoch: 255 | Train loss: 0.06397 | Test loss: 0.11690 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0811\n",
      "Batch 10, Loss: 0.0543\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0152\n",
      "Batch 40, Loss: 0.0339\n",
      "Batch 50, Loss: 0.0206\n",
      "Batch 60, Loss: 0.0102\n",
      "Batch 70, Loss: 0.0102\n",
      "Batch 80, Loss: 0.0933\n",
      "Batch 90, Loss: 0.0041\n",
      "Epoch: 256 | Train loss: 0.05540 | Test loss: 0.15718 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0129\n",
      "Batch 10, Loss: 0.2565\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0571\n",
      "Batch 40, Loss: 0.0112\n",
      "Batch 50, Loss: 0.0398\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0024\n",
      "Batch 90, Loss: 0.0085\n",
      "Epoch: 257 | Train loss: 0.04555 | Test loss: 0.05520 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0498\n",
      "Batch 10, Loss: 0.0060\n",
      "Batch 20, Loss: 0.0070\n",
      "Batch 30, Loss: 0.6046\n",
      "Batch 40, Loss: 0.0498\n",
      "Batch 50, Loss: 0.1842\n",
      "Batch 60, Loss: 0.0213\n",
      "Batch 70, Loss: 0.0428\n",
      "Batch 80, Loss: 0.0491\n",
      "Batch 90, Loss: 0.0023\n",
      "Epoch: 258 | Train loss: 0.06371 | Test loss: 0.06206 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.1725\n",
      "Batch 10, Loss: 0.0057\n",
      "Batch 20, Loss: 0.0155\n",
      "Batch 30, Loss: 0.0225\n",
      "Batch 40, Loss: 0.0878\n",
      "Batch 50, Loss: 0.0565\n",
      "Batch 60, Loss: 0.0483\n",
      "Batch 70, Loss: 0.0093\n",
      "Batch 80, Loss: 0.0513\n",
      "Batch 90, Loss: 0.0448\n",
      "Epoch: 259 | Train loss: 0.06083 | Test loss: 2.59516 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0859\n",
      "Batch 10, Loss: 0.0019\n",
      "Batch 20, Loss: 0.0361\n",
      "Batch 30, Loss: 0.0730\n",
      "Batch 40, Loss: 0.0092\n",
      "Batch 50, Loss: 0.0560\n",
      "Batch 60, Loss: 0.0151\n",
      "Batch 70, Loss: 0.0081\n",
      "Batch 80, Loss: 0.0128\n",
      "Batch 90, Loss: 0.0065\n",
      "Epoch: 260 | Train loss: 0.06139 | Test loss: 0.04666 | Test accuracy: 0.56959\n",
      "Batch 0, Loss: 0.0162\n",
      "Batch 10, Loss: 0.0121\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.0168\n",
      "Batch 40, Loss: 0.0298\n",
      "Batch 50, Loss: 0.0008\n",
      "Batch 60, Loss: 0.0364\n",
      "Batch 70, Loss: 0.0307\n",
      "Batch 80, Loss: 0.0025\n",
      "Batch 90, Loss: 0.0778\n",
      "Epoch: 261 | Train loss: 0.06351 | Test loss: 0.11593 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0090\n",
      "Batch 10, Loss: 0.0233\n",
      "Batch 20, Loss: 0.1931\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.0134\n",
      "Batch 50, Loss: 0.0333\n",
      "Batch 60, Loss: 0.0189\n",
      "Batch 70, Loss: 0.0743\n",
      "Batch 80, Loss: 0.0016\n",
      "Batch 90, Loss: 0.2787\n",
      "Epoch: 262 | Train loss: 0.06898 | Test loss: 0.21640 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0146\n",
      "Batch 10, Loss: 0.0188\n",
      "Batch 20, Loss: 0.1161\n",
      "Batch 30, Loss: 0.0088\n",
      "Batch 40, Loss: 0.1945\n",
      "Batch 50, Loss: 0.0252\n",
      "Batch 60, Loss: 0.0049\n",
      "Batch 70, Loss: 0.0085\n",
      "Batch 80, Loss: 0.1679\n",
      "Batch 90, Loss: 0.0056\n",
      "Epoch: 263 | Train loss: 0.05451 | Test loss: 0.03916 | Test accuracy: 0.65464\n",
      "Batch 0, Loss: 0.0058\n",
      "Batch 10, Loss: 0.0169\n",
      "Batch 20, Loss: 0.0530\n",
      "Batch 30, Loss: 0.0265\n",
      "Batch 40, Loss: 0.0658\n",
      "Batch 50, Loss: 0.1766\n",
      "Batch 60, Loss: 0.0132\n",
      "Batch 70, Loss: 0.1879\n",
      "Batch 80, Loss: 0.0468\n",
      "Batch 90, Loss: 0.0618\n",
      "Epoch: 264 | Train loss: 0.06598 | Test loss: 1.34347 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0283\n",
      "Batch 10, Loss: 0.0554\n",
      "Batch 20, Loss: 0.0256\n",
      "Batch 30, Loss: 0.0153\n",
      "Batch 40, Loss: 0.1659\n",
      "Batch 50, Loss: 0.0075\n",
      "Batch 60, Loss: 0.0112\n",
      "Batch 70, Loss: 0.0211\n",
      "Batch 80, Loss: 0.0004\n",
      "Batch 90, Loss: 0.0198\n",
      "Epoch: 265 | Train loss: 0.06570 | Test loss: 0.05882 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0032\n",
      "Batch 10, Loss: 0.0184\n",
      "Batch 20, Loss: 0.0046\n",
      "Batch 30, Loss: 0.0077\n",
      "Batch 40, Loss: 0.0090\n",
      "Batch 50, Loss: 0.0016\n",
      "Batch 60, Loss: 0.0041\n",
      "Batch 70, Loss: 0.0025\n",
      "Batch 80, Loss: 0.0104\n",
      "Batch 90, Loss: 0.0157\n",
      "Epoch: 266 | Train loss: 0.03478 | Test loss: 0.09185 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0107\n",
      "Batch 10, Loss: 0.0099\n",
      "Batch 20, Loss: 0.0871\n",
      "Batch 30, Loss: 0.6838\n",
      "Batch 40, Loss: 0.0727\n",
      "Batch 50, Loss: 0.0075\n",
      "Batch 60, Loss: 0.0170\n",
      "Batch 70, Loss: 0.0216\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0080\n",
      "Epoch: 267 | Train loss: 0.05831 | Test loss: 0.04475 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.0410\n",
      "Batch 10, Loss: 0.0684\n",
      "Batch 20, Loss: 0.0147\n",
      "Batch 30, Loss: 0.1199\n",
      "Batch 40, Loss: 0.0004\n",
      "Batch 50, Loss: 0.0348\n",
      "Batch 60, Loss: 0.3395\n",
      "Batch 70, Loss: 0.1076\n",
      "Batch 80, Loss: 0.0469\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 268 | Train loss: 0.06794 | Test loss: 0.46961 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0248\n",
      "Batch 10, Loss: 0.0040\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.0173\n",
      "Batch 40, Loss: 0.0216\n",
      "Batch 50, Loss: 0.0283\n",
      "Batch 60, Loss: 0.0812\n",
      "Batch 70, Loss: 0.0983\n",
      "Batch 80, Loss: 0.0555\n",
      "Batch 90, Loss: 0.0071\n",
      "Epoch: 269 | Train loss: 0.06850 | Test loss: 0.22051 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0316\n",
      "Batch 10, Loss: 0.0844\n",
      "Batch 20, Loss: 0.0925\n",
      "Batch 30, Loss: 0.1610\n",
      "Batch 40, Loss: 0.0464\n",
      "Batch 50, Loss: 0.0091\n",
      "Batch 60, Loss: 0.0027\n",
      "Batch 70, Loss: 0.2139\n",
      "Batch 80, Loss: 0.0364\n",
      "Batch 90, Loss: 0.0215\n",
      "Epoch: 270 | Train loss: 0.08653 | Test loss: 0.06407 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0036\n",
      "Batch 10, Loss: 0.0876\n",
      "Batch 20, Loss: 0.0142\n",
      "Batch 30, Loss: 0.0412\n",
      "Batch 40, Loss: 0.0127\n",
      "Batch 50, Loss: 0.0411\n",
      "Batch 60, Loss: 0.0087\n",
      "Batch 70, Loss: 0.0080\n",
      "Batch 80, Loss: 0.0059\n",
      "Batch 90, Loss: 0.0344\n",
      "Epoch: 271 | Train loss: 0.05586 | Test loss: 0.04521 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0143\n",
      "Batch 10, Loss: 0.0743\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0143\n",
      "Batch 40, Loss: 0.0204\n",
      "Batch 50, Loss: 0.0120\n",
      "Batch 60, Loss: 0.0233\n",
      "Batch 70, Loss: 0.0102\n",
      "Batch 80, Loss: 0.1362\n",
      "Batch 90, Loss: 0.0494\n",
      "Epoch: 272 | Train loss: 0.04054 | Test loss: 0.04609 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.1223\n",
      "Batch 10, Loss: 0.0243\n",
      "Batch 20, Loss: 0.0062\n",
      "Batch 30, Loss: 0.0162\n",
      "Batch 40, Loss: 0.0094\n",
      "Batch 50, Loss: 0.3441\n",
      "Batch 60, Loss: 0.0111\n",
      "Batch 70, Loss: 0.0972\n",
      "Batch 80, Loss: 0.0888\n",
      "Batch 90, Loss: 0.0346\n",
      "Epoch: 273 | Train loss: 0.05351 | Test loss: 0.07053 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0083\n",
      "Batch 10, Loss: 0.0067\n",
      "Batch 20, Loss: 0.1956\n",
      "Batch 30, Loss: 0.0140\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0095\n",
      "Batch 60, Loss: 0.0284\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0146\n",
      "Batch 90, Loss: 0.0047\n",
      "Epoch: 274 | Train loss: 0.03506 | Test loss: 0.19351 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0087\n",
      "Batch 10, Loss: 0.0090\n",
      "Batch 20, Loss: 0.0062\n",
      "Batch 30, Loss: 0.0188\n",
      "Batch 40, Loss: 0.0072\n",
      "Batch 50, Loss: 0.0271\n",
      "Batch 60, Loss: 0.0089\n",
      "Batch 70, Loss: 0.0130\n",
      "Batch 80, Loss: 0.0157\n",
      "Batch 90, Loss: 0.0035\n",
      "Epoch: 275 | Train loss: 0.03826 | Test loss: 0.05725 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0723\n",
      "Batch 10, Loss: 0.0133\n",
      "Batch 20, Loss: 0.0149\n",
      "Batch 30, Loss: 0.0701\n",
      "Batch 40, Loss: 0.0073\n",
      "Batch 50, Loss: 0.0138\n",
      "Batch 60, Loss: 0.1225\n",
      "Batch 70, Loss: 0.0342\n",
      "Batch 80, Loss: 0.0027\n",
      "Batch 90, Loss: 0.0355\n",
      "Epoch: 276 | Train loss: 0.05559 | Test loss: 0.03820 | Test accuracy: 0.56959\n",
      "Batch 0, Loss: 0.0873\n",
      "Batch 10, Loss: 0.0195\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0075\n",
      "Batch 40, Loss: 0.0732\n",
      "Batch 50, Loss: 0.0191\n",
      "Batch 60, Loss: 0.0528\n",
      "Batch 70, Loss: 0.0162\n",
      "Batch 80, Loss: 0.0162\n",
      "Batch 90, Loss: 0.0118\n",
      "Epoch: 277 | Train loss: 0.05215 | Test loss: 0.08521 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0192\n",
      "Batch 10, Loss: 0.0126\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.0160\n",
      "Batch 60, Loss: 0.1608\n",
      "Batch 70, Loss: 0.0468\n",
      "Batch 80, Loss: 0.0095\n",
      "Batch 90, Loss: 0.0141\n",
      "Epoch: 278 | Train loss: 0.03560 | Test loss: 0.05098 | Test accuracy: 0.57216\n",
      "Batch 0, Loss: 0.0314\n",
      "Batch 10, Loss: 0.0906\n",
      "Batch 20, Loss: 0.0203\n",
      "Batch 30, Loss: 0.0104\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0738\n",
      "Batch 60, Loss: 0.2509\n",
      "Batch 70, Loss: 0.0352\n",
      "Batch 80, Loss: 0.0340\n",
      "Batch 90, Loss: 0.0095\n",
      "Epoch: 279 | Train loss: 0.07956 | Test loss: 0.29757 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.1941\n",
      "Batch 10, Loss: 0.0039\n",
      "Batch 20, Loss: 0.0138\n",
      "Batch 30, Loss: 0.0122\n",
      "Batch 40, Loss: 0.0910\n",
      "Batch 50, Loss: 0.0084\n",
      "Batch 60, Loss: 0.0107\n",
      "Batch 70, Loss: 0.0240\n",
      "Batch 80, Loss: 0.1453\n",
      "Batch 90, Loss: 0.0167\n",
      "Epoch: 280 | Train loss: 0.06200 | Test loss: 2.72481 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0172\n",
      "Batch 10, Loss: 0.1392\n",
      "Batch 20, Loss: 0.0064\n",
      "Batch 30, Loss: 0.1233\n",
      "Batch 40, Loss: 0.0663\n",
      "Batch 50, Loss: 0.0200\n",
      "Batch 60, Loss: 0.1856\n",
      "Batch 70, Loss: 0.1388\n",
      "Batch 80, Loss: 0.0088\n",
      "Batch 90, Loss: 0.0141\n",
      "Epoch: 281 | Train loss: 0.06390 | Test loss: 0.23884 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0366\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0038\n",
      "Batch 30, Loss: 0.0105\n",
      "Batch 40, Loss: 0.0312\n",
      "Batch 50, Loss: 0.1267\n",
      "Batch 60, Loss: 0.1386\n",
      "Batch 70, Loss: 0.0114\n",
      "Batch 80, Loss: 0.0097\n",
      "Batch 90, Loss: 0.0138\n",
      "Epoch: 282 | Train loss: 0.04068 | Test loss: 0.04538 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0617\n",
      "Batch 10, Loss: 0.0243\n",
      "Batch 20, Loss: 0.0097\n",
      "Batch 30, Loss: 0.1046\n",
      "Batch 40, Loss: 0.0047\n",
      "Batch 50, Loss: 0.2178\n",
      "Batch 60, Loss: 0.1796\n",
      "Batch 70, Loss: 0.0556\n",
      "Batch 80, Loss: 0.0110\n",
      "Batch 90, Loss: 0.0162\n",
      "Epoch: 283 | Train loss: 0.05400 | Test loss: 0.04459 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0088\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0058\n",
      "Batch 30, Loss: 0.1117\n",
      "Batch 40, Loss: 0.3268\n",
      "Batch 50, Loss: 0.0536\n",
      "Batch 60, Loss: 0.1102\n",
      "Batch 70, Loss: 0.0027\n",
      "Batch 80, Loss: 0.1460\n",
      "Batch 90, Loss: 0.1737\n",
      "Epoch: 284 | Train loss: 0.05002 | Test loss: 0.46716 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0109\n",
      "Batch 10, Loss: 0.0482\n",
      "Batch 20, Loss: 0.0265\n",
      "Batch 30, Loss: 0.0119\n",
      "Batch 40, Loss: 0.0406\n",
      "Batch 50, Loss: 0.0008\n",
      "Batch 60, Loss: 0.0090\n",
      "Batch 70, Loss: 0.2168\n",
      "Batch 80, Loss: 0.0299\n",
      "Batch 90, Loss: 0.0065\n",
      "Epoch: 285 | Train loss: 0.04037 | Test loss: 0.53600 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0058\n",
      "Batch 10, Loss: 0.0599\n",
      "Batch 20, Loss: 0.2520\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.0609\n",
      "Batch 50, Loss: 0.0041\n",
      "Batch 60, Loss: 0.0269\n",
      "Batch 70, Loss: 0.0113\n",
      "Batch 80, Loss: 0.2018\n",
      "Batch 90, Loss: 0.0116\n",
      "Epoch: 286 | Train loss: 0.05796 | Test loss: 0.18351 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1622\n",
      "Batch 10, Loss: 0.0570\n",
      "Batch 20, Loss: 0.0055\n",
      "Batch 30, Loss: 0.1489\n",
      "Batch 40, Loss: 0.2509\n",
      "Batch 50, Loss: 0.0519\n",
      "Batch 60, Loss: 0.0105\n",
      "Batch 70, Loss: 0.0216\n",
      "Batch 80, Loss: 0.0132\n",
      "Batch 90, Loss: 0.0067\n",
      "Epoch: 287 | Train loss: 0.03345 | Test loss: 0.06037 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1180\n",
      "Batch 10, Loss: 0.0034\n",
      "Batch 20, Loss: 0.1387\n",
      "Batch 30, Loss: 0.0132\n",
      "Batch 40, Loss: 0.0066\n",
      "Batch 50, Loss: 0.1844\n",
      "Batch 60, Loss: 0.0079\n",
      "Batch 70, Loss: 0.0025\n",
      "Batch 80, Loss: 0.0023\n",
      "Batch 90, Loss: 0.0104\n",
      "Epoch: 288 | Train loss: 0.04128 | Test loss: 0.05794 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0005\n",
      "Batch 10, Loss: 0.1504\n",
      "Batch 20, Loss: 0.0531\n",
      "Batch 30, Loss: 0.0052\n",
      "Batch 40, Loss: 0.0333\n",
      "Batch 50, Loss: 0.0089\n",
      "Batch 60, Loss: 0.0093\n",
      "Batch 70, Loss: 0.0189\n",
      "Batch 80, Loss: 0.0061\n",
      "Batch 90, Loss: 0.0113\n",
      "Epoch: 289 | Train loss: 0.05156 | Test loss: 0.20493 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0393\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.1860\n",
      "Batch 30, Loss: 0.0989\n",
      "Batch 40, Loss: 0.0341\n",
      "Batch 50, Loss: 0.6514\n",
      "Batch 60, Loss: 0.0264\n",
      "Batch 70, Loss: 0.0142\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0107\n",
      "Epoch: 290 | Train loss: 0.06499 | Test loss: 0.05623 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0295\n",
      "Batch 20, Loss: 0.0134\n",
      "Batch 30, Loss: 0.0669\n",
      "Batch 40, Loss: 0.0939\n",
      "Batch 50, Loss: 0.0265\n",
      "Batch 60, Loss: 0.0304\n",
      "Batch 70, Loss: 0.0065\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0123\n",
      "Epoch: 291 | Train loss: 0.04449 | Test loss: 0.07575 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0562\n",
      "Batch 10, Loss: 0.0029\n",
      "Batch 20, Loss: 0.0193\n",
      "Batch 30, Loss: 0.7667\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.8886\n",
      "Batch 60, Loss: 0.0470\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0100\n",
      "Batch 90, Loss: 0.0119\n",
      "Epoch: 292 | Train loss: 0.05074 | Test loss: 0.28115 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0050\n",
      "Batch 10, Loss: 0.0743\n",
      "Batch 20, Loss: 0.0132\n",
      "Batch 30, Loss: 0.0117\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.1666\n",
      "Batch 60, Loss: 0.0096\n",
      "Batch 70, Loss: 0.0116\n",
      "Batch 80, Loss: 0.4365\n",
      "Batch 90, Loss: 0.3030\n",
      "Epoch: 293 | Train loss: 0.04891 | Test loss: 0.79572 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0311\n",
      "Batch 10, Loss: 0.0832\n",
      "Batch 20, Loss: 0.0041\n",
      "Batch 30, Loss: 0.1985\n",
      "Batch 40, Loss: 0.0099\n",
      "Batch 50, Loss: 0.0123\n",
      "Batch 60, Loss: 0.0203\n",
      "Batch 70, Loss: 0.0105\n",
      "Batch 80, Loss: 0.0111\n",
      "Batch 90, Loss: 0.0138\n",
      "Epoch: 294 | Train loss: 0.04003 | Test loss: 0.15005 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0114\n",
      "Batch 10, Loss: 0.0277\n",
      "Batch 20, Loss: 0.0711\n",
      "Batch 30, Loss: 0.0112\n",
      "Batch 40, Loss: 0.0028\n",
      "Batch 50, Loss: 0.1083\n",
      "Batch 60, Loss: 0.0021\n",
      "Batch 70, Loss: 0.0298\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0110\n",
      "Epoch: 295 | Train loss: 0.04634 | Test loss: 0.05120 | Test accuracy: 0.43814\n",
      "Batch 0, Loss: 0.0579\n",
      "Batch 10, Loss: 0.0055\n",
      "Batch 20, Loss: 0.0499\n",
      "Batch 30, Loss: 0.0619\n",
      "Batch 40, Loss: 0.2773\n",
      "Batch 50, Loss: 0.0036\n",
      "Batch 60, Loss: 0.0967\n",
      "Batch 70, Loss: 0.0168\n",
      "Batch 80, Loss: 0.2143\n",
      "Batch 90, Loss: 0.0089\n",
      "Epoch: 296 | Train loss: 0.04198 | Test loss: 0.24730 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0048\n",
      "Batch 10, Loss: 0.0372\n",
      "Batch 20, Loss: 0.4852\n",
      "Batch 30, Loss: 0.6343\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0157\n",
      "Batch 60, Loss: 0.0367\n",
      "Batch 70, Loss: 0.0083\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0151\n",
      "Epoch: 297 | Train loss: 0.06575 | Test loss: 0.04408 | Test accuracy: 0.57216\n",
      "Batch 0, Loss: 0.0632\n",
      "Batch 10, Loss: 0.0139\n",
      "Batch 20, Loss: 0.0110\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0011\n",
      "Batch 50, Loss: 0.0361\n",
      "Batch 60, Loss: 0.2240\n",
      "Batch 70, Loss: 0.4138\n",
      "Batch 80, Loss: 0.0770\n",
      "Batch 90, Loss: 0.0429\n",
      "Epoch: 298 | Train loss: 0.05792 | Test loss: 0.04151 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0205\n",
      "Batch 10, Loss: 0.0049\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0241\n",
      "Batch 40, Loss: 0.0927\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.0158\n",
      "Batch 70, Loss: 0.0038\n",
      "Batch 80, Loss: 0.0273\n",
      "Batch 90, Loss: 0.0056\n",
      "Epoch: 299 | Train loss: 0.05104 | Test loss: 0.14942 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0423\n",
      "Batch 10, Loss: 0.0100\n",
      "Batch 20, Loss: 0.1523\n",
      "Batch 30, Loss: 0.1036\n",
      "Batch 40, Loss: 0.0089\n",
      "Batch 50, Loss: 0.0072\n",
      "Batch 60, Loss: 0.0041\n",
      "Batch 70, Loss: 0.0225\n",
      "Batch 80, Loss: 0.0305\n",
      "Batch 90, Loss: 0.0010\n",
      "Epoch: 300 | Train loss: 0.05575 | Test loss: 0.15086 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.0114\n",
      "Batch 20, Loss: 0.0047\n",
      "Batch 30, Loss: 0.0246\n",
      "Batch 40, Loss: 0.0400\n",
      "Batch 50, Loss: 0.0297\n",
      "Batch 60, Loss: 0.0061\n",
      "Batch 70, Loss: 0.0043\n",
      "Batch 80, Loss: 0.1271\n",
      "Batch 90, Loss: 0.0053\n",
      "Epoch: 301 | Train loss: 0.02351 | Test loss: 0.76959 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2221\n",
      "Batch 10, Loss: 0.0028\n",
      "Batch 20, Loss: 0.0047\n",
      "Batch 30, Loss: 0.0114\n",
      "Batch 40, Loss: 0.0259\n",
      "Batch 50, Loss: 0.0353\n",
      "Batch 60, Loss: 0.0791\n",
      "Batch 70, Loss: 0.0087\n",
      "Batch 80, Loss: 0.0094\n",
      "Batch 90, Loss: 0.0098\n",
      "Epoch: 302 | Train loss: 0.05238 | Test loss: 0.83407 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0057\n",
      "Batch 10, Loss: 0.1273\n",
      "Batch 20, Loss: 0.0200\n",
      "Batch 30, Loss: 0.0141\n",
      "Batch 40, Loss: 0.0098\n",
      "Batch 50, Loss: 0.0244\n",
      "Batch 60, Loss: 0.1355\n",
      "Batch 70, Loss: 0.0618\n",
      "Batch 80, Loss: 0.0732\n",
      "Batch 90, Loss: 0.0249\n",
      "Epoch: 303 | Train loss: 0.06362 | Test loss: 0.95484 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0111\n",
      "Batch 20, Loss: 0.0312\n",
      "Batch 30, Loss: 0.3742\n",
      "Batch 40, Loss: 0.0136\n",
      "Batch 50, Loss: 0.1988\n",
      "Batch 60, Loss: 0.0057\n",
      "Batch 70, Loss: 0.0035\n",
      "Batch 80, Loss: 0.0512\n",
      "Batch 90, Loss: 0.0054\n",
      "Epoch: 304 | Train loss: 0.06399 | Test loss: 0.14638 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0224\n",
      "Batch 20, Loss: 0.0160\n",
      "Batch 30, Loss: 0.0102\n",
      "Batch 40, Loss: 0.0209\n",
      "Batch 50, Loss: 0.0087\n",
      "Batch 60, Loss: 0.0607\n",
      "Batch 70, Loss: 0.1464\n",
      "Batch 80, Loss: 0.0067\n",
      "Batch 90, Loss: 0.0350\n",
      "Epoch: 305 | Train loss: 0.05379 | Test loss: 0.12587 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.0668\n",
      "Batch 20, Loss: 0.0564\n",
      "Batch 30, Loss: 0.2183\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.0264\n",
      "Batch 60, Loss: 0.0486\n",
      "Batch 70, Loss: 0.0041\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 306 | Train loss: 0.02565 | Test loss: 0.07589 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0152\n",
      "Batch 10, Loss: 0.0295\n",
      "Batch 20, Loss: 0.0334\n",
      "Batch 30, Loss: 0.0070\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0041\n",
      "Batch 60, Loss: 0.0699\n",
      "Batch 70, Loss: 0.0149\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 307 | Train loss: 0.03464 | Test loss: 0.32389 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0093\n",
      "Batch 10, Loss: 0.0019\n",
      "Batch 20, Loss: 0.0086\n",
      "Batch 30, Loss: 0.0302\n",
      "Batch 40, Loss: 0.0039\n",
      "Batch 50, Loss: 0.0354\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0553\n",
      "Batch 80, Loss: 0.0090\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 308 | Train loss: 0.04807 | Test loss: 0.05372 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0309\n",
      "Batch 10, Loss: 0.0540\n",
      "Batch 20, Loss: 0.0112\n",
      "Batch 30, Loss: 0.0201\n",
      "Batch 40, Loss: 0.0184\n",
      "Batch 50, Loss: 0.0062\n",
      "Batch 60, Loss: 0.0939\n",
      "Batch 70, Loss: 0.0061\n",
      "Batch 80, Loss: 0.0163\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 309 | Train loss: 0.04863 | Test loss: 0.30532 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0186\n",
      "Batch 10, Loss: 0.0021\n",
      "Batch 20, Loss: 0.0167\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.0125\n",
      "Batch 50, Loss: 0.0027\n",
      "Batch 60, Loss: 0.0627\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0136\n",
      "Batch 90, Loss: 0.0055\n",
      "Epoch: 310 | Train loss: 0.03248 | Test loss: 0.06974 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0613\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0033\n",
      "Batch 40, Loss: 0.1378\n",
      "Batch 50, Loss: 0.2384\n",
      "Batch 60, Loss: 0.1388\n",
      "Batch 70, Loss: 0.1275\n",
      "Batch 80, Loss: 0.0670\n",
      "Batch 90, Loss: 0.0853\n",
      "Epoch: 311 | Train loss: 0.04251 | Test loss: 0.04421 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0085\n",
      "Batch 20, Loss: 0.0115\n",
      "Batch 30, Loss: 0.0014\n",
      "Batch 40, Loss: 0.0238\n",
      "Batch 50, Loss: 0.0675\n",
      "Batch 60, Loss: 0.0093\n",
      "Batch 70, Loss: 0.0044\n",
      "Batch 80, Loss: 0.0309\n",
      "Batch 90, Loss: 0.0974\n",
      "Epoch: 312 | Train loss: 0.05004 | Test loss: 0.11385 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0057\n",
      "Batch 20, Loss: 0.1697\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0174\n",
      "Batch 50, Loss: 0.1500\n",
      "Batch 60, Loss: 0.0286\n",
      "Batch 70, Loss: 0.6655\n",
      "Batch 80, Loss: 0.2276\n",
      "Batch 90, Loss: 0.1190\n",
      "Epoch: 313 | Train loss: 0.06723 | Test loss: 0.04096 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0153\n",
      "Batch 10, Loss: 0.0044\n",
      "Batch 20, Loss: 0.0449\n",
      "Batch 30, Loss: 0.0018\n",
      "Batch 40, Loss: 0.0234\n",
      "Batch 50, Loss: 0.1208\n",
      "Batch 60, Loss: 0.0011\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0236\n",
      "Epoch: 314 | Train loss: 0.04445 | Test loss: 0.06131 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0881\n",
      "Batch 10, Loss: 0.0230\n",
      "Batch 20, Loss: 0.2038\n",
      "Batch 30, Loss: 0.0049\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0004\n",
      "Batch 70, Loss: 0.0100\n",
      "Batch 80, Loss: 0.2953\n",
      "Batch 90, Loss: 0.0359\n",
      "Epoch: 315 | Train loss: 0.04369 | Test loss: 0.04518 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0066\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0005\n",
      "Batch 30, Loss: 0.0082\n",
      "Batch 40, Loss: 0.0585\n",
      "Batch 50, Loss: 0.0059\n",
      "Batch 60, Loss: 0.0208\n",
      "Batch 70, Loss: 0.0195\n",
      "Batch 80, Loss: 0.0060\n",
      "Batch 90, Loss: 0.0260\n",
      "Epoch: 316 | Train loss: 0.04246 | Test loss: 0.04802 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0447\n",
      "Batch 10, Loss: 0.0209\n",
      "Batch 20, Loss: 0.1218\n",
      "Batch 30, Loss: 0.0325\n",
      "Batch 40, Loss: 0.0160\n",
      "Batch 50, Loss: 0.0258\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0303\n",
      "Batch 80, Loss: 0.1633\n",
      "Batch 90, Loss: 0.2279\n",
      "Epoch: 317 | Train loss: 0.06883 | Test loss: 0.13119 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.6317\n",
      "Batch 10, Loss: 0.0339\n",
      "Batch 20, Loss: 0.0155\n",
      "Batch 30, Loss: 0.0942\n",
      "Batch 40, Loss: 0.0087\n",
      "Batch 50, Loss: 0.0045\n",
      "Batch 60, Loss: 0.0168\n",
      "Batch 70, Loss: 0.0724\n",
      "Batch 80, Loss: 0.2140\n",
      "Batch 90, Loss: 0.0040\n",
      "Epoch: 318 | Train loss: 0.05394 | Test loss: 0.04716 | Test accuracy: 0.56701\n",
      "Batch 0, Loss: 0.1856\n",
      "Batch 10, Loss: 0.0331\n",
      "Batch 20, Loss: 0.0170\n",
      "Batch 30, Loss: 0.0146\n",
      "Batch 40, Loss: 0.0339\n",
      "Batch 50, Loss: 0.0377\n",
      "Batch 60, Loss: 0.0346\n",
      "Batch 70, Loss: 0.0152\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.2329\n",
      "Epoch: 319 | Train loss: 0.04493 | Test loss: 1.54797 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0088\n",
      "Batch 10, Loss: 0.2007\n",
      "Batch 20, Loss: 0.2056\n",
      "Batch 30, Loss: 0.0086\n",
      "Batch 40, Loss: 0.0106\n",
      "Batch 50, Loss: 0.0272\n",
      "Batch 60, Loss: 0.0185\n",
      "Batch 70, Loss: 0.0041\n",
      "Batch 80, Loss: 0.3966\n",
      "Batch 90, Loss: 0.0242\n",
      "Epoch: 320 | Train loss: 0.07095 | Test loss: 0.06967 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0638\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0090\n",
      "Batch 30, Loss: 0.0294\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.7931\n",
      "Batch 60, Loss: 0.1217\n",
      "Batch 70, Loss: 0.0307\n",
      "Batch 80, Loss: 0.0056\n",
      "Batch 90, Loss: 0.0211\n",
      "Epoch: 321 | Train loss: 0.04041 | Test loss: 0.54327 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0115\n",
      "Batch 10, Loss: 0.0199\n",
      "Batch 20, Loss: 0.0082\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.3847\n",
      "Batch 50, Loss: 0.3565\n",
      "Batch 60, Loss: 0.2960\n",
      "Batch 70, Loss: 0.0232\n",
      "Batch 80, Loss: 0.0141\n",
      "Batch 90, Loss: 0.0461\n",
      "Epoch: 322 | Train loss: 0.05019 | Test loss: 0.04936 | Test accuracy: 0.56701\n",
      "Batch 0, Loss: 0.0101\n",
      "Batch 10, Loss: 0.0348\n",
      "Batch 20, Loss: 0.0173\n",
      "Batch 30, Loss: 0.0212\n",
      "Batch 40, Loss: 0.0033\n",
      "Batch 50, Loss: 0.0108\n",
      "Batch 60, Loss: 0.0413\n",
      "Batch 70, Loss: 0.0683\n",
      "Batch 80, Loss: 0.0117\n",
      "Batch 90, Loss: 0.0116\n",
      "Epoch: 323 | Train loss: 0.02410 | Test loss: 0.04893 | Test accuracy: 0.50773\n",
      "Batch 0, Loss: 0.0049\n",
      "Batch 10, Loss: 0.0126\n",
      "Batch 20, Loss: 0.0184\n",
      "Batch 30, Loss: 0.0593\n",
      "Batch 40, Loss: 0.0034\n",
      "Batch 50, Loss: 0.0123\n",
      "Batch 60, Loss: 0.0358\n",
      "Batch 70, Loss: 0.1091\n",
      "Batch 80, Loss: 0.0105\n",
      "Batch 90, Loss: 0.0255\n",
      "Epoch: 324 | Train loss: 0.04861 | Test loss: 0.04950 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0233\n",
      "Batch 10, Loss: 0.0264\n",
      "Batch 20, Loss: 0.0265\n",
      "Batch 30, Loss: 0.0317\n",
      "Batch 40, Loss: 0.0254\n",
      "Batch 50, Loss: 0.0663\n",
      "Batch 60, Loss: 0.0043\n",
      "Batch 70, Loss: 0.0989\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0160\n",
      "Epoch: 325 | Train loss: 0.05772 | Test loss: 0.05042 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.0325\n",
      "Batch 10, Loss: 0.0129\n",
      "Batch 20, Loss: 0.1020\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0279\n",
      "Batch 50, Loss: 0.0337\n",
      "Batch 60, Loss: 0.0427\n",
      "Batch 70, Loss: 0.0344\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 326 | Train loss: 0.03381 | Test loss: 0.15772 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0142\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0107\n",
      "Batch 30, Loss: 0.0635\n",
      "Batch 40, Loss: 0.0126\n",
      "Batch 50, Loss: 0.0021\n",
      "Batch 60, Loss: 0.0222\n",
      "Batch 70, Loss: 0.0509\n",
      "Batch 80, Loss: 0.0161\n",
      "Batch 90, Loss: 0.1168\n",
      "Epoch: 327 | Train loss: 0.03322 | Test loss: 0.37068 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.4847\n",
      "Batch 10, Loss: 0.0123\n",
      "Batch 20, Loss: 0.0237\n",
      "Batch 30, Loss: 0.1174\n",
      "Batch 40, Loss: 0.0022\n",
      "Batch 50, Loss: 0.0389\n",
      "Batch 60, Loss: 0.0232\n",
      "Batch 70, Loss: 0.0160\n",
      "Batch 80, Loss: 0.1630\n",
      "Batch 90, Loss: 0.0048\n",
      "Epoch: 328 | Train loss: 0.05571 | Test loss: 0.15573 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0132\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.0167\n",
      "Batch 40, Loss: 0.1606\n",
      "Batch 50, Loss: 0.0062\n",
      "Batch 60, Loss: 0.0148\n",
      "Batch 70, Loss: 0.0136\n",
      "Batch 80, Loss: 0.0537\n",
      "Batch 90, Loss: 0.0247\n",
      "Epoch: 329 | Train loss: 0.05683 | Test loss: 0.04863 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0182\n",
      "Batch 20, Loss: 0.0002\n",
      "Batch 30, Loss: 0.0121\n",
      "Batch 40, Loss: 0.0235\n",
      "Batch 50, Loss: 0.0110\n",
      "Batch 60, Loss: 0.0033\n",
      "Batch 70, Loss: 0.0038\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 330 | Train loss: 0.03135 | Test loss: 0.34406 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0239\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.2568\n",
      "Batch 40, Loss: 0.0629\n",
      "Batch 50, Loss: 0.1532\n",
      "Batch 60, Loss: 0.0817\n",
      "Batch 70, Loss: 0.0505\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0258\n",
      "Epoch: 331 | Train loss: 0.07155 | Test loss: 0.04637 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0239\n",
      "Batch 10, Loss: 0.1356\n",
      "Batch 20, Loss: 0.1227\n",
      "Batch 30, Loss: 0.1544\n",
      "Batch 40, Loss: 0.0197\n",
      "Batch 50, Loss: 0.0299\n",
      "Batch 60, Loss: 0.0206\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0079\n",
      "Batch 90, Loss: 0.0055\n",
      "Epoch: 332 | Train loss: 0.06133 | Test loss: 0.17851 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0056\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0097\n",
      "Batch 40, Loss: 0.0169\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.4279\n",
      "Batch 70, Loss: 0.0247\n",
      "Batch 80, Loss: 0.0264\n",
      "Batch 90, Loss: 0.0068\n",
      "Epoch: 333 | Train loss: 0.04075 | Test loss: 0.06480 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.4271\n",
      "Batch 10, Loss: 0.0348\n",
      "Batch 20, Loss: 0.0424\n",
      "Batch 30, Loss: 0.1035\n",
      "Batch 40, Loss: 0.0175\n",
      "Batch 50, Loss: 0.0377\n",
      "Batch 60, Loss: 0.0191\n",
      "Batch 70, Loss: 0.0228\n",
      "Batch 80, Loss: 0.0064\n",
      "Batch 90, Loss: 0.0126\n",
      "Epoch: 334 | Train loss: 0.06971 | Test loss: 0.06845 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0180\n",
      "Batch 10, Loss: 0.0024\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0332\n",
      "Batch 50, Loss: 0.1213\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0130\n",
      "Batch 80, Loss: 0.0847\n",
      "Batch 90, Loss: 0.0208\n",
      "Epoch: 335 | Train loss: 0.05701 | Test loss: 0.06429 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1075\n",
      "Batch 10, Loss: 0.0101\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.2431\n",
      "Batch 40, Loss: 0.0574\n",
      "Batch 50, Loss: 0.0119\n",
      "Batch 60, Loss: 0.0018\n",
      "Batch 70, Loss: 0.0074\n",
      "Batch 80, Loss: 0.0052\n",
      "Batch 90, Loss: 0.0762\n",
      "Epoch: 336 | Train loss: 0.04235 | Test loss: 0.08650 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0274\n",
      "Batch 10, Loss: 0.0476\n",
      "Batch 20, Loss: 0.0037\n",
      "Batch 30, Loss: 0.0040\n",
      "Batch 40, Loss: 0.0713\n",
      "Batch 50, Loss: 0.0164\n",
      "Batch 60, Loss: 0.0127\n",
      "Batch 70, Loss: 0.0514\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0923\n",
      "Epoch: 337 | Train loss: 0.04331 | Test loss: 0.11538 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0094\n",
      "Batch 10, Loss: 0.1162\n",
      "Batch 20, Loss: 0.0135\n",
      "Batch 30, Loss: 0.0100\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0043\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0208\n",
      "Batch 80, Loss: 0.0322\n",
      "Batch 90, Loss: 0.0922\n",
      "Epoch: 338 | Train loss: 0.05564 | Test loss: 0.22718 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0553\n",
      "Batch 10, Loss: 0.0151\n",
      "Batch 20, Loss: 0.0850\n",
      "Batch 30, Loss: 0.0470\n",
      "Batch 40, Loss: 0.0019\n",
      "Batch 50, Loss: 0.0176\n",
      "Batch 60, Loss: 0.3131\n",
      "Batch 70, Loss: 0.0426\n",
      "Batch 80, Loss: 0.0247\n",
      "Batch 90, Loss: 0.0042\n",
      "Epoch: 339 | Train loss: 0.05573 | Test loss: 0.05942 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0467\n",
      "Batch 10, Loss: 0.0194\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0119\n",
      "Batch 50, Loss: 0.0047\n",
      "Batch 60, Loss: 0.0851\n",
      "Batch 70, Loss: 0.0236\n",
      "Batch 80, Loss: 0.0285\n",
      "Batch 90, Loss: 0.0012\n",
      "Epoch: 340 | Train loss: 0.03001 | Test loss: 0.39762 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0054\n",
      "Batch 10, Loss: 0.0021\n",
      "Batch 20, Loss: 0.0136\n",
      "Batch 30, Loss: 0.0270\n",
      "Batch 40, Loss: 0.0431\n",
      "Batch 50, Loss: 0.0205\n",
      "Batch 60, Loss: 0.0334\n",
      "Batch 70, Loss: 0.0176\n",
      "Batch 80, Loss: 0.0432\n",
      "Batch 90, Loss: 0.0294\n",
      "Epoch: 341 | Train loss: 0.07622 | Test loss: 0.14547 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0824\n",
      "Batch 10, Loss: 0.0201\n",
      "Batch 20, Loss: 0.3520\n",
      "Batch 30, Loss: 0.0066\n",
      "Batch 40, Loss: 0.0114\n",
      "Batch 50, Loss: 0.0122\n",
      "Batch 60, Loss: 0.0431\n",
      "Batch 70, Loss: 0.0294\n",
      "Batch 80, Loss: 0.0023\n",
      "Batch 90, Loss: 0.0252\n",
      "Epoch: 342 | Train loss: 0.03759 | Test loss: 0.17232 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0843\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0069\n",
      "Batch 30, Loss: 0.0165\n",
      "Batch 40, Loss: 0.0141\n",
      "Batch 50, Loss: 0.1348\n",
      "Batch 60, Loss: 0.1747\n",
      "Batch 70, Loss: 0.0115\n",
      "Batch 80, Loss: 0.0273\n",
      "Batch 90, Loss: 0.0762\n",
      "Epoch: 343 | Train loss: 0.04180 | Test loss: 0.07769 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0106\n",
      "Batch 10, Loss: 0.0167\n",
      "Batch 20, Loss: 0.0156\n",
      "Batch 30, Loss: 0.0014\n",
      "Batch 40, Loss: 0.0540\n",
      "Batch 50, Loss: 0.5028\n",
      "Batch 60, Loss: 0.0357\n",
      "Batch 70, Loss: 0.0047\n",
      "Batch 80, Loss: 0.0045\n",
      "Batch 90, Loss: 0.0064\n",
      "Epoch: 344 | Train loss: 0.05195 | Test loss: 0.04400 | Test accuracy: 0.50773\n",
      "Batch 0, Loss: 0.0077\n",
      "Batch 10, Loss: 0.0893\n",
      "Batch 20, Loss: 0.0094\n",
      "Batch 30, Loss: 0.0005\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0145\n",
      "Batch 60, Loss: 0.0080\n",
      "Batch 70, Loss: 0.0539\n",
      "Batch 80, Loss: 0.0016\n",
      "Batch 90, Loss: 0.0816\n",
      "Epoch: 345 | Train loss: 0.03269 | Test loss: 0.06854 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0317\n",
      "Batch 10, Loss: 0.0040\n",
      "Batch 20, Loss: 0.0006\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.5376\n",
      "Batch 50, Loss: 0.0227\n",
      "Batch 60, Loss: 0.1397\n",
      "Batch 70, Loss: 0.0898\n",
      "Batch 80, Loss: 0.0175\n",
      "Batch 90, Loss: 0.0182\n",
      "Epoch: 346 | Train loss: 0.06568 | Test loss: 0.08866 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0347\n",
      "Batch 10, Loss: 0.0903\n",
      "Batch 20, Loss: 0.2196\n",
      "Batch 30, Loss: 0.0686\n",
      "Batch 40, Loss: 0.0180\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0704\n",
      "Batch 70, Loss: 0.0629\n",
      "Batch 80, Loss: 0.0816\n",
      "Batch 90, Loss: 0.0442\n",
      "Epoch: 347 | Train loss: 0.06234 | Test loss: 0.06867 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0913\n",
      "Batch 10, Loss: 0.0086\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0174\n",
      "Batch 50, Loss: 0.0316\n",
      "Batch 60, Loss: 0.0334\n",
      "Batch 70, Loss: 0.0196\n",
      "Batch 80, Loss: 0.0714\n",
      "Batch 90, Loss: 0.0285\n",
      "Epoch: 348 | Train loss: 0.05536 | Test loss: 0.04534 | Test accuracy: 0.57216\n",
      "Batch 0, Loss: 0.0104\n",
      "Batch 10, Loss: 0.0389\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0012\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.0374\n",
      "Batch 60, Loss: 0.0006\n",
      "Batch 70, Loss: 0.0046\n",
      "Batch 80, Loss: 0.0333\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 349 | Train loss: 0.03341 | Test loss: 0.09734 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.2118\n",
      "Batch 10, Loss: 0.0131\n",
      "Batch 20, Loss: 0.0036\n",
      "Batch 30, Loss: 0.0301\n",
      "Batch 40, Loss: 0.0423\n",
      "Batch 50, Loss: 0.0008\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.1550\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0900\n",
      "Epoch: 350 | Train loss: 0.05274 | Test loss: 0.85170 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0817\n",
      "Batch 10, Loss: 0.0116\n",
      "Batch 20, Loss: 0.0002\n",
      "Batch 30, Loss: 0.0012\n",
      "Batch 40, Loss: 0.0044\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.1627\n",
      "Batch 80, Loss: 0.0188\n",
      "Batch 90, Loss: 0.0197\n",
      "Epoch: 351 | Train loss: 0.03625 | Test loss: 0.06348 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.0079\n",
      "Batch 10, Loss: 0.0981\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.0841\n",
      "Batch 40, Loss: 0.0144\n",
      "Batch 50, Loss: 0.0074\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.0082\n",
      "Batch 80, Loss: 0.0116\n",
      "Batch 90, Loss: 0.0144\n",
      "Epoch: 352 | Train loss: 0.05806 | Test loss: 0.24166 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0029\n",
      "Batch 10, Loss: 0.1235\n",
      "Batch 20, Loss: 0.4064\n",
      "Batch 30, Loss: 0.0261\n",
      "Batch 40, Loss: 0.0198\n",
      "Batch 50, Loss: 0.0138\n",
      "Batch 60, Loss: 0.0349\n",
      "Batch 70, Loss: 0.0259\n",
      "Batch 80, Loss: 0.0492\n",
      "Batch 90, Loss: 0.2585\n",
      "Epoch: 353 | Train loss: 0.06823 | Test loss: 0.20071 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0318\n",
      "Batch 10, Loss: 0.0099\n",
      "Batch 20, Loss: 0.0793\n",
      "Batch 30, Loss: 0.0060\n",
      "Batch 40, Loss: 0.1734\n",
      "Batch 50, Loss: 0.1016\n",
      "Batch 60, Loss: 0.0291\n",
      "Batch 70, Loss: 0.0117\n",
      "Batch 80, Loss: 0.0294\n",
      "Batch 90, Loss: 0.0134\n",
      "Epoch: 354 | Train loss: 0.06138 | Test loss: 1.44871 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0145\n",
      "Batch 10, Loss: 0.4786\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 1.0559\n",
      "Batch 40, Loss: 0.0071\n",
      "Batch 50, Loss: 0.0235\n",
      "Batch 60, Loss: 0.0047\n",
      "Batch 70, Loss: 0.0070\n",
      "Batch 80, Loss: 0.0004\n",
      "Batch 90, Loss: 0.0017\n",
      "Epoch: 355 | Train loss: 0.05784 | Test loss: 0.05558 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0337\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.0915\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.0099\n",
      "Batch 50, Loss: 0.0357\n",
      "Batch 60, Loss: 0.0100\n",
      "Batch 70, Loss: 0.0046\n",
      "Batch 80, Loss: 0.0024\n",
      "Batch 90, Loss: 0.0220\n",
      "Epoch: 356 | Train loss: 0.05777 | Test loss: 0.06473 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0193\n",
      "Batch 10, Loss: 0.0395\n",
      "Batch 20, Loss: 0.0464\n",
      "Batch 30, Loss: 0.0347\n",
      "Batch 40, Loss: 0.0025\n",
      "Batch 50, Loss: 0.0313\n",
      "Batch 60, Loss: 0.0100\n",
      "Batch 70, Loss: 0.0143\n",
      "Batch 80, Loss: 0.5187\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 357 | Train loss: 0.05300 | Test loss: 0.09156 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0029\n",
      "Batch 10, Loss: 0.0757\n",
      "Batch 20, Loss: 0.0195\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0057\n",
      "Batch 50, Loss: 0.0488\n",
      "Batch 60, Loss: 0.0120\n",
      "Batch 70, Loss: 0.0054\n",
      "Batch 80, Loss: 0.0011\n",
      "Batch 90, Loss: 0.0196\n",
      "Epoch: 358 | Train loss: 0.02488 | Test loss: 0.06526 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.2049\n",
      "Batch 10, Loss: 0.0296\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0301\n",
      "Batch 40, Loss: 0.0140\n",
      "Batch 50, Loss: 0.0092\n",
      "Batch 60, Loss: 0.0029\n",
      "Batch 70, Loss: 0.1070\n",
      "Batch 80, Loss: 0.0265\n",
      "Batch 90, Loss: 0.0222\n",
      "Epoch: 359 | Train loss: 0.03909 | Test loss: 0.10705 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0100\n",
      "Batch 10, Loss: 0.0234\n",
      "Batch 20, Loss: 0.3057\n",
      "Batch 30, Loss: 0.1679\n",
      "Batch 40, Loss: 0.0099\n",
      "Batch 50, Loss: 0.0192\n",
      "Batch 60, Loss: 0.0445\n",
      "Batch 70, Loss: 0.0403\n",
      "Batch 80, Loss: 0.0019\n",
      "Batch 90, Loss: 0.0522\n",
      "Epoch: 360 | Train loss: 0.08206 | Test loss: 0.52676 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0094\n",
      "Batch 10, Loss: 0.0048\n",
      "Batch 20, Loss: 0.0259\n",
      "Batch 30, Loss: 0.0076\n",
      "Batch 40, Loss: 0.3236\n",
      "Batch 50, Loss: 0.3613\n",
      "Batch 60, Loss: 0.1091\n",
      "Batch 70, Loss: 0.0248\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0006\n",
      "Epoch: 361 | Train loss: 0.05779 | Test loss: 0.15977 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0767\n",
      "Batch 10, Loss: 0.0138\n",
      "Batch 20, Loss: 0.0049\n",
      "Batch 30, Loss: 0.1051\n",
      "Batch 40, Loss: 0.0008\n",
      "Batch 50, Loss: 0.0181\n",
      "Batch 60, Loss: 0.0079\n",
      "Batch 70, Loss: 0.1204\n",
      "Batch 80, Loss: 0.0057\n",
      "Batch 90, Loss: 0.0067\n",
      "Epoch: 362 | Train loss: 0.04961 | Test loss: 0.06257 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1160\n",
      "Batch 10, Loss: 0.0465\n",
      "Batch 20, Loss: 0.0059\n",
      "Batch 30, Loss: 0.0147\n",
      "Batch 40, Loss: 0.0139\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.0563\n",
      "Batch 70, Loss: 0.0653\n",
      "Batch 80, Loss: 0.0036\n",
      "Batch 90, Loss: 0.0119\n",
      "Epoch: 363 | Train loss: 0.04078 | Test loss: 0.14984 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.1887\n",
      "Batch 20, Loss: 0.0070\n",
      "Batch 30, Loss: 0.2358\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.0356\n",
      "Batch 60, Loss: 0.3725\n",
      "Batch 70, Loss: 0.1067\n",
      "Batch 80, Loss: 0.3906\n",
      "Batch 90, Loss: 0.0180\n",
      "Epoch: 364 | Train loss: 0.03744 | Test loss: 0.06465 | Test accuracy: 0.54897\n",
      "Batch 0, Loss: 0.0104\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.0012\n",
      "Batch 40, Loss: 0.0746\n",
      "Batch 50, Loss: 0.0069\n",
      "Batch 60, Loss: 0.0071\n",
      "Batch 70, Loss: 0.0378\n",
      "Batch 80, Loss: 0.0025\n",
      "Batch 90, Loss: 0.0123\n",
      "Epoch: 365 | Train loss: 0.02540 | Test loss: 0.13367 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0039\n",
      "Batch 10, Loss: 0.0088\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.1478\n",
      "Batch 50, Loss: 0.6511\n",
      "Batch 60, Loss: 0.0172\n",
      "Batch 70, Loss: 0.0310\n",
      "Batch 80, Loss: 0.0055\n",
      "Batch 90, Loss: 0.3210\n",
      "Epoch: 366 | Train loss: 0.05261 | Test loss: 0.04868 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0089\n",
      "Batch 10, Loss: 0.0122\n",
      "Batch 20, Loss: 0.0164\n",
      "Batch 30, Loss: 0.0111\n",
      "Batch 40, Loss: 0.0437\n",
      "Batch 50, Loss: 0.0650\n",
      "Batch 60, Loss: 0.0910\n",
      "Batch 70, Loss: 0.0515\n",
      "Batch 80, Loss: 0.3381\n",
      "Batch 90, Loss: 0.0220\n",
      "Epoch: 367 | Train loss: 0.06411 | Test loss: 0.08677 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0328\n",
      "Batch 40, Loss: 0.0028\n",
      "Batch 50, Loss: 0.2739\n",
      "Batch 60, Loss: 0.0074\n",
      "Batch 70, Loss: 0.0006\n",
      "Batch 80, Loss: 0.0062\n",
      "Batch 90, Loss: 0.0080\n",
      "Epoch: 368 | Train loss: 0.05076 | Test loss: 0.20875 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0655\n",
      "Batch 10, Loss: 0.4102\n",
      "Batch 20, Loss: 0.3169\n",
      "Batch 30, Loss: 0.4365\n",
      "Batch 40, Loss: 0.1048\n",
      "Batch 50, Loss: 0.0787\n",
      "Batch 60, Loss: 0.1779\n",
      "Batch 70, Loss: 0.0405\n",
      "Batch 80, Loss: 0.0235\n",
      "Batch 90, Loss: 0.0406\n",
      "Epoch: 369 | Train loss: 0.07257 | Test loss: 0.54663 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0068\n",
      "Batch 10, Loss: 0.0337\n",
      "Batch 20, Loss: 0.0016\n",
      "Batch 30, Loss: 0.0557\n",
      "Batch 40, Loss: 0.0090\n",
      "Batch 50, Loss: 0.0038\n",
      "Batch 60, Loss: 0.1649\n",
      "Batch 70, Loss: 0.2231\n",
      "Batch 80, Loss: 0.0264\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 370 | Train loss: 0.04581 | Test loss: 0.08678 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0681\n",
      "Batch 10, Loss: 0.0699\n",
      "Batch 20, Loss: 0.0158\n",
      "Batch 30, Loss: 0.1131\n",
      "Batch 40, Loss: 0.0544\n",
      "Batch 50, Loss: 0.0828\n",
      "Batch 60, Loss: 0.0068\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.0127\n",
      "Batch 90, Loss: 0.0146\n",
      "Epoch: 371 | Train loss: 0.04307 | Test loss: 0.07981 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.1000\n",
      "Batch 10, Loss: 0.0010\n",
      "Batch 20, Loss: 0.0356\n",
      "Batch 30, Loss: 0.0173\n",
      "Batch 40, Loss: 0.0200\n",
      "Batch 50, Loss: 0.1058\n",
      "Batch 60, Loss: 0.0210\n",
      "Batch 70, Loss: 0.0057\n",
      "Batch 80, Loss: 0.0022\n",
      "Batch 90, Loss: 0.0087\n",
      "Epoch: 372 | Train loss: 0.04224 | Test loss: 0.05942 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0945\n",
      "Batch 10, Loss: 0.0386\n",
      "Batch 20, Loss: 0.0289\n",
      "Batch 30, Loss: 0.0017\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0423\n",
      "Batch 60, Loss: 0.0179\n",
      "Batch 70, Loss: 0.0357\n",
      "Batch 80, Loss: 0.0240\n",
      "Batch 90, Loss: 0.0345\n",
      "Epoch: 373 | Train loss: 0.03468 | Test loss: 0.04756 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0045\n",
      "Batch 10, Loss: 0.1722\n",
      "Batch 20, Loss: 0.0033\n",
      "Batch 30, Loss: 0.6299\n",
      "Batch 40, Loss: 0.1549\n",
      "Batch 50, Loss: 0.0078\n",
      "Batch 60, Loss: 0.0277\n",
      "Batch 70, Loss: 0.0833\n",
      "Batch 80, Loss: 0.0002\n",
      "Batch 90, Loss: 0.0400\n",
      "Epoch: 374 | Train loss: 0.04355 | Test loss: 0.06681 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0166\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0309\n",
      "Batch 30, Loss: 0.0050\n",
      "Batch 40, Loss: 0.0558\n",
      "Batch 50, Loss: 0.0083\n",
      "Batch 60, Loss: 0.0473\n",
      "Batch 70, Loss: 0.0471\n",
      "Batch 80, Loss: 0.0019\n",
      "Batch 90, Loss: 0.2919\n",
      "Epoch: 375 | Train loss: 0.03784 | Test loss: 0.37081 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0088\n",
      "Batch 20, Loss: 0.0123\n",
      "Batch 30, Loss: 0.0259\n",
      "Batch 40, Loss: 0.1601\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0147\n",
      "Batch 70, Loss: 0.0806\n",
      "Batch 80, Loss: 0.1010\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 376 | Train loss: 0.02893 | Test loss: 0.05095 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0029\n",
      "Batch 10, Loss: 0.0065\n",
      "Batch 20, Loss: 0.1181\n",
      "Batch 30, Loss: 0.0722\n",
      "Batch 40, Loss: 0.0358\n",
      "Batch 50, Loss: 0.1041\n",
      "Batch 60, Loss: 0.0465\n",
      "Batch 70, Loss: 0.0376\n",
      "Batch 80, Loss: 0.0034\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 377 | Train loss: 0.05163 | Test loss: 0.32499 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0040\n",
      "Batch 20, Loss: 0.0176\n",
      "Batch 30, Loss: 0.0161\n",
      "Batch 40, Loss: 0.0060\n",
      "Batch 50, Loss: 0.0064\n",
      "Batch 60, Loss: 0.0453\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.0018\n",
      "Batch 90, Loss: 0.0101\n",
      "Epoch: 378 | Train loss: 0.03728 | Test loss: 0.32604 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.1604\n",
      "Batch 20, Loss: 0.0081\n",
      "Batch 30, Loss: 0.0681\n",
      "Batch 40, Loss: 0.0143\n",
      "Batch 50, Loss: 0.0921\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0274\n",
      "Batch 80, Loss: 0.0234\n",
      "Batch 90, Loss: 0.1159\n",
      "Epoch: 379 | Train loss: 0.07673 | Test loss: 1.27740 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0055\n",
      "Batch 10, Loss: 0.7805\n",
      "Batch 20, Loss: 0.3652\n",
      "Batch 30, Loss: 0.0060\n",
      "Batch 40, Loss: 0.0779\n",
      "Batch 50, Loss: 0.0120\n",
      "Batch 60, Loss: 0.1213\n",
      "Batch 70, Loss: 0.0227\n",
      "Batch 80, Loss: 0.0228\n",
      "Batch 90, Loss: 0.0222\n",
      "Epoch: 380 | Train loss: 0.06311 | Test loss: 0.10235 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3995\n",
      "Batch 10, Loss: 0.0223\n",
      "Batch 20, Loss: 0.1605\n",
      "Batch 30, Loss: 0.0230\n",
      "Batch 40, Loss: 0.0102\n",
      "Batch 50, Loss: 0.0154\n",
      "Batch 60, Loss: 0.0398\n",
      "Batch 70, Loss: 0.2151\n",
      "Batch 80, Loss: 0.0747\n",
      "Batch 90, Loss: 0.0013\n",
      "Epoch: 381 | Train loss: 0.06033 | Test loss: 0.05110 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0062\n",
      "Batch 10, Loss: 0.0329\n",
      "Batch 20, Loss: 0.0479\n",
      "Batch 30, Loss: 0.0082\n",
      "Batch 40, Loss: 0.0061\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0043\n",
      "Batch 70, Loss: 0.1254\n",
      "Batch 80, Loss: 0.0035\n",
      "Batch 90, Loss: 0.0005\n",
      "Epoch: 382 | Train loss: 0.01987 | Test loss: 0.04351 | Test accuracy: 0.58247\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0150\n",
      "Batch 20, Loss: 0.0019\n",
      "Batch 30, Loss: 0.0009\n",
      "Batch 40, Loss: 0.0268\n",
      "Batch 50, Loss: 0.0179\n",
      "Batch 60, Loss: 0.0064\n",
      "Batch 70, Loss: 0.0514\n",
      "Batch 80, Loss: 0.0108\n",
      "Batch 90, Loss: 0.0144\n",
      "Epoch: 383 | Train loss: 0.03341 | Test loss: 0.20966 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0252\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0009\n",
      "Batch 30, Loss: 0.0538\n",
      "Batch 40, Loss: 0.0727\n",
      "Batch 50, Loss: 0.0063\n",
      "Batch 60, Loss: 0.0071\n",
      "Batch 70, Loss: 0.0059\n",
      "Batch 80, Loss: 0.0024\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 384 | Train loss: 0.03157 | Test loss: 0.03982 | Test accuracy: 0.64175\n",
      "Batch 0, Loss: 0.0022\n",
      "Batch 10, Loss: 0.0006\n",
      "Batch 20, Loss: 0.0012\n",
      "Batch 30, Loss: 0.0041\n",
      "Batch 40, Loss: 0.0227\n",
      "Batch 50, Loss: 0.0204\n",
      "Batch 60, Loss: 0.0211\n",
      "Batch 70, Loss: 0.0047\n",
      "Batch 80, Loss: 0.1380\n",
      "Batch 90, Loss: 0.0122\n",
      "Epoch: 385 | Train loss: 0.04906 | Test loss: 0.45736 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1197\n",
      "Batch 10, Loss: 0.0035\n",
      "Batch 20, Loss: 0.0730\n",
      "Batch 30, Loss: 0.0213\n",
      "Batch 40, Loss: 0.0467\n",
      "Batch 50, Loss: 0.0114\n",
      "Batch 60, Loss: 0.0025\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0039\n",
      "Batch 90, Loss: 0.0008\n",
      "Epoch: 386 | Train loss: 0.04530 | Test loss: 0.39137 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0060\n",
      "Batch 10, Loss: 0.1780\n",
      "Batch 20, Loss: 0.0804\n",
      "Batch 30, Loss: 0.0126\n",
      "Batch 40, Loss: 0.0080\n",
      "Batch 50, Loss: 0.0340\n",
      "Batch 60, Loss: 0.7779\n",
      "Batch 70, Loss: 0.0025\n",
      "Batch 80, Loss: 0.1781\n",
      "Batch 90, Loss: 0.0494\n",
      "Epoch: 387 | Train loss: 0.07019 | Test loss: 0.04559 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0161\n",
      "Batch 10, Loss: 0.1377\n",
      "Batch 20, Loss: 0.0391\n",
      "Batch 30, Loss: 0.0529\n",
      "Batch 40, Loss: 0.0080\n",
      "Batch 50, Loss: 0.0084\n",
      "Batch 60, Loss: 0.0855\n",
      "Batch 70, Loss: 0.0398\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0265\n",
      "Epoch: 388 | Train loss: 0.06301 | Test loss: 0.31008 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1814\n",
      "Batch 10, Loss: 0.3707\n",
      "Batch 20, Loss: 0.0063\n",
      "Batch 30, Loss: 0.0225\n",
      "Batch 40, Loss: 0.0288\n",
      "Batch 50, Loss: 0.0169\n",
      "Batch 60, Loss: 0.0205\n",
      "Batch 70, Loss: 0.0051\n",
      "Batch 80, Loss: 0.0457\n",
      "Batch 90, Loss: 0.0006\n",
      "Epoch: 389 | Train loss: 0.05870 | Test loss: 0.27163 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0184\n",
      "Batch 10, Loss: 0.0174\n",
      "Batch 20, Loss: 0.0235\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.0183\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0405\n",
      "Batch 70, Loss: 0.0060\n",
      "Batch 80, Loss: 0.0384\n",
      "Batch 90, Loss: 0.0011\n",
      "Epoch: 390 | Train loss: 0.05833 | Test loss: 0.23169 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0081\n",
      "Batch 10, Loss: 0.0018\n",
      "Batch 20, Loss: 0.0199\n",
      "Batch 30, Loss: 0.0581\n",
      "Batch 40, Loss: 0.0155\n",
      "Batch 50, Loss: 0.0022\n",
      "Batch 60, Loss: 0.0215\n",
      "Batch 70, Loss: 0.0305\n",
      "Batch 80, Loss: 0.0063\n",
      "Batch 90, Loss: 0.0260\n",
      "Epoch: 391 | Train loss: 0.03959 | Test loss: 0.14684 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0116\n",
      "Batch 10, Loss: 0.0115\n",
      "Batch 20, Loss: 0.0073\n",
      "Batch 30, Loss: 0.0208\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.0200\n",
      "Batch 60, Loss: 0.0009\n",
      "Batch 70, Loss: 0.0146\n",
      "Batch 80, Loss: 0.0086\n",
      "Batch 90, Loss: 0.0193\n",
      "Epoch: 392 | Train loss: 0.03340 | Test loss: 0.18440 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0131\n",
      "Batch 10, Loss: 0.0786\n",
      "Batch 20, Loss: 0.0594\n",
      "Batch 30, Loss: 0.0090\n",
      "Batch 40, Loss: 0.0367\n",
      "Batch 50, Loss: 0.0175\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0120\n",
      "Batch 80, Loss: 0.0258\n",
      "Batch 90, Loss: 0.0160\n",
      "Epoch: 393 | Train loss: 0.04019 | Test loss: 0.61800 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3834\n",
      "Batch 10, Loss: 0.0167\n",
      "Batch 20, Loss: 0.0269\n",
      "Batch 30, Loss: 0.0217\n",
      "Batch 40, Loss: 0.0015\n",
      "Batch 50, Loss: 0.0306\n",
      "Batch 60, Loss: 0.0179\n",
      "Batch 70, Loss: 0.2966\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0711\n",
      "Epoch: 394 | Train loss: 0.08739 | Test loss: 0.23982 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0213\n",
      "Batch 10, Loss: 0.0120\n",
      "Batch 20, Loss: 0.0218\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.0067\n",
      "Batch 50, Loss: 0.6575\n",
      "Batch 60, Loss: 0.0086\n",
      "Batch 70, Loss: 0.0123\n",
      "Batch 80, Loss: 0.0115\n",
      "Batch 90, Loss: 0.0259\n",
      "Epoch: 395 | Train loss: 0.07214 | Test loss: 0.45096 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0172\n",
      "Batch 10, Loss: 0.0035\n",
      "Batch 20, Loss: 0.0295\n",
      "Batch 30, Loss: 0.0174\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.0319\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0041\n",
      "Batch 90, Loss: 0.0244\n",
      "Epoch: 396 | Train loss: 0.02609 | Test loss: 0.16542 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0055\n",
      "Batch 10, Loss: 0.0119\n",
      "Batch 20, Loss: 0.0030\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0302\n",
      "Batch 50, Loss: 0.0632\n",
      "Batch 60, Loss: 0.0169\n",
      "Batch 70, Loss: 0.0190\n",
      "Batch 80, Loss: 0.0334\n",
      "Batch 90, Loss: 0.0084\n",
      "Epoch: 397 | Train loss: 0.04813 | Test loss: 0.06164 | Test accuracy: 0.49227\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0077\n",
      "Batch 20, Loss: 0.0126\n",
      "Batch 30, Loss: 0.0162\n",
      "Batch 40, Loss: 0.0052\n",
      "Batch 50, Loss: 0.0113\n",
      "Batch 60, Loss: 0.0567\n",
      "Batch 70, Loss: 0.0173\n",
      "Batch 80, Loss: 0.0131\n",
      "Batch 90, Loss: 0.0021\n",
      "Epoch: 398 | Train loss: 0.03201 | Test loss: 0.08137 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0234\n",
      "Batch 10, Loss: 0.0004\n",
      "Batch 20, Loss: 0.0049\n",
      "Batch 30, Loss: 0.5333\n",
      "Batch 40, Loss: 0.0194\n",
      "Batch 50, Loss: 0.0554\n",
      "Batch 60, Loss: 0.0281\n",
      "Batch 70, Loss: 0.0318\n",
      "Batch 80, Loss: 0.0109\n",
      "Batch 90, Loss: 0.0542\n",
      "Epoch: 399 | Train loss: 0.05197 | Test loss: 0.05932 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0180\n",
      "Batch 10, Loss: 0.0183\n",
      "Batch 20, Loss: 0.1599\n",
      "Batch 30, Loss: 0.0985\n",
      "Batch 40, Loss: 0.0057\n",
      "Batch 50, Loss: 0.0803\n",
      "Batch 60, Loss: 0.0192\n",
      "Batch 70, Loss: 0.0286\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0114\n",
      "Epoch: 400 | Train loss: 0.05092 | Test loss: 0.10166 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0421\n",
      "Batch 10, Loss: 0.0064\n",
      "Batch 20, Loss: 0.0146\n",
      "Batch 30, Loss: 0.2846\n",
      "Batch 40, Loss: 0.0241\n",
      "Batch 50, Loss: 0.0381\n",
      "Batch 60, Loss: 0.0147\n",
      "Batch 70, Loss: 0.2203\n",
      "Batch 80, Loss: 0.0301\n",
      "Batch 90, Loss: 0.0680\n",
      "Epoch: 401 | Train loss: 0.04772 | Test loss: 0.14234 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0802\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0140\n",
      "Batch 30, Loss: 0.0118\n",
      "Batch 40, Loss: 0.0145\n",
      "Batch 50, Loss: 0.1467\n",
      "Batch 60, Loss: 0.0643\n",
      "Batch 70, Loss: 0.0376\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0080\n",
      "Epoch: 402 | Train loss: 0.04878 | Test loss: 0.05069 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0111\n",
      "Batch 10, Loss: 0.0306\n",
      "Batch 20, Loss: 0.0618\n",
      "Batch 30, Loss: 0.0085\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0330\n",
      "Batch 60, Loss: 0.0056\n",
      "Batch 70, Loss: 0.0132\n",
      "Batch 80, Loss: 0.0422\n",
      "Batch 90, Loss: 0.0231\n",
      "Epoch: 403 | Train loss: 0.04240 | Test loss: 0.12945 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0217\n",
      "Batch 10, Loss: 0.0197\n",
      "Batch 20, Loss: 0.0064\n",
      "Batch 30, Loss: 0.5453\n",
      "Batch 40, Loss: 0.0791\n",
      "Batch 50, Loss: 0.0056\n",
      "Batch 60, Loss: 0.0107\n",
      "Batch 70, Loss: 0.0580\n",
      "Batch 80, Loss: 0.0332\n",
      "Batch 90, Loss: 0.1179\n",
      "Epoch: 404 | Train loss: 0.04721 | Test loss: 0.26128 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0401\n",
      "Batch 10, Loss: 0.0649\n",
      "Batch 20, Loss: 0.0044\n",
      "Batch 30, Loss: 0.1598\n",
      "Batch 40, Loss: 0.2382\n",
      "Batch 50, Loss: 0.1587\n",
      "Batch 60, Loss: 0.0211\n",
      "Batch 70, Loss: 0.2368\n",
      "Batch 80, Loss: 0.0058\n",
      "Batch 90, Loss: 0.1089\n",
      "Epoch: 405 | Train loss: 0.13008 | Test loss: 0.04676 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0009\n",
      "Batch 10, Loss: 0.0301\n",
      "Batch 20, Loss: 0.0127\n",
      "Batch 30, Loss: 0.0104\n",
      "Batch 40, Loss: 0.0654\n",
      "Batch 50, Loss: 0.0417\n",
      "Batch 60, Loss: 0.0060\n",
      "Batch 70, Loss: 0.0028\n",
      "Batch 80, Loss: 0.0089\n",
      "Batch 90, Loss: 0.0007\n",
      "Epoch: 406 | Train loss: 0.04242 | Test loss: 1.97595 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0502\n",
      "Batch 10, Loss: 0.0192\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.0612\n",
      "Batch 40, Loss: 0.0115\n",
      "Batch 50, Loss: 0.0107\n",
      "Batch 60, Loss: 0.0281\n",
      "Batch 70, Loss: 0.0140\n",
      "Batch 80, Loss: 0.0377\n",
      "Batch 90, Loss: 0.0899\n",
      "Epoch: 407 | Train loss: 0.05735 | Test loss: 0.04494 | Test accuracy: 0.55155\n",
      "Batch 0, Loss: 0.0883\n",
      "Batch 10, Loss: 0.1426\n",
      "Batch 20, Loss: 0.1433\n",
      "Batch 30, Loss: 0.0213\n",
      "Batch 40, Loss: 0.0458\n",
      "Batch 50, Loss: 0.0047\n",
      "Batch 60, Loss: 0.1007\n",
      "Batch 70, Loss: 0.0352\n",
      "Batch 80, Loss: 0.0032\n",
      "Batch 90, Loss: 0.2526\n",
      "Epoch: 408 | Train loss: 0.03956 | Test loss: 0.05429 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0572\n",
      "Batch 20, Loss: 0.0557\n",
      "Batch 30, Loss: 0.0034\n",
      "Batch 40, Loss: 0.0526\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0825\n",
      "Batch 70, Loss: 0.4671\n",
      "Batch 80, Loss: 0.0287\n",
      "Batch 90, Loss: 0.0174\n",
      "Epoch: 409 | Train loss: 0.04670 | Test loss: 0.14544 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0308\n",
      "Batch 10, Loss: 0.0060\n",
      "Batch 20, Loss: 0.1637\n",
      "Batch 30, Loss: 0.0020\n",
      "Batch 40, Loss: 0.1256\n",
      "Batch 50, Loss: 0.0113\n",
      "Batch 60, Loss: 0.0073\n",
      "Batch 70, Loss: 0.0077\n",
      "Batch 80, Loss: 0.0093\n",
      "Batch 90, Loss: 0.0633\n",
      "Epoch: 410 | Train loss: 0.03990 | Test loss: 0.09417 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0141\n",
      "Batch 10, Loss: 0.0153\n",
      "Batch 20, Loss: 0.2026\n",
      "Batch 30, Loss: 0.0585\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0139\n",
      "Batch 60, Loss: 0.0527\n",
      "Batch 70, Loss: 0.0278\n",
      "Batch 80, Loss: 0.0129\n",
      "Batch 90, Loss: 0.0070\n",
      "Epoch: 411 | Train loss: 0.04157 | Test loss: 0.30906 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0387\n",
      "Batch 10, Loss: 0.1309\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0062\n",
      "Batch 40, Loss: 0.0060\n",
      "Batch 50, Loss: 0.1878\n",
      "Batch 60, Loss: 0.0204\n",
      "Batch 70, Loss: 0.2444\n",
      "Batch 80, Loss: 0.0041\n",
      "Batch 90, Loss: 0.1791\n",
      "Epoch: 412 | Train loss: 0.08777 | Test loss: 0.07836 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0298\n",
      "Batch 10, Loss: 0.0105\n",
      "Batch 20, Loss: 0.2601\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.0026\n",
      "Batch 50, Loss: 0.0022\n",
      "Batch 60, Loss: 0.1334\n",
      "Batch 70, Loss: 0.0471\n",
      "Batch 80, Loss: 0.0090\n",
      "Batch 90, Loss: 0.0175\n",
      "Epoch: 413 | Train loss: 0.04472 | Test loss: 0.33661 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0284\n",
      "Batch 20, Loss: 0.0188\n",
      "Batch 30, Loss: 0.0251\n",
      "Batch 40, Loss: 0.1326\n",
      "Batch 50, Loss: 0.0624\n",
      "Batch 60, Loss: 0.0047\n",
      "Batch 70, Loss: 0.0105\n",
      "Batch 80, Loss: 0.0126\n",
      "Batch 90, Loss: 0.0026\n",
      "Epoch: 414 | Train loss: 0.05304 | Test loss: 0.61091 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0612\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0126\n",
      "Batch 30, Loss: 0.0075\n",
      "Batch 40, Loss: 0.0719\n",
      "Batch 50, Loss: 0.0034\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0023\n",
      "Batch 80, Loss: 0.0420\n",
      "Batch 90, Loss: 0.0418\n",
      "Epoch: 415 | Train loss: 0.02526 | Test loss: 0.76792 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0056\n",
      "Batch 10, Loss: 0.0243\n",
      "Batch 20, Loss: 0.0107\n",
      "Batch 30, Loss: 0.0124\n",
      "Batch 40, Loss: 0.0115\n",
      "Batch 50, Loss: 0.0276\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.1902\n",
      "Batch 80, Loss: 0.0308\n",
      "Batch 90, Loss: 0.0011\n",
      "Epoch: 416 | Train loss: 0.02294 | Test loss: 0.04884 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0237\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0435\n",
      "Batch 30, Loss: 0.0009\n",
      "Batch 40, Loss: 0.0027\n",
      "Batch 50, Loss: 0.1135\n",
      "Batch 60, Loss: 0.0387\n",
      "Batch 70, Loss: 0.0090\n",
      "Batch 80, Loss: 0.0017\n",
      "Batch 90, Loss: 0.0097\n",
      "Epoch: 417 | Train loss: 0.03885 | Test loss: 0.04133 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0606\n",
      "Batch 10, Loss: 0.1086\n",
      "Batch 20, Loss: 0.0492\n",
      "Batch 30, Loss: 0.0543\n",
      "Batch 40, Loss: 0.0301\n",
      "Batch 50, Loss: 0.0857\n",
      "Batch 60, Loss: 0.0182\n",
      "Batch 70, Loss: 0.0019\n",
      "Batch 80, Loss: 0.0043\n",
      "Batch 90, Loss: 0.0182\n",
      "Epoch: 418 | Train loss: 0.06814 | Test loss: 0.05490 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0337\n",
      "Batch 10, Loss: 0.0035\n",
      "Batch 20, Loss: 0.0535\n",
      "Batch 30, Loss: 0.0188\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.1043\n",
      "Batch 60, Loss: 0.0047\n",
      "Batch 70, Loss: 0.0107\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0571\n",
      "Epoch: 419 | Train loss: 0.04597 | Test loss: 0.51177 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0011\n",
      "Batch 10, Loss: 0.0496\n",
      "Batch 20, Loss: 0.0045\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0143\n",
      "Batch 50, Loss: 0.0282\n",
      "Batch 60, Loss: 0.0078\n",
      "Batch 70, Loss: 0.0074\n",
      "Batch 80, Loss: 0.0806\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 420 | Train loss: 0.02647 | Test loss: 0.04581 | Test accuracy: 0.71907\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0104\n",
      "Batch 20, Loss: 0.0115\n",
      "Batch 30, Loss: 0.0254\n",
      "Batch 40, Loss: 0.2591\n",
      "Batch 50, Loss: 0.0012\n",
      "Batch 60, Loss: 0.0105\n",
      "Batch 70, Loss: 0.0789\n",
      "Batch 80, Loss: 0.0196\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 421 | Train loss: 0.04949 | Test loss: 0.64104 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0095\n",
      "Batch 10, Loss: 0.0107\n",
      "Batch 20, Loss: 0.1056\n",
      "Batch 30, Loss: 0.0222\n",
      "Batch 40, Loss: 0.1351\n",
      "Batch 50, Loss: 0.0201\n",
      "Batch 60, Loss: 0.1067\n",
      "Batch 70, Loss: 0.0023\n",
      "Batch 80, Loss: 0.0388\n",
      "Batch 90, Loss: 0.0087\n",
      "Epoch: 422 | Train loss: 0.02907 | Test loss: 0.20040 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0112\n",
      "Batch 10, Loss: 0.0329\n",
      "Batch 20, Loss: 0.0411\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.1878\n",
      "Batch 50, Loss: 0.0862\n",
      "Batch 60, Loss: 0.0028\n",
      "Batch 70, Loss: 0.2168\n",
      "Batch 80, Loss: 0.0286\n",
      "Batch 90, Loss: 0.0129\n",
      "Epoch: 423 | Train loss: 0.04280 | Test loss: 0.61566 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0010\n",
      "Batch 10, Loss: 0.0230\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0327\n",
      "Batch 40, Loss: 0.1150\n",
      "Batch 50, Loss: 0.0707\n",
      "Batch 60, Loss: 0.0019\n",
      "Batch 70, Loss: 0.0275\n",
      "Batch 80, Loss: 0.0986\n",
      "Batch 90, Loss: 0.0061\n",
      "Epoch: 424 | Train loss: 0.03335 | Test loss: 0.07711 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0081\n",
      "Batch 10, Loss: 0.0063\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0030\n",
      "Batch 40, Loss: 0.0523\n",
      "Batch 50, Loss: 0.1448\n",
      "Batch 60, Loss: 0.0361\n",
      "Batch 70, Loss: 0.0443\n",
      "Batch 80, Loss: 0.1369\n",
      "Batch 90, Loss: 0.0465\n",
      "Epoch: 425 | Train loss: 0.08218 | Test loss: 0.18567 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0389\n",
      "Batch 10, Loss: 0.1264\n",
      "Batch 20, Loss: 0.0142\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.0025\n",
      "Batch 50, Loss: 0.0469\n",
      "Batch 60, Loss: 0.0523\n",
      "Batch 70, Loss: 0.0004\n",
      "Batch 80, Loss: 0.0079\n",
      "Batch 90, Loss: 0.0543\n",
      "Epoch: 426 | Train loss: 0.03427 | Test loss: 0.50417 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0159\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0217\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.0081\n",
      "Batch 50, Loss: 0.0392\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0160\n",
      "Batch 90, Loss: 0.1204\n",
      "Epoch: 427 | Train loss: 0.03042 | Test loss: 0.06931 | Test accuracy: 0.37887\n",
      "Batch 0, Loss: 0.0208\n",
      "Batch 10, Loss: 0.0108\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0748\n",
      "Batch 40, Loss: 0.0725\n",
      "Batch 50, Loss: 0.0116\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.0200\n",
      "Batch 80, Loss: 0.1829\n",
      "Batch 90, Loss: 0.0245\n",
      "Epoch: 428 | Train loss: 0.05042 | Test loss: 0.05380 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0112\n",
      "Batch 10, Loss: 0.0028\n",
      "Batch 20, Loss: 0.0017\n",
      "Batch 30, Loss: 0.0953\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.2485\n",
      "Batch 60, Loss: 0.0296\n",
      "Batch 70, Loss: 0.0152\n",
      "Batch 80, Loss: 0.1701\n",
      "Batch 90, Loss: 0.0152\n",
      "Epoch: 429 | Train loss: 0.05869 | Test loss: 0.09119 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0496\n",
      "Batch 10, Loss: 0.0264\n",
      "Batch 20, Loss: 0.0226\n",
      "Batch 30, Loss: 0.1030\n",
      "Batch 40, Loss: 0.0355\n",
      "Batch 50, Loss: 0.0235\n",
      "Batch 60, Loss: 0.0069\n",
      "Batch 70, Loss: 0.0112\n",
      "Batch 80, Loss: 0.0392\n",
      "Batch 90, Loss: 0.0917\n",
      "Epoch: 430 | Train loss: 0.07634 | Test loss: 0.07159 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0109\n",
      "Batch 10, Loss: 0.0050\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0314\n",
      "Batch 40, Loss: 0.2114\n",
      "Batch 50, Loss: 0.2060\n",
      "Batch 60, Loss: 0.0043\n",
      "Batch 70, Loss: 0.0583\n",
      "Batch 80, Loss: 0.0429\n",
      "Batch 90, Loss: 0.0088\n",
      "Epoch: 431 | Train loss: 0.05209 | Test loss: 0.04869 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0147\n",
      "Batch 10, Loss: 0.0109\n",
      "Batch 20, Loss: 0.0900\n",
      "Batch 30, Loss: 0.0401\n",
      "Batch 40, Loss: 0.0352\n",
      "Batch 50, Loss: 0.0158\n",
      "Batch 60, Loss: 0.0873\n",
      "Batch 70, Loss: 0.0206\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0164\n",
      "Epoch: 432 | Train loss: 0.06952 | Test loss: 0.41166 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0052\n",
      "Batch 10, Loss: 0.0101\n",
      "Batch 20, Loss: 0.0216\n",
      "Batch 30, Loss: 0.0452\n",
      "Batch 40, Loss: 0.0276\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.2082\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.0362\n",
      "Batch 90, Loss: 0.0099\n",
      "Epoch: 433 | Train loss: 0.04926 | Test loss: 0.04716 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0187\n",
      "Batch 10, Loss: 0.0051\n",
      "Batch 20, Loss: 0.0104\n",
      "Batch 30, Loss: 0.1577\n",
      "Batch 40, Loss: 0.0994\n",
      "Batch 50, Loss: 0.0079\n",
      "Batch 60, Loss: 0.0051\n",
      "Batch 70, Loss: 0.0455\n",
      "Batch 80, Loss: 0.0106\n",
      "Batch 90, Loss: 0.1582\n",
      "Epoch: 434 | Train loss: 0.04220 | Test loss: 0.04468 | Test accuracy: 0.57216\n",
      "Batch 0, Loss: 0.2048\n",
      "Batch 10, Loss: 0.0294\n",
      "Batch 20, Loss: 0.0565\n",
      "Batch 30, Loss: 0.0395\n",
      "Batch 40, Loss: 0.0071\n",
      "Batch 50, Loss: 0.1635\n",
      "Batch 60, Loss: 0.0007\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.0043\n",
      "Batch 90, Loss: 0.0014\n",
      "Epoch: 435 | Train loss: 0.03478 | Test loss: 0.64971 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0015\n",
      "Batch 10, Loss: 0.2548\n",
      "Batch 20, Loss: 0.0096\n",
      "Batch 30, Loss: 0.0121\n",
      "Batch 40, Loss: 0.0502\n",
      "Batch 50, Loss: 0.0658\n",
      "Batch 60, Loss: 0.0643\n",
      "Batch 70, Loss: 0.2387\n",
      "Batch 80, Loss: 0.0378\n",
      "Batch 90, Loss: 0.0395\n",
      "Epoch: 436 | Train loss: 0.04826 | Test loss: 0.04258 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0006\n",
      "Batch 10, Loss: 0.0042\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.2298\n",
      "Batch 50, Loss: 0.0504\n",
      "Batch 60, Loss: 0.0093\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0047\n",
      "Batch 90, Loss: 0.0110\n",
      "Epoch: 437 | Train loss: 0.03484 | Test loss: 0.04499 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0093\n",
      "Batch 10, Loss: 0.0102\n",
      "Batch 20, Loss: 0.0265\n",
      "Batch 30, Loss: 0.0125\n",
      "Batch 40, Loss: 0.0166\n",
      "Batch 50, Loss: 0.0065\n",
      "Batch 60, Loss: 0.0102\n",
      "Batch 70, Loss: 0.0595\n",
      "Batch 80, Loss: 0.1717\n",
      "Batch 90, Loss: 0.0349\n",
      "Epoch: 438 | Train loss: 0.03177 | Test loss: 0.05638 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0057\n",
      "Batch 10, Loss: 0.0026\n",
      "Batch 20, Loss: 0.0553\n",
      "Batch 30, Loss: 0.0133\n",
      "Batch 40, Loss: 0.0040\n",
      "Batch 50, Loss: 0.0159\n",
      "Batch 60, Loss: 0.0074\n",
      "Batch 70, Loss: 0.0263\n",
      "Batch 80, Loss: 0.0216\n",
      "Batch 90, Loss: 0.0071\n",
      "Epoch: 439 | Train loss: 0.02726 | Test loss: 0.23497 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0076\n",
      "Batch 10, Loss: 0.0508\n",
      "Batch 20, Loss: 0.0225\n",
      "Batch 30, Loss: 0.0040\n",
      "Batch 40, Loss: 0.0097\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0443\n",
      "Batch 70, Loss: 0.0051\n",
      "Batch 80, Loss: 0.0068\n",
      "Batch 90, Loss: 0.0043\n",
      "Epoch: 440 | Train loss: 0.04685 | Test loss: 0.15663 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0272\n",
      "Batch 10, Loss: 0.0364\n",
      "Batch 20, Loss: 0.4275\n",
      "Batch 30, Loss: 0.5522\n",
      "Batch 40, Loss: 0.0104\n",
      "Batch 50, Loss: 0.0139\n",
      "Batch 60, Loss: 0.0133\n",
      "Batch 70, Loss: 0.0645\n",
      "Batch 80, Loss: 0.0440\n",
      "Batch 90, Loss: 0.0041\n",
      "Epoch: 441 | Train loss: 0.05081 | Test loss: 0.17782 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0035\n",
      "Batch 10, Loss: 0.0083\n",
      "Batch 20, Loss: 0.0434\n",
      "Batch 30, Loss: 0.0113\n",
      "Batch 40, Loss: 0.0124\n",
      "Batch 50, Loss: 0.0797\n",
      "Batch 60, Loss: 0.0053\n",
      "Batch 70, Loss: 0.1621\n",
      "Batch 80, Loss: 0.0741\n",
      "Batch 90, Loss: 0.0293\n",
      "Epoch: 442 | Train loss: 0.03763 | Test loss: 0.07359 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0563\n",
      "Batch 20, Loss: 0.0012\n",
      "Batch 30, Loss: 0.0022\n",
      "Batch 40, Loss: 0.1258\n",
      "Batch 50, Loss: 0.3388\n",
      "Batch 60, Loss: 0.0085\n",
      "Batch 70, Loss: 0.0061\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0067\n",
      "Epoch: 443 | Train loss: 0.03688 | Test loss: 0.15172 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0489\n",
      "Batch 10, Loss: 0.0111\n",
      "Batch 20, Loss: 0.0313\n",
      "Batch 30, Loss: 0.0028\n",
      "Batch 40, Loss: 0.0120\n",
      "Batch 50, Loss: 0.0165\n",
      "Batch 60, Loss: 0.0189\n",
      "Batch 70, Loss: 0.0250\n",
      "Batch 80, Loss: 0.0056\n",
      "Batch 90, Loss: 0.0795\n",
      "Epoch: 444 | Train loss: 0.03733 | Test loss: 1.10229 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0396\n",
      "Batch 10, Loss: 0.0051\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0255\n",
      "Batch 50, Loss: 0.0453\n",
      "Batch 60, Loss: 0.0401\n",
      "Batch 70, Loss: 0.0199\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0552\n",
      "Epoch: 445 | Train loss: 0.03524 | Test loss: 0.19855 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0136\n",
      "Batch 10, Loss: 0.2390\n",
      "Batch 20, Loss: 0.0207\n",
      "Batch 30, Loss: 0.0241\n",
      "Batch 40, Loss: 0.1007\n",
      "Batch 50, Loss: 0.0978\n",
      "Batch 60, Loss: 0.0303\n",
      "Batch 70, Loss: 0.3045\n",
      "Batch 80, Loss: 0.0554\n",
      "Batch 90, Loss: 0.0585\n",
      "Epoch: 446 | Train loss: 0.05867 | Test loss: 0.30726 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0898\n",
      "Batch 10, Loss: 0.0682\n",
      "Batch 20, Loss: 0.0458\n",
      "Batch 30, Loss: 0.0505\n",
      "Batch 40, Loss: 0.1882\n",
      "Batch 50, Loss: 0.0886\n",
      "Batch 60, Loss: 0.0139\n",
      "Batch 70, Loss: 0.0060\n",
      "Batch 80, Loss: 0.1252\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 447 | Train loss: 0.10286 | Test loss: 0.05623 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0099\n",
      "Batch 10, Loss: 0.1290\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.1703\n",
      "Batch 40, Loss: 0.0163\n",
      "Batch 50, Loss: 0.0467\n",
      "Batch 60, Loss: 0.0777\n",
      "Batch 70, Loss: 0.0089\n",
      "Batch 80, Loss: 0.3125\n",
      "Batch 90, Loss: 0.0488\n",
      "Epoch: 448 | Train loss: 0.06889 | Test loss: 0.13415 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0090\n",
      "Batch 10, Loss: 0.0765\n",
      "Batch 20, Loss: 0.0144\n",
      "Batch 30, Loss: 0.0132\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.0833\n",
      "Batch 60, Loss: 0.1129\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0130\n",
      "Batch 90, Loss: 0.0076\n",
      "Epoch: 449 | Train loss: 0.05757 | Test loss: 0.16836 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0908\n",
      "Batch 10, Loss: 0.0210\n",
      "Batch 20, Loss: 0.0135\n",
      "Batch 30, Loss: 0.0054\n",
      "Batch 40, Loss: 0.0130\n",
      "Batch 50, Loss: 0.0082\n",
      "Batch 60, Loss: 0.0455\n",
      "Batch 70, Loss: 0.0127\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0009\n",
      "Epoch: 450 | Train loss: 0.03298 | Test loss: 0.06408 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0864\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0290\n",
      "Batch 30, Loss: 0.0373\n",
      "Batch 40, Loss: 0.0142\n",
      "Batch 50, Loss: 0.0065\n",
      "Batch 60, Loss: 0.0369\n",
      "Batch 70, Loss: 0.0254\n",
      "Batch 80, Loss: 0.3762\n",
      "Batch 90, Loss: 0.0153\n",
      "Epoch: 451 | Train loss: 0.05439 | Test loss: 0.04659 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0053\n",
      "Batch 10, Loss: 0.0062\n",
      "Batch 20, Loss: 0.0340\n",
      "Batch 30, Loss: 0.0074\n",
      "Batch 40, Loss: 0.0053\n",
      "Batch 50, Loss: 0.0049\n",
      "Batch 60, Loss: 0.0456\n",
      "Batch 70, Loss: 0.0032\n",
      "Batch 80, Loss: 0.1260\n",
      "Batch 90, Loss: 0.0293\n",
      "Epoch: 452 | Train loss: 0.05065 | Test loss: 0.09736 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.6091\n",
      "Batch 20, Loss: 0.0060\n",
      "Batch 30, Loss: 0.0452\n",
      "Batch 40, Loss: 0.0066\n",
      "Batch 50, Loss: 0.0270\n",
      "Batch 60, Loss: 0.0166\n",
      "Batch 70, Loss: 0.0160\n",
      "Batch 80, Loss: 0.0391\n",
      "Batch 90, Loss: 0.0043\n",
      "Epoch: 453 | Train loss: 0.03148 | Test loss: 0.46001 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0927\n",
      "Batch 10, Loss: 0.0022\n",
      "Batch 20, Loss: 0.0119\n",
      "Batch 30, Loss: 0.0140\n",
      "Batch 40, Loss: 0.0075\n",
      "Batch 50, Loss: 0.0039\n",
      "Batch 60, Loss: 0.0028\n",
      "Batch 70, Loss: 0.0054\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 454 | Train loss: 0.02493 | Test loss: 0.17568 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0093\n",
      "Batch 10, Loss: 0.0032\n",
      "Batch 20, Loss: 0.0130\n",
      "Batch 30, Loss: 0.1508\n",
      "Batch 40, Loss: 0.1230\n",
      "Batch 50, Loss: 0.1147\n",
      "Batch 60, Loss: 0.0625\n",
      "Batch 70, Loss: 0.0192\n",
      "Batch 80, Loss: 0.0376\n",
      "Batch 90, Loss: 0.0206\n",
      "Epoch: 455 | Train loss: 0.06717 | Test loss: 0.26627 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0060\n",
      "Batch 10, Loss: 0.0184\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.1340\n",
      "Batch 40, Loss: 0.0096\n",
      "Batch 50, Loss: 0.0467\n",
      "Batch 60, Loss: 0.2383\n",
      "Batch 70, Loss: 0.1575\n",
      "Batch 80, Loss: 0.0251\n",
      "Batch 90, Loss: 0.0123\n",
      "Epoch: 456 | Train loss: 0.07282 | Test loss: 0.13807 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.1162\n",
      "Batch 10, Loss: 0.0397\n",
      "Batch 20, Loss: 0.0234\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.0737\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.0151\n",
      "Batch 70, Loss: 0.0821\n",
      "Batch 80, Loss: 0.0920\n",
      "Batch 90, Loss: 0.0068\n",
      "Epoch: 457 | Train loss: 0.04384 | Test loss: 0.08959 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.1046\n",
      "Batch 10, Loss: 0.0106\n",
      "Batch 20, Loss: 0.0058\n",
      "Batch 30, Loss: 0.0088\n",
      "Batch 40, Loss: 0.0193\n",
      "Batch 50, Loss: 0.0169\n",
      "Batch 60, Loss: 0.0716\n",
      "Batch 70, Loss: 0.6531\n",
      "Batch 80, Loss: 0.0663\n",
      "Batch 90, Loss: 0.0426\n",
      "Epoch: 458 | Train loss: 0.05421 | Test loss: 0.05441 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1255\n",
      "Batch 10, Loss: 0.2930\n",
      "Batch 20, Loss: 0.0088\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0114\n",
      "Batch 50, Loss: 0.0002\n",
      "Batch 60, Loss: 0.0152\n",
      "Batch 70, Loss: 0.0205\n",
      "Batch 80, Loss: 0.0442\n",
      "Batch 90, Loss: 0.0022\n",
      "Epoch: 459 | Train loss: 0.02484 | Test loss: 0.11528 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0187\n",
      "Batch 20, Loss: 0.6504\n",
      "Batch 30, Loss: 0.0775\n",
      "Batch 40, Loss: 0.0037\n",
      "Batch 50, Loss: 0.0062\n",
      "Batch 60, Loss: 0.0056\n",
      "Batch 70, Loss: 0.0046\n",
      "Batch 80, Loss: 0.0089\n",
      "Batch 90, Loss: 0.0336\n",
      "Epoch: 460 | Train loss: 0.06411 | Test loss: 0.30705 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.1583\n",
      "Batch 20, Loss: 0.0535\n",
      "Batch 30, Loss: 0.0018\n",
      "Batch 40, Loss: 0.0096\n",
      "Batch 50, Loss: 0.0111\n",
      "Batch 60, Loss: 0.0098\n",
      "Batch 70, Loss: 0.0837\n",
      "Batch 80, Loss: 0.0012\n",
      "Batch 90, Loss: 0.1251\n",
      "Epoch: 461 | Train loss: 0.03760 | Test loss: 0.09422 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0561\n",
      "Batch 10, Loss: 0.0239\n",
      "Batch 20, Loss: 0.0517\n",
      "Batch 30, Loss: 0.0225\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0011\n",
      "Batch 70, Loss: 0.0147\n",
      "Batch 80, Loss: 0.0256\n",
      "Batch 90, Loss: 0.0073\n",
      "Epoch: 462 | Train loss: 0.03616 | Test loss: 0.46723 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.0035\n",
      "Batch 20, Loss: 0.0142\n",
      "Batch 30, Loss: 0.0077\n",
      "Batch 40, Loss: 0.1976\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0182\n",
      "Batch 90, Loss: 0.3163\n",
      "Epoch: 463 | Train loss: 0.03452 | Test loss: 0.15634 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0048\n",
      "Batch 10, Loss: 0.0679\n",
      "Batch 20, Loss: 0.0342\n",
      "Batch 30, Loss: 0.0060\n",
      "Batch 40, Loss: 0.0077\n",
      "Batch 50, Loss: 0.0544\n",
      "Batch 60, Loss: 0.0406\n",
      "Batch 70, Loss: 0.0335\n",
      "Batch 80, Loss: 0.0062\n",
      "Batch 90, Loss: 0.0035\n",
      "Epoch: 464 | Train loss: 0.04301 | Test loss: 0.33951 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0269\n",
      "Batch 20, Loss: 0.0067\n",
      "Batch 30, Loss: 0.0034\n",
      "Batch 40, Loss: 0.2371\n",
      "Batch 50, Loss: 0.0149\n",
      "Batch 60, Loss: 0.0109\n",
      "Batch 70, Loss: 0.0050\n",
      "Batch 80, Loss: 0.0063\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 465 | Train loss: 0.05310 | Test loss: 0.10866 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1420\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.0108\n",
      "Batch 40, Loss: 0.0261\n",
      "Batch 50, Loss: 0.1806\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0196\n",
      "Batch 80, Loss: 0.0132\n",
      "Batch 90, Loss: 0.0572\n",
      "Epoch: 466 | Train loss: 0.06534 | Test loss: 0.13063 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0608\n",
      "Batch 10, Loss: 0.0105\n",
      "Batch 20, Loss: 0.0214\n",
      "Batch 30, Loss: 0.0871\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0836\n",
      "Batch 70, Loss: 0.0276\n",
      "Batch 80, Loss: 0.0330\n",
      "Batch 90, Loss: 0.0309\n",
      "Epoch: 467 | Train loss: 0.04521 | Test loss: 0.07309 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0280\n",
      "Batch 10, Loss: 0.0097\n",
      "Batch 20, Loss: 0.1643\n",
      "Batch 30, Loss: 0.0235\n",
      "Batch 40, Loss: 0.0880\n",
      "Batch 50, Loss: 0.0137\n",
      "Batch 60, Loss: 0.0874\n",
      "Batch 70, Loss: 0.0005\n",
      "Batch 80, Loss: 0.0313\n",
      "Batch 90, Loss: 0.0098\n",
      "Epoch: 468 | Train loss: 0.03421 | Test loss: 0.08320 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0121\n",
      "Batch 10, Loss: 0.0179\n",
      "Batch 20, Loss: 0.0330\n",
      "Batch 30, Loss: 0.0082\n",
      "Batch 40, Loss: 0.1895\n",
      "Batch 50, Loss: 0.1277\n",
      "Batch 60, Loss: 0.0135\n",
      "Batch 70, Loss: 0.0046\n",
      "Batch 80, Loss: 0.0265\n",
      "Batch 90, Loss: 0.0353\n",
      "Epoch: 469 | Train loss: 0.05391 | Test loss: 0.38814 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0410\n",
      "Batch 10, Loss: 0.0028\n",
      "Batch 20, Loss: 0.0227\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.0158\n",
      "Batch 50, Loss: 0.0025\n",
      "Batch 60, Loss: 0.0358\n",
      "Batch 70, Loss: 0.1017\n",
      "Batch 80, Loss: 0.0136\n",
      "Batch 90, Loss: 0.0920\n",
      "Epoch: 470 | Train loss: 0.02221 | Test loss: 0.06421 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0182\n",
      "Batch 10, Loss: 0.0170\n",
      "Batch 20, Loss: 0.0455\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.0035\n",
      "Batch 60, Loss: 0.0352\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0189\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 471 | Train loss: 0.02501 | Test loss: 0.32515 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0283\n",
      "Batch 10, Loss: 0.0485\n",
      "Batch 20, Loss: 0.0105\n",
      "Batch 30, Loss: 0.0099\n",
      "Batch 40, Loss: 0.0095\n",
      "Batch 50, Loss: 0.0308\n",
      "Batch 60, Loss: 0.0639\n",
      "Batch 70, Loss: 0.0182\n",
      "Batch 80, Loss: 0.0151\n",
      "Batch 90, Loss: 0.0145\n",
      "Epoch: 472 | Train loss: 0.05945 | Test loss: 0.09223 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.1057\n",
      "Batch 10, Loss: 0.0290\n",
      "Batch 20, Loss: 0.0027\n",
      "Batch 30, Loss: 0.1062\n",
      "Batch 40, Loss: 0.0428\n",
      "Batch 50, Loss: 0.0083\n",
      "Batch 60, Loss: 0.0167\n",
      "Batch 70, Loss: 0.0030\n",
      "Batch 80, Loss: 0.0306\n",
      "Batch 90, Loss: 0.0129\n",
      "Epoch: 473 | Train loss: 0.05946 | Test loss: 0.48065 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0100\n",
      "Batch 10, Loss: 0.1783\n",
      "Batch 20, Loss: 0.0326\n",
      "Batch 30, Loss: 0.0100\n",
      "Batch 40, Loss: 0.0045\n",
      "Batch 50, Loss: 0.0203\n",
      "Batch 60, Loss: 0.0030\n",
      "Batch 70, Loss: 0.1195\n",
      "Batch 80, Loss: 0.0073\n",
      "Batch 90, Loss: 0.0418\n",
      "Epoch: 474 | Train loss: 0.03189 | Test loss: 0.53806 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0104\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.1932\n",
      "Batch 30, Loss: 0.0075\n",
      "Batch 40, Loss: 0.0586\n",
      "Batch 50, Loss: 0.0462\n",
      "Batch 60, Loss: 0.0570\n",
      "Batch 70, Loss: 0.1391\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.0075\n",
      "Epoch: 475 | Train loss: 0.03606 | Test loss: 0.21641 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0555\n",
      "Batch 20, Loss: 0.0939\n",
      "Batch 30, Loss: 0.0048\n",
      "Batch 40, Loss: 0.0064\n",
      "Batch 50, Loss: 0.0618\n",
      "Batch 60, Loss: 0.0088\n",
      "Batch 70, Loss: 0.0259\n",
      "Batch 80, Loss: 0.0046\n",
      "Batch 90, Loss: 0.0462\n",
      "Epoch: 476 | Train loss: 0.07640 | Test loss: 0.34793 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0330\n",
      "Batch 10, Loss: 0.0114\n",
      "Batch 20, Loss: 0.0144\n",
      "Batch 30, Loss: 0.0092\n",
      "Batch 40, Loss: 0.0067\n",
      "Batch 50, Loss: 0.0657\n",
      "Batch 60, Loss: 0.0851\n",
      "Batch 70, Loss: 0.0338\n",
      "Batch 80, Loss: 0.2490\n",
      "Batch 90, Loss: 0.0131\n",
      "Epoch: 477 | Train loss: 0.04514 | Test loss: 0.77124 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0223\n",
      "Batch 10, Loss: 0.0096\n",
      "Batch 20, Loss: 0.0017\n",
      "Batch 30, Loss: 0.1020\n",
      "Batch 40, Loss: 0.2197\n",
      "Batch 50, Loss: 0.3850\n",
      "Batch 60, Loss: 0.0942\n",
      "Batch 70, Loss: 0.0620\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0310\n",
      "Epoch: 478 | Train loss: 0.06237 | Test loss: 0.10149 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0277\n",
      "Batch 10, Loss: 0.1410\n",
      "Batch 20, Loss: 0.0106\n",
      "Batch 30, Loss: 0.0378\n",
      "Batch 40, Loss: 0.0139\n",
      "Batch 50, Loss: 0.1704\n",
      "Batch 60, Loss: 0.1679\n",
      "Batch 70, Loss: 0.0205\n",
      "Batch 80, Loss: 0.1747\n",
      "Batch 90, Loss: 0.1150\n",
      "Epoch: 479 | Train loss: 0.06284 | Test loss: 1.18253 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0431\n",
      "Batch 10, Loss: 0.0106\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0119\n",
      "Batch 40, Loss: 0.0459\n",
      "Batch 50, Loss: 0.0032\n",
      "Batch 60, Loss: 0.0914\n",
      "Batch 70, Loss: 0.0169\n",
      "Batch 80, Loss: 0.4947\n",
      "Batch 90, Loss: 0.0601\n",
      "Epoch: 480 | Train loss: 0.06906 | Test loss: 0.12913 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0066\n",
      "Batch 10, Loss: 0.0110\n",
      "Batch 20, Loss: 0.0089\n",
      "Batch 30, Loss: 0.1873\n",
      "Batch 40, Loss: 0.2021\n",
      "Batch 50, Loss: 0.0739\n",
      "Batch 60, Loss: 0.0067\n",
      "Batch 70, Loss: 0.0414\n",
      "Batch 80, Loss: 0.2172\n",
      "Batch 90, Loss: 0.0097\n",
      "Epoch: 481 | Train loss: 0.05586 | Test loss: 0.18895 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0218\n",
      "Batch 10, Loss: 0.0124\n",
      "Batch 20, Loss: 0.1512\n",
      "Batch 30, Loss: 0.1829\n",
      "Batch 40, Loss: 0.0346\n",
      "Batch 50, Loss: 0.0516\n",
      "Batch 60, Loss: 0.0722\n",
      "Batch 70, Loss: 0.0696\n",
      "Batch 80, Loss: 0.0379\n",
      "Batch 90, Loss: 0.0030\n",
      "Epoch: 482 | Train loss: 0.06123 | Test loss: 0.24407 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1818\n",
      "Batch 10, Loss: 0.0384\n",
      "Batch 20, Loss: 0.0158\n",
      "Batch 30, Loss: 0.0236\n",
      "Batch 40, Loss: 0.1056\n",
      "Batch 50, Loss: 0.0273\n",
      "Batch 60, Loss: 0.0120\n",
      "Batch 70, Loss: 0.0054\n",
      "Batch 80, Loss: 0.0366\n",
      "Batch 90, Loss: 0.0507\n",
      "Epoch: 483 | Train loss: 0.04182 | Test loss: 0.06392 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0068\n",
      "Batch 10, Loss: 0.0040\n",
      "Batch 20, Loss: 0.0060\n",
      "Batch 30, Loss: 0.2008\n",
      "Batch 40, Loss: 0.0033\n",
      "Batch 50, Loss: 0.0036\n",
      "Batch 60, Loss: 0.0978\n",
      "Batch 70, Loss: 0.0006\n",
      "Batch 80, Loss: 0.0258\n",
      "Batch 90, Loss: 0.0202\n",
      "Epoch: 484 | Train loss: 0.06113 | Test loss: 0.29937 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0014\n",
      "Batch 10, Loss: 0.0082\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.5560\n",
      "Batch 40, Loss: 0.0040\n",
      "Batch 50, Loss: 0.0717\n",
      "Batch 60, Loss: 0.0058\n",
      "Batch 70, Loss: 0.0507\n",
      "Batch 80, Loss: 0.0415\n",
      "Batch 90, Loss: 0.0026\n",
      "Epoch: 485 | Train loss: 0.04736 | Test loss: 0.04985 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1224\n",
      "Batch 10, Loss: 0.0021\n",
      "Batch 20, Loss: 0.0235\n",
      "Batch 30, Loss: 0.0090\n",
      "Batch 40, Loss: 0.2587\n",
      "Batch 50, Loss: 0.0139\n",
      "Batch 60, Loss: 0.0573\n",
      "Batch 70, Loss: 0.0754\n",
      "Batch 80, Loss: 0.0076\n",
      "Batch 90, Loss: 0.0110\n",
      "Epoch: 486 | Train loss: 0.03337 | Test loss: 0.39864 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0019\n",
      "Batch 10, Loss: 0.0395\n",
      "Batch 20, Loss: 0.0094\n",
      "Batch 30, Loss: 0.0425\n",
      "Batch 40, Loss: 0.0486\n",
      "Batch 50, Loss: 0.0817\n",
      "Batch 60, Loss: 0.0464\n",
      "Batch 70, Loss: 0.0534\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0027\n",
      "Epoch: 487 | Train loss: 0.03747 | Test loss: 0.27490 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0705\n",
      "Batch 10, Loss: 0.0010\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0432\n",
      "Batch 40, Loss: 0.0091\n",
      "Batch 50, Loss: 0.0065\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.0154\n",
      "Batch 80, Loss: 0.0035\n",
      "Batch 90, Loss: 0.0024\n",
      "Epoch: 488 | Train loss: 0.04690 | Test loss: 0.35649 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0130\n",
      "Batch 10, Loss: 0.0126\n",
      "Batch 20, Loss: 0.2477\n",
      "Batch 30, Loss: 0.0033\n",
      "Batch 40, Loss: 0.0630\n",
      "Batch 50, Loss: 0.0746\n",
      "Batch 60, Loss: 0.3082\n",
      "Batch 70, Loss: 0.0075\n",
      "Batch 80, Loss: 0.0174\n",
      "Batch 90, Loss: 0.0732\n",
      "Epoch: 489 | Train loss: 0.06023 | Test loss: 0.37096 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0207\n",
      "Batch 10, Loss: 0.0186\n",
      "Batch 20, Loss: 0.0373\n",
      "Batch 30, Loss: 0.0010\n",
      "Batch 40, Loss: 0.0038\n",
      "Batch 50, Loss: 0.0979\n",
      "Batch 60, Loss: 0.0450\n",
      "Batch 70, Loss: 0.0033\n",
      "Batch 80, Loss: 0.0105\n",
      "Batch 90, Loss: 0.0036\n",
      "Epoch: 490 | Train loss: 0.02848 | Test loss: 0.38572 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0052\n",
      "Batch 10, Loss: 0.0175\n",
      "Batch 20, Loss: 0.2718\n",
      "Batch 30, Loss: 0.0184\n",
      "Batch 40, Loss: 0.1153\n",
      "Batch 50, Loss: 0.0453\n",
      "Batch 60, Loss: 0.0033\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0040\n",
      "Batch 90, Loss: 0.0585\n",
      "Epoch: 491 | Train loss: 0.04276 | Test loss: 0.28033 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.0313\n",
      "Batch 40, Loss: 0.0379\n",
      "Batch 50, Loss: 0.0605\n",
      "Batch 60, Loss: 0.0994\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0105\n",
      "Batch 90, Loss: 0.0171\n",
      "Epoch: 492 | Train loss: 0.03257 | Test loss: 0.16345 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0046\n",
      "Batch 10, Loss: 0.0032\n",
      "Batch 20, Loss: 0.0049\n",
      "Batch 30, Loss: 0.0050\n",
      "Batch 40, Loss: 0.0305\n",
      "Batch 50, Loss: 0.0009\n",
      "Batch 60, Loss: 0.0443\n",
      "Batch 70, Loss: 0.0053\n",
      "Batch 80, Loss: 0.0096\n",
      "Batch 90, Loss: 0.0065\n",
      "Epoch: 493 | Train loss: 0.03034 | Test loss: 0.20908 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0343\n",
      "Batch 10, Loss: 0.1423\n",
      "Batch 20, Loss: 0.0496\n",
      "Batch 30, Loss: 0.0156\n",
      "Batch 40, Loss: 0.0914\n",
      "Batch 50, Loss: 0.0174\n",
      "Batch 60, Loss: 0.0134\n",
      "Batch 70, Loss: 0.0066\n",
      "Batch 80, Loss: 0.0449\n",
      "Batch 90, Loss: 0.0086\n",
      "Epoch: 494 | Train loss: 0.07750 | Test loss: 0.32849 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0056\n",
      "Batch 10, Loss: 0.0822\n",
      "Batch 20, Loss: 0.0238\n",
      "Batch 30, Loss: 0.0099\n",
      "Batch 40, Loss: 0.0066\n",
      "Batch 50, Loss: 0.0268\n",
      "Batch 60, Loss: 0.0547\n",
      "Batch 70, Loss: 0.0997\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.0040\n",
      "Epoch: 495 | Train loss: 0.03489 | Test loss: 1.19857 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0166\n",
      "Batch 10, Loss: 0.0070\n",
      "Batch 20, Loss: 0.0101\n",
      "Batch 30, Loss: 0.0094\n",
      "Batch 40, Loss: 0.0144\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0006\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0133\n",
      "Batch 90, Loss: 0.0100\n",
      "Epoch: 496 | Train loss: 0.05163 | Test loss: 0.12206 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0070\n",
      "Batch 20, Loss: 0.0074\n",
      "Batch 30, Loss: 0.0204\n",
      "Batch 40, Loss: 0.0102\n",
      "Batch 50, Loss: 0.0199\n",
      "Batch 60, Loss: 0.0065\n",
      "Batch 70, Loss: 0.0200\n",
      "Batch 80, Loss: 0.0053\n",
      "Batch 90, Loss: 0.0003\n",
      "Epoch: 497 | Train loss: 0.03714 | Test loss: 0.27351 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0322\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.2079\n",
      "Batch 30, Loss: 0.2521\n",
      "Batch 40, Loss: 0.1085\n",
      "Batch 50, Loss: 0.0152\n",
      "Batch 60, Loss: 0.0467\n",
      "Batch 70, Loss: 0.0603\n",
      "Batch 80, Loss: 0.4385\n",
      "Batch 90, Loss: 0.0075\n",
      "Epoch: 498 | Train loss: 0.09473 | Test loss: 0.43864 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.1813\n",
      "Batch 10, Loss: 0.0130\n",
      "Batch 20, Loss: 0.0174\n",
      "Batch 30, Loss: 0.0058\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0167\n",
      "Batch 60, Loss: 0.0249\n",
      "Batch 70, Loss: 0.0178\n",
      "Batch 80, Loss: 0.2935\n",
      "Batch 90, Loss: 0.0176\n",
      "Epoch: 499 | Train loss: 0.04340 | Test loss: 0.04433 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0174\n",
      "Batch 30, Loss: 0.0102\n",
      "Batch 40, Loss: 0.0248\n",
      "Batch 50, Loss: 0.1031\n",
      "Batch 60, Loss: 0.1159\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.1495\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 500 | Train loss: 0.05494 | Test loss: 0.10731 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.1416\n",
      "Batch 20, Loss: 0.0206\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.4849\n",
      "Batch 50, Loss: 0.0093\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.1182\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0053\n",
      "Epoch: 501 | Train loss: 0.05678 | Test loss: 0.23099 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0010\n",
      "Batch 10, Loss: 0.2035\n",
      "Batch 20, Loss: 0.0412\n",
      "Batch 30, Loss: 0.1590\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0085\n",
      "Batch 70, Loss: 0.0441\n",
      "Batch 80, Loss: 0.0116\n",
      "Batch 90, Loss: 0.0022\n",
      "Epoch: 502 | Train loss: 0.04515 | Test loss: 0.10422 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0541\n",
      "Batch 10, Loss: 0.0211\n",
      "Batch 20, Loss: 0.0304\n",
      "Batch 30, Loss: 0.0116\n",
      "Batch 40, Loss: 0.0099\n",
      "Batch 50, Loss: 0.0133\n",
      "Batch 60, Loss: 0.0042\n",
      "Batch 70, Loss: 0.0154\n",
      "Batch 80, Loss: 0.1254\n",
      "Batch 90, Loss: 0.0106\n",
      "Epoch: 503 | Train loss: 0.03222 | Test loss: 0.04823 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0417\n",
      "Batch 10, Loss: 0.0098\n",
      "Batch 20, Loss: 0.0107\n",
      "Batch 30, Loss: 0.0006\n",
      "Batch 40, Loss: 0.0194\n",
      "Batch 50, Loss: 0.0161\n",
      "Batch 60, Loss: 0.0030\n",
      "Batch 70, Loss: 0.0287\n",
      "Batch 80, Loss: 0.0898\n",
      "Batch 90, Loss: 0.0295\n",
      "Epoch: 504 | Train loss: 0.03902 | Test loss: 0.51837 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0102\n",
      "Batch 10, Loss: 0.0499\n",
      "Batch 20, Loss: 0.0073\n",
      "Batch 30, Loss: 0.0135\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0295\n",
      "Batch 60, Loss: 0.0448\n",
      "Batch 70, Loss: 0.0593\n",
      "Batch 80, Loss: 0.0314\n",
      "Batch 90, Loss: 0.0110\n",
      "Epoch: 505 | Train loss: 0.05538 | Test loss: 0.13844 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0064\n",
      "Batch 10, Loss: 0.0058\n",
      "Batch 20, Loss: 0.0512\n",
      "Batch 30, Loss: 0.0050\n",
      "Batch 40, Loss: 0.0207\n",
      "Batch 50, Loss: 0.1583\n",
      "Batch 60, Loss: 0.0387\n",
      "Batch 70, Loss: 0.0007\n",
      "Batch 80, Loss: 0.0005\n",
      "Batch 90, Loss: 0.0185\n",
      "Epoch: 506 | Train loss: 0.02556 | Test loss: 0.05153 | Test accuracy: 0.49227\n",
      "Batch 0, Loss: 0.0005\n",
      "Batch 10, Loss: 0.0005\n",
      "Batch 20, Loss: 0.0031\n",
      "Batch 30, Loss: 0.0614\n",
      "Batch 40, Loss: 0.0166\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0114\n",
      "Batch 70, Loss: 0.0810\n",
      "Batch 80, Loss: 0.1896\n",
      "Batch 90, Loss: 0.0056\n",
      "Epoch: 507 | Train loss: 0.03094 | Test loss: 0.10602 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.2057\n",
      "Batch 10, Loss: 0.0067\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0128\n",
      "Batch 40, Loss: 0.0020\n",
      "Batch 50, Loss: 0.1220\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0041\n",
      "Batch 80, Loss: 0.0032\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 508 | Train loss: 0.02961 | Test loss: 0.16380 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0231\n",
      "Batch 20, Loss: 0.0019\n",
      "Batch 30, Loss: 0.0066\n",
      "Batch 40, Loss: 0.0102\n",
      "Batch 50, Loss: 0.0133\n",
      "Batch 60, Loss: 0.0296\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0703\n",
      "Batch 90, Loss: 0.0991\n",
      "Epoch: 509 | Train loss: 0.06258 | Test loss: 0.35715 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0358\n",
      "Batch 10, Loss: 0.0295\n",
      "Batch 20, Loss: 0.0377\n",
      "Batch 30, Loss: 0.0491\n",
      "Batch 40, Loss: 0.0088\n",
      "Batch 50, Loss: 0.0690\n",
      "Batch 60, Loss: 0.0066\n",
      "Batch 70, Loss: 0.0950\n",
      "Batch 80, Loss: 0.0020\n",
      "Batch 90, Loss: 0.0006\n",
      "Epoch: 510 | Train loss: 0.04809 | Test loss: 0.18551 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0134\n",
      "Batch 10, Loss: 0.0251\n",
      "Batch 20, Loss: 0.0058\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0332\n",
      "Batch 50, Loss: 0.0035\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0008\n",
      "Batch 80, Loss: 0.0011\n",
      "Batch 90, Loss: 0.0200\n",
      "Epoch: 511 | Train loss: 0.03416 | Test loss: 0.06963 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0061\n",
      "Batch 10, Loss: 0.0053\n",
      "Batch 20, Loss: 0.0243\n",
      "Batch 30, Loss: 0.1476\n",
      "Batch 40, Loss: 0.0079\n",
      "Batch 50, Loss: 0.0104\n",
      "Batch 60, Loss: 0.1003\n",
      "Batch 70, Loss: 0.1172\n",
      "Batch 80, Loss: 0.0061\n",
      "Batch 90, Loss: 0.0156\n",
      "Epoch: 512 | Train loss: 0.04897 | Test loss: 0.08537 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0627\n",
      "Batch 10, Loss: 0.0164\n",
      "Batch 20, Loss: 0.0240\n",
      "Batch 30, Loss: 0.0052\n",
      "Batch 40, Loss: 0.1182\n",
      "Batch 50, Loss: 0.0161\n",
      "Batch 60, Loss: 0.0135\n",
      "Batch 70, Loss: 0.0254\n",
      "Batch 80, Loss: 0.0086\n",
      "Batch 90, Loss: 0.0775\n",
      "Epoch: 513 | Train loss: 0.07627 | Test loss: 0.25852 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0014\n",
      "Batch 10, Loss: 0.0278\n",
      "Batch 20, Loss: 0.0446\n",
      "Batch 30, Loss: 0.0450\n",
      "Batch 40, Loss: 0.0943\n",
      "Batch 50, Loss: 0.0315\n",
      "Batch 60, Loss: 0.0529\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0580\n",
      "Batch 90, Loss: 0.0279\n",
      "Epoch: 514 | Train loss: 0.04593 | Test loss: 0.09452 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0350\n",
      "Batch 10, Loss: 0.0953\n",
      "Batch 20, Loss: 0.0111\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0499\n",
      "Batch 50, Loss: 0.0005\n",
      "Batch 60, Loss: 0.0033\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0025\n",
      "Epoch: 515 | Train loss: 0.02837 | Test loss: 0.09392 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0107\n",
      "Batch 30, Loss: 0.0070\n",
      "Batch 40, Loss: 0.0070\n",
      "Batch 50, Loss: 0.0121\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.0053\n",
      "Batch 80, Loss: 0.0057\n",
      "Batch 90, Loss: 0.4732\n",
      "Epoch: 516 | Train loss: 0.05642 | Test loss: 0.15151 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0289\n",
      "Batch 10, Loss: 0.1030\n",
      "Batch 20, Loss: 0.0148\n",
      "Batch 30, Loss: 0.1632\n",
      "Batch 40, Loss: 0.0154\n",
      "Batch 50, Loss: 0.0525\n",
      "Batch 60, Loss: 0.0050\n",
      "Batch 70, Loss: 0.1062\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0278\n",
      "Epoch: 517 | Train loss: 0.04015 | Test loss: 0.42896 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0086\n",
      "Batch 10, Loss: 0.0006\n",
      "Batch 20, Loss: 0.0218\n",
      "Batch 30, Loss: 0.0197\n",
      "Batch 40, Loss: 0.0053\n",
      "Batch 50, Loss: 0.0106\n",
      "Batch 60, Loss: 0.0416\n",
      "Batch 70, Loss: 0.0145\n",
      "Batch 80, Loss: 0.2769\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 518 | Train loss: 0.05556 | Test loss: 0.04198 | Test accuracy: 0.57732\n",
      "Batch 0, Loss: 0.0708\n",
      "Batch 10, Loss: 0.0458\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0416\n",
      "Batch 60, Loss: 0.0061\n",
      "Batch 70, Loss: 0.0305\n",
      "Batch 80, Loss: 0.0053\n",
      "Batch 90, Loss: 0.0150\n",
      "Epoch: 519 | Train loss: 0.04452 | Test loss: 0.07753 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0219\n",
      "Batch 10, Loss: 0.0028\n",
      "Batch 20, Loss: 0.0032\n",
      "Batch 30, Loss: 0.0198\n",
      "Batch 40, Loss: 0.0234\n",
      "Batch 50, Loss: 0.0113\n",
      "Batch 60, Loss: 0.0205\n",
      "Batch 70, Loss: 0.0527\n",
      "Batch 80, Loss: 0.0074\n",
      "Batch 90, Loss: 0.1584\n",
      "Epoch: 520 | Train loss: 0.02814 | Test loss: 0.08589 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0118\n",
      "Batch 20, Loss: 0.0877\n",
      "Batch 30, Loss: 0.0003\n",
      "Batch 40, Loss: 0.2293\n",
      "Batch 50, Loss: 0.0023\n",
      "Batch 60, Loss: 0.0078\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.1801\n",
      "Batch 90, Loss: 0.3560\n",
      "Epoch: 521 | Train loss: 0.03701 | Test loss: 0.63050 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0050\n",
      "Batch 10, Loss: 0.0183\n",
      "Batch 20, Loss: 0.0056\n",
      "Batch 30, Loss: 0.0147\n",
      "Batch 40, Loss: 0.3161\n",
      "Batch 50, Loss: 0.0352\n",
      "Batch 60, Loss: 0.0320\n",
      "Batch 70, Loss: 0.0197\n",
      "Batch 80, Loss: 0.2621\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 522 | Train loss: 0.05461 | Test loss: 0.44817 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0340\n",
      "Batch 10, Loss: 0.0150\n",
      "Batch 20, Loss: 0.0493\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0036\n",
      "Batch 50, Loss: 0.0286\n",
      "Batch 60, Loss: 0.0048\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0646\n",
      "Batch 90, Loss: 0.0499\n",
      "Epoch: 523 | Train loss: 0.05831 | Test loss: 0.29510 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.1717\n",
      "Batch 10, Loss: 0.0084\n",
      "Batch 20, Loss: 0.1425\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.0074\n",
      "Batch 50, Loss: 0.0275\n",
      "Batch 60, Loss: 0.0056\n",
      "Batch 70, Loss: 0.1204\n",
      "Batch 80, Loss: 0.0024\n",
      "Batch 90, Loss: 0.0201\n",
      "Epoch: 524 | Train loss: 0.06795 | Test loss: 0.08136 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0113\n",
      "Batch 10, Loss: 0.0159\n",
      "Batch 20, Loss: 0.0062\n",
      "Batch 30, Loss: 0.0253\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.0262\n",
      "Batch 60, Loss: 0.2203\n",
      "Batch 70, Loss: 0.2334\n",
      "Batch 80, Loss: 0.2128\n",
      "Batch 90, Loss: 0.0546\n",
      "Epoch: 525 | Train loss: 0.04051 | Test loss: 0.37790 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0132\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0010\n",
      "Batch 40, Loss: 0.0992\n",
      "Batch 50, Loss: 0.0296\n",
      "Batch 60, Loss: 0.0132\n",
      "Batch 70, Loss: 0.0647\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 526 | Train loss: 0.03148 | Test loss: 0.08066 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0172\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0059\n",
      "Batch 30, Loss: 0.0887\n",
      "Batch 40, Loss: 0.9754\n",
      "Batch 50, Loss: 0.0032\n",
      "Batch 60, Loss: 0.0578\n",
      "Batch 70, Loss: 0.0048\n",
      "Batch 80, Loss: 0.0409\n",
      "Batch 90, Loss: 0.0036\n",
      "Epoch: 527 | Train loss: 0.06561 | Test loss: 0.51455 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0218\n",
      "Batch 10, Loss: 0.0661\n",
      "Batch 20, Loss: 0.0505\n",
      "Batch 30, Loss: 0.0313\n",
      "Batch 40, Loss: 0.0072\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0011\n",
      "Batch 80, Loss: 0.0425\n",
      "Batch 90, Loss: 0.1415\n",
      "Epoch: 528 | Train loss: 0.04103 | Test loss: 0.06454 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0115\n",
      "Batch 10, Loss: 0.0119\n",
      "Batch 20, Loss: 0.0098\n",
      "Batch 30, Loss: 0.0216\n",
      "Batch 40, Loss: 0.1305\n",
      "Batch 50, Loss: 0.0731\n",
      "Batch 60, Loss: 0.0034\n",
      "Batch 70, Loss: 0.0118\n",
      "Batch 80, Loss: 0.0142\n",
      "Batch 90, Loss: 0.0053\n",
      "Epoch: 529 | Train loss: 0.04623 | Test loss: 0.16383 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.0511\n",
      "Batch 20, Loss: 0.0534\n",
      "Batch 30, Loss: 0.0503\n",
      "Batch 40, Loss: 0.1638\n",
      "Batch 50, Loss: 0.0879\n",
      "Batch 60, Loss: 0.0242\n",
      "Batch 70, Loss: 0.0381\n",
      "Batch 80, Loss: 0.0138\n",
      "Batch 90, Loss: 0.0682\n",
      "Epoch: 530 | Train loss: 0.04661 | Test loss: 0.13939 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.2150\n",
      "Batch 10, Loss: 0.0199\n",
      "Batch 20, Loss: 0.0065\n",
      "Batch 30, Loss: 0.0403\n",
      "Batch 40, Loss: 0.0013\n",
      "Batch 50, Loss: 0.1081\n",
      "Batch 60, Loss: 0.0260\n",
      "Batch 70, Loss: 0.0262\n",
      "Batch 80, Loss: 0.0140\n",
      "Batch 90, Loss: 0.0004\n",
      "Epoch: 531 | Train loss: 0.05536 | Test loss: 0.04603 | Test accuracy: 0.46134\n",
      "Batch 0, Loss: 0.0232\n",
      "Batch 10, Loss: 0.0066\n",
      "Batch 20, Loss: 0.0018\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0353\n",
      "Batch 50, Loss: 0.0053\n",
      "Batch 60, Loss: 0.0517\n",
      "Batch 70, Loss: 0.2324\n",
      "Batch 80, Loss: 0.1142\n",
      "Batch 90, Loss: 0.0169\n",
      "Epoch: 532 | Train loss: 0.04660 | Test loss: 0.87295 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0218\n",
      "Batch 10, Loss: 0.0145\n",
      "Batch 20, Loss: 0.0223\n",
      "Batch 30, Loss: 0.0021\n",
      "Batch 40, Loss: 0.0215\n",
      "Batch 50, Loss: 0.0026\n",
      "Batch 60, Loss: 0.0043\n",
      "Batch 70, Loss: 0.0118\n",
      "Batch 80, Loss: 0.0179\n",
      "Batch 90, Loss: 0.0012\n",
      "Epoch: 533 | Train loss: 0.02885 | Test loss: 0.07774 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0157\n",
      "Batch 20, Loss: 0.0310\n",
      "Batch 30, Loss: 0.0039\n",
      "Batch 40, Loss: 0.0034\n",
      "Batch 50, Loss: 0.0534\n",
      "Batch 60, Loss: 0.0041\n",
      "Batch 70, Loss: 0.1663\n",
      "Batch 80, Loss: 0.0079\n",
      "Batch 90, Loss: 0.0115\n",
      "Epoch: 534 | Train loss: 0.05591 | Test loss: 0.22669 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0417\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0087\n",
      "Batch 30, Loss: 0.0189\n",
      "Batch 40, Loss: 0.1089\n",
      "Batch 50, Loss: 0.3176\n",
      "Batch 60, Loss: 0.0172\n",
      "Batch 70, Loss: 0.0499\n",
      "Batch 80, Loss: 0.0247\n",
      "Batch 90, Loss: 0.0388\n",
      "Epoch: 535 | Train loss: 0.05734 | Test loss: 0.47297 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1169\n",
      "Batch 10, Loss: 0.4698\n",
      "Batch 20, Loss: 0.0121\n",
      "Batch 30, Loss: 0.0181\n",
      "Batch 40, Loss: 0.1094\n",
      "Batch 50, Loss: 0.1549\n",
      "Batch 60, Loss: 0.0070\n",
      "Batch 70, Loss: 0.3703\n",
      "Batch 80, Loss: 0.0448\n",
      "Batch 90, Loss: 0.0194\n",
      "Epoch: 536 | Train loss: 0.05687 | Test loss: 0.08490 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0207\n",
      "Batch 20, Loss: 0.1631\n",
      "Batch 30, Loss: 0.1630\n",
      "Batch 40, Loss: 0.1963\n",
      "Batch 50, Loss: 0.0118\n",
      "Batch 60, Loss: 0.0692\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0872\n",
      "Batch 90, Loss: 0.2545\n",
      "Epoch: 537 | Train loss: 0.06119 | Test loss: 0.09671 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0579\n",
      "Batch 10, Loss: 0.0009\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0097\n",
      "Batch 40, Loss: 0.0092\n",
      "Batch 50, Loss: 0.0008\n",
      "Batch 60, Loss: 0.0032\n",
      "Batch 70, Loss: 0.0060\n",
      "Batch 80, Loss: 0.0017\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 538 | Train loss: 0.04280 | Test loss: 0.16454 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0113\n",
      "Batch 10, Loss: 0.0357\n",
      "Batch 20, Loss: 0.0945\n",
      "Batch 30, Loss: 0.0010\n",
      "Batch 40, Loss: 0.0084\n",
      "Batch 50, Loss: 0.4013\n",
      "Batch 60, Loss: 0.0123\n",
      "Batch 70, Loss: 0.0260\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.3637\n",
      "Epoch: 539 | Train loss: 0.04277 | Test loss: 0.08008 | Test accuracy: 0.46392\n",
      "Batch 0, Loss: 0.0031\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0065\n",
      "Batch 30, Loss: 0.0189\n",
      "Batch 40, Loss: 0.0187\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.0286\n",
      "Batch 70, Loss: 0.0630\n",
      "Batch 80, Loss: 0.0044\n",
      "Batch 90, Loss: 0.2971\n",
      "Epoch: 540 | Train loss: 0.03614 | Test loss: 1.23393 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0155\n",
      "Batch 10, Loss: 0.0228\n",
      "Batch 20, Loss: 0.0262\n",
      "Batch 30, Loss: 0.0173\n",
      "Batch 40, Loss: 0.0074\n",
      "Batch 50, Loss: 0.0848\n",
      "Batch 60, Loss: 0.0152\n",
      "Batch 70, Loss: 0.0394\n",
      "Batch 80, Loss: 0.0303\n",
      "Batch 90, Loss: 0.0193\n",
      "Epoch: 541 | Train loss: 0.04924 | Test loss: 0.08561 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0692\n",
      "Batch 10, Loss: 0.0065\n",
      "Batch 20, Loss: 0.0118\n",
      "Batch 30, Loss: 0.0044\n",
      "Batch 40, Loss: 0.1381\n",
      "Batch 50, Loss: 0.0464\n",
      "Batch 60, Loss: 0.0044\n",
      "Batch 70, Loss: 0.0127\n",
      "Batch 80, Loss: 0.0903\n",
      "Batch 90, Loss: 0.0277\n",
      "Epoch: 542 | Train loss: 0.04724 | Test loss: 0.13751 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.0137\n",
      "Batch 20, Loss: 0.0262\n",
      "Batch 30, Loss: 0.7116\n",
      "Batch 40, Loss: 0.1546\n",
      "Batch 50, Loss: 0.0548\n",
      "Batch 60, Loss: 0.0762\n",
      "Batch 70, Loss: 0.0076\n",
      "Batch 80, Loss: 0.0157\n",
      "Batch 90, Loss: 0.0108\n",
      "Epoch: 543 | Train loss: 0.05251 | Test loss: 0.15109 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0067\n",
      "Batch 10, Loss: 0.0697\n",
      "Batch 20, Loss: 0.0033\n",
      "Batch 30, Loss: 0.0082\n",
      "Batch 40, Loss: 0.1710\n",
      "Batch 50, Loss: 0.0078\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0127\n",
      "Batch 90, Loss: 0.0014\n",
      "Epoch: 544 | Train loss: 0.04020 | Test loss: 0.06005 | Test accuracy: 0.69588\n",
      "Batch 0, Loss: 0.0358\n",
      "Batch 10, Loss: 0.0339\n",
      "Batch 20, Loss: 0.0031\n",
      "Batch 30, Loss: 0.0110\n",
      "Batch 40, Loss: 0.0034\n",
      "Batch 50, Loss: 0.0097\n",
      "Batch 60, Loss: 0.0744\n",
      "Batch 70, Loss: 0.0030\n",
      "Batch 80, Loss: 0.0141\n",
      "Batch 90, Loss: 0.0173\n",
      "Epoch: 545 | Train loss: 0.02886 | Test loss: 0.31478 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0167\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0261\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0322\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0085\n",
      "Batch 80, Loss: 0.0406\n",
      "Batch 90, Loss: 0.0566\n",
      "Epoch: 546 | Train loss: 0.04410 | Test loss: 0.07202 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0049\n",
      "Batch 20, Loss: 0.1549\n",
      "Batch 30, Loss: 0.0210\n",
      "Batch 40, Loss: 0.0161\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0009\n",
      "Batch 70, Loss: 0.0207\n",
      "Batch 80, Loss: 0.0243\n",
      "Batch 90, Loss: 0.0087\n",
      "Epoch: 547 | Train loss: 0.03262 | Test loss: 0.12137 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0342\n",
      "Batch 10, Loss: 0.0243\n",
      "Batch 20, Loss: 0.0078\n",
      "Batch 30, Loss: 0.0093\n",
      "Batch 40, Loss: 0.0481\n",
      "Batch 50, Loss: 0.0160\n",
      "Batch 60, Loss: 0.2486\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0142\n",
      "Batch 90, Loss: 0.0101\n",
      "Epoch: 548 | Train loss: 0.04469 | Test loss: 0.14424 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0007\n",
      "Batch 10, Loss: 0.0020\n",
      "Batch 20, Loss: 0.0124\n",
      "Batch 30, Loss: 0.0105\n",
      "Batch 40, Loss: 0.0015\n",
      "Batch 50, Loss: 0.0575\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0486\n",
      "Batch 80, Loss: 0.0203\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 549 | Train loss: 0.02446 | Test loss: 0.19280 | Test accuracy: 0.49485\n",
      "Batch 0, Loss: 0.0046\n",
      "Batch 10, Loss: 0.0215\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0213\n",
      "Batch 50, Loss: 0.0229\n",
      "Batch 60, Loss: 0.0103\n",
      "Batch 70, Loss: 0.0784\n",
      "Batch 80, Loss: 0.0106\n",
      "Batch 90, Loss: 0.0076\n",
      "Epoch: 550 | Train loss: 0.03870 | Test loss: 0.10620 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0216\n",
      "Batch 10, Loss: 0.0170\n",
      "Batch 20, Loss: 0.0137\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0017\n",
      "Batch 60, Loss: 0.0105\n",
      "Batch 70, Loss: 0.0284\n",
      "Batch 80, Loss: 0.0227\n",
      "Batch 90, Loss: 0.0016\n",
      "Epoch: 551 | Train loss: 0.04285 | Test loss: 1.03920 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0035\n",
      "Batch 10, Loss: 0.0091\n",
      "Batch 20, Loss: 0.0029\n",
      "Batch 30, Loss: 0.0109\n",
      "Batch 40, Loss: 0.2923\n",
      "Batch 50, Loss: 0.0200\n",
      "Batch 60, Loss: 0.0040\n",
      "Batch 70, Loss: 0.1945\n",
      "Batch 80, Loss: 0.1280\n",
      "Batch 90, Loss: 0.0170\n",
      "Epoch: 552 | Train loss: 0.08599 | Test loss: 0.06511 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0060\n",
      "Batch 10, Loss: 0.0065\n",
      "Batch 20, Loss: 0.0137\n",
      "Batch 30, Loss: 0.1874\n",
      "Batch 40, Loss: 0.0153\n",
      "Batch 50, Loss: 0.0027\n",
      "Batch 60, Loss: 0.0118\n",
      "Batch 70, Loss: 0.0721\n",
      "Batch 80, Loss: 0.0954\n",
      "Batch 90, Loss: 0.0040\n",
      "Epoch: 553 | Train loss: 0.04280 | Test loss: 0.23933 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0158\n",
      "Batch 20, Loss: 0.0030\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0729\n",
      "Batch 50, Loss: 0.0089\n",
      "Batch 60, Loss: 0.0029\n",
      "Batch 70, Loss: 0.0002\n",
      "Batch 80, Loss: 0.1359\n",
      "Batch 90, Loss: 0.0064\n",
      "Epoch: 554 | Train loss: 0.02391 | Test loss: 0.05734 | Test accuracy: 0.46907\n",
      "Batch 0, Loss: 0.0081\n",
      "Batch 10, Loss: 0.0144\n",
      "Batch 20, Loss: 0.0137\n",
      "Batch 30, Loss: 0.0055\n",
      "Batch 40, Loss: 0.0682\n",
      "Batch 50, Loss: 0.1467\n",
      "Batch 60, Loss: 0.0016\n",
      "Batch 70, Loss: 0.2113\n",
      "Batch 80, Loss: 0.0109\n",
      "Batch 90, Loss: 0.0058\n",
      "Epoch: 555 | Train loss: 0.03327 | Test loss: 3.26057 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1137\n",
      "Batch 10, Loss: 0.0111\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0179\n",
      "Batch 40, Loss: 0.0256\n",
      "Batch 50, Loss: 0.0038\n",
      "Batch 60, Loss: 0.0002\n",
      "Batch 70, Loss: 0.0027\n",
      "Batch 80, Loss: 0.0031\n",
      "Batch 90, Loss: 0.0180\n",
      "Epoch: 556 | Train loss: 0.03480 | Test loss: 0.05441 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0066\n",
      "Batch 10, Loss: 0.0100\n",
      "Batch 20, Loss: 0.0061\n",
      "Batch 30, Loss: 0.0978\n",
      "Batch 40, Loss: 0.0025\n",
      "Batch 50, Loss: 0.0012\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0020\n",
      "Batch 80, Loss: 0.0116\n",
      "Batch 90, Loss: 0.0057\n",
      "Epoch: 557 | Train loss: 0.02373 | Test loss: 0.12287 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0329\n",
      "Batch 10, Loss: 0.0289\n",
      "Batch 20, Loss: 0.1960\n",
      "Batch 30, Loss: 0.5186\n",
      "Batch 40, Loss: 0.0084\n",
      "Batch 50, Loss: 0.0066\n",
      "Batch 60, Loss: 0.0475\n",
      "Batch 70, Loss: 0.0006\n",
      "Batch 80, Loss: 0.0001\n",
      "Batch 90, Loss: 0.0452\n",
      "Epoch: 558 | Train loss: 0.04317 | Test loss: 0.39661 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0007\n",
      "Batch 10, Loss: 0.3309\n",
      "Batch 20, Loss: 0.0157\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.0369\n",
      "Batch 50, Loss: 0.1030\n",
      "Batch 60, Loss: 0.0408\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0051\n",
      "Epoch: 559 | Train loss: 0.05837 | Test loss: 0.05023 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0659\n",
      "Batch 10, Loss: 0.0211\n",
      "Batch 20, Loss: 0.0355\n",
      "Batch 30, Loss: 0.0016\n",
      "Batch 40, Loss: 0.0419\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0256\n",
      "Batch 70, Loss: 0.0038\n",
      "Batch 80, Loss: 0.0066\n",
      "Batch 90, Loss: 0.0154\n",
      "Epoch: 560 | Train loss: 0.04383 | Test loss: 0.56185 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0009\n",
      "Batch 10, Loss: 0.0073\n",
      "Batch 20, Loss: 0.0048\n",
      "Batch 30, Loss: 0.1274\n",
      "Batch 40, Loss: 0.0132\n",
      "Batch 50, Loss: 0.0041\n",
      "Batch 60, Loss: 0.0090\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0004\n",
      "Batch 90, Loss: 0.0009\n",
      "Epoch: 561 | Train loss: 0.01984 | Test loss: 0.04576 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.0150\n",
      "Batch 20, Loss: 0.0288\n",
      "Batch 30, Loss: 0.0008\n",
      "Batch 40, Loss: 0.0052\n",
      "Batch 50, Loss: 0.0787\n",
      "Batch 60, Loss: 0.0068\n",
      "Batch 70, Loss: 0.0924\n",
      "Batch 80, Loss: 0.0564\n",
      "Batch 90, Loss: 0.0314\n",
      "Epoch: 562 | Train loss: 0.04676 | Test loss: 0.11758 | Test accuracy: 0.67526\n",
      "Batch 0, Loss: 0.2165\n",
      "Batch 10, Loss: 0.0627\n",
      "Batch 20, Loss: 0.0316\n",
      "Batch 30, Loss: 0.0166\n",
      "Batch 40, Loss: 0.0098\n",
      "Batch 50, Loss: 0.0189\n",
      "Batch 60, Loss: 0.0389\n",
      "Batch 70, Loss: 0.0001\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0444\n",
      "Epoch: 563 | Train loss: 0.03837 | Test loss: 0.05728 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0145\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0347\n",
      "Batch 30, Loss: 0.0150\n",
      "Batch 40, Loss: 0.0400\n",
      "Batch 50, Loss: 0.0924\n",
      "Batch 60, Loss: 0.0023\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0604\n",
      "Batch 90, Loss: 0.0041\n",
      "Epoch: 564 | Train loss: 0.04641 | Test loss: 0.17801 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0071\n",
      "Batch 20, Loss: 0.0716\n",
      "Batch 30, Loss: 0.0033\n",
      "Batch 40, Loss: 0.0240\n",
      "Batch 50, Loss: 0.0026\n",
      "Batch 60, Loss: 0.0110\n",
      "Batch 70, Loss: 0.1102\n",
      "Batch 80, Loss: 0.0178\n",
      "Batch 90, Loss: 0.1945\n",
      "Epoch: 565 | Train loss: 0.06534 | Test loss: 0.06072 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.4386\n",
      "Batch 10, Loss: 0.0064\n",
      "Batch 20, Loss: 0.0308\n",
      "Batch 30, Loss: 0.1073\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0856\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.0222\n",
      "Batch 80, Loss: 0.0194\n",
      "Batch 90, Loss: 0.0161\n",
      "Epoch: 566 | Train loss: 0.04873 | Test loss: 0.05456 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0159\n",
      "Batch 20, Loss: 0.0546\n",
      "Batch 30, Loss: 0.0198\n",
      "Batch 40, Loss: 0.0471\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.2813\n",
      "Batch 70, Loss: 0.0477\n",
      "Batch 80, Loss: 0.0124\n",
      "Batch 90, Loss: 0.0316\n",
      "Epoch: 567 | Train loss: 0.06857 | Test loss: 0.10257 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1163\n",
      "Batch 10, Loss: 0.0062\n",
      "Batch 20, Loss: 0.7982\n",
      "Batch 30, Loss: 0.2223\n",
      "Batch 40, Loss: 0.0687\n",
      "Batch 50, Loss: 0.0223\n",
      "Batch 60, Loss: 0.0279\n",
      "Batch 70, Loss: 0.0105\n",
      "Batch 80, Loss: 0.0122\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 568 | Train loss: 0.08331 | Test loss: 0.07026 | Test accuracy: 0.51289\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.1625\n",
      "Batch 20, Loss: 0.1540\n",
      "Batch 30, Loss: 0.0199\n",
      "Batch 40, Loss: 0.0277\n",
      "Batch 50, Loss: 0.0040\n",
      "Batch 60, Loss: 0.0089\n",
      "Batch 70, Loss: 0.3346\n",
      "Batch 80, Loss: 0.0174\n",
      "Batch 90, Loss: 0.0320\n",
      "Epoch: 569 | Train loss: 0.06005 | Test loss: 0.32359 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0592\n",
      "Batch 10, Loss: 0.2207\n",
      "Batch 20, Loss: 0.0044\n",
      "Batch 30, Loss: 0.1185\n",
      "Batch 40, Loss: 0.0158\n",
      "Batch 50, Loss: 0.0111\n",
      "Batch 60, Loss: 0.1128\n",
      "Batch 70, Loss: 0.0058\n",
      "Batch 80, Loss: 0.0138\n",
      "Batch 90, Loss: 0.0438\n",
      "Epoch: 570 | Train loss: 0.06401 | Test loss: 0.05983 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0048\n",
      "Batch 10, Loss: 0.0671\n",
      "Batch 20, Loss: 0.0076\n",
      "Batch 30, Loss: 0.0244\n",
      "Batch 40, Loss: 0.0079\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0139\n",
      "Batch 70, Loss: 0.0698\n",
      "Batch 80, Loss: 0.0164\n",
      "Batch 90, Loss: 0.0010\n",
      "Epoch: 571 | Train loss: 0.04277 | Test loss: 0.22003 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0212\n",
      "Batch 10, Loss: 0.0173\n",
      "Batch 20, Loss: 0.0152\n",
      "Batch 30, Loss: 0.0074\n",
      "Batch 40, Loss: 0.0084\n",
      "Batch 50, Loss: 0.0695\n",
      "Batch 60, Loss: 0.0070\n",
      "Batch 70, Loss: 0.0970\n",
      "Batch 80, Loss: 0.3104\n",
      "Batch 90, Loss: 0.1434\n",
      "Epoch: 572 | Train loss: 0.08069 | Test loss: 0.10004 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0586\n",
      "Batch 10, Loss: 0.1003\n",
      "Batch 20, Loss: 0.0999\n",
      "Batch 30, Loss: 0.1088\n",
      "Batch 40, Loss: 0.0019\n",
      "Batch 50, Loss: 0.0097\n",
      "Batch 60, Loss: 0.0644\n",
      "Batch 70, Loss: 0.0136\n",
      "Batch 80, Loss: 0.0051\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 573 | Train loss: 0.07046 | Test loss: 0.62990 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0039\n",
      "Batch 10, Loss: 0.0372\n",
      "Batch 20, Loss: 0.0206\n",
      "Batch 30, Loss: 0.0260\n",
      "Batch 40, Loss: 0.1164\n",
      "Batch 50, Loss: 0.0248\n",
      "Batch 60, Loss: 0.0008\n",
      "Batch 70, Loss: 0.0253\n",
      "Batch 80, Loss: 0.0024\n",
      "Batch 90, Loss: 0.0222\n",
      "Epoch: 574 | Train loss: 0.03806 | Test loss: 0.04728 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0015\n",
      "Batch 10, Loss: 0.0076\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0132\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0301\n",
      "Batch 80, Loss: 0.0007\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 575 | Train loss: 0.03002 | Test loss: 0.33987 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0022\n",
      "Batch 10, Loss: 0.0304\n",
      "Batch 20, Loss: 0.0353\n",
      "Batch 30, Loss: 0.0054\n",
      "Batch 40, Loss: 0.1053\n",
      "Batch 50, Loss: 0.0560\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0145\n",
      "Batch 80, Loss: 0.0462\n",
      "Batch 90, Loss: 0.0132\n",
      "Epoch: 576 | Train loss: 0.03986 | Test loss: 0.07655 | Test accuracy: 0.42010\n",
      "Batch 0, Loss: 0.1376\n",
      "Batch 10, Loss: 0.0628\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0195\n",
      "Batch 50, Loss: 0.0102\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0616\n",
      "Batch 80, Loss: 0.1855\n",
      "Batch 90, Loss: 0.0089\n",
      "Epoch: 577 | Train loss: 0.04294 | Test loss: 0.14581 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0985\n",
      "Batch 10, Loss: 0.2040\n",
      "Batch 20, Loss: 0.0065\n",
      "Batch 30, Loss: 0.1321\n",
      "Batch 40, Loss: 0.0213\n",
      "Batch 50, Loss: 0.0121\n",
      "Batch 60, Loss: 0.0121\n",
      "Batch 70, Loss: 0.1316\n",
      "Batch 80, Loss: 0.0067\n",
      "Batch 90, Loss: 0.2125\n",
      "Epoch: 578 | Train loss: 0.07680 | Test loss: 0.59643 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0499\n",
      "Batch 10, Loss: 0.0741\n",
      "Batch 20, Loss: 0.0508\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0823\n",
      "Batch 50, Loss: 0.0583\n",
      "Batch 60, Loss: 0.0008\n",
      "Batch 70, Loss: 0.0632\n",
      "Batch 80, Loss: 0.0111\n",
      "Batch 90, Loss: 0.0088\n",
      "Epoch: 579 | Train loss: 0.05791 | Test loss: 1.13202 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0045\n",
      "Batch 10, Loss: 0.0308\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.0047\n",
      "Batch 40, Loss: 0.0085\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0049\n",
      "Batch 70, Loss: 0.0174\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0003\n",
      "Epoch: 580 | Train loss: 0.03178 | Test loss: 0.04509 | Test accuracy: 0.50000\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0193\n",
      "Batch 30, Loss: 0.0007\n",
      "Batch 40, Loss: 0.1876\n",
      "Batch 50, Loss: 0.0368\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0243\n",
      "Batch 90, Loss: 0.0398\n",
      "Epoch: 581 | Train loss: 0.04030 | Test loss: 0.18187 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0448\n",
      "Batch 10, Loss: 0.1442\n",
      "Batch 20, Loss: 0.0109\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.2073\n",
      "Batch 50, Loss: 0.0052\n",
      "Batch 60, Loss: 0.0085\n",
      "Batch 70, Loss: 0.0135\n",
      "Batch 80, Loss: 0.0418\n",
      "Batch 90, Loss: 0.5514\n",
      "Epoch: 582 | Train loss: 0.05995 | Test loss: 0.30735 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0288\n",
      "Batch 10, Loss: 0.4079\n",
      "Batch 20, Loss: 0.1224\n",
      "Batch 30, Loss: 0.0301\n",
      "Batch 40, Loss: 0.0050\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0058\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0767\n",
      "Batch 90, Loss: 0.0072\n",
      "Epoch: 583 | Train loss: 0.06855 | Test loss: 0.04350 | Test accuracy: 0.56959\n",
      "Batch 0, Loss: 0.0120\n",
      "Batch 10, Loss: 0.0488\n",
      "Batch 20, Loss: 0.0046\n",
      "Batch 30, Loss: 0.0094\n",
      "Batch 40, Loss: 0.0449\n",
      "Batch 50, Loss: 0.0663\n",
      "Batch 60, Loss: 0.0907\n",
      "Batch 70, Loss: 0.0056\n",
      "Batch 80, Loss: 0.0195\n",
      "Batch 90, Loss: 0.0305\n",
      "Epoch: 584 | Train loss: 0.03978 | Test loss: 0.11362 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0320\n",
      "Batch 10, Loss: 0.0166\n",
      "Batch 20, Loss: 0.3215\n",
      "Batch 30, Loss: 0.0135\n",
      "Batch 40, Loss: 0.2645\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0102\n",
      "Batch 70, Loss: 0.0142\n",
      "Batch 80, Loss: 0.0399\n",
      "Batch 90, Loss: 0.0147\n",
      "Epoch: 585 | Train loss: 0.05091 | Test loss: 0.57899 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0059\n",
      "Batch 10, Loss: 0.1517\n",
      "Batch 20, Loss: 0.0938\n",
      "Batch 30, Loss: 0.0193\n",
      "Batch 40, Loss: 0.0331\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0153\n",
      "Batch 70, Loss: 0.0033\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0456\n",
      "Epoch: 586 | Train loss: 0.03608 | Test loss: 0.36953 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0250\n",
      "Batch 20, Loss: 0.0017\n",
      "Batch 30, Loss: 0.0091\n",
      "Batch 40, Loss: 0.0204\n",
      "Batch 50, Loss: 0.0092\n",
      "Batch 60, Loss: 0.0193\n",
      "Batch 70, Loss: 0.0344\n",
      "Batch 80, Loss: 0.0204\n",
      "Batch 90, Loss: 0.0036\n",
      "Epoch: 587 | Train loss: 0.04310 | Test loss: 0.38265 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0104\n",
      "Batch 20, Loss: 0.0172\n",
      "Batch 30, Loss: 0.0089\n",
      "Batch 40, Loss: 0.0136\n",
      "Batch 50, Loss: 0.0160\n",
      "Batch 60, Loss: 0.2557\n",
      "Batch 70, Loss: 0.0041\n",
      "Batch 80, Loss: 0.0186\n",
      "Batch 90, Loss: 0.0084\n",
      "Epoch: 588 | Train loss: 0.04586 | Test loss: 0.38656 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.0187\n",
      "Batch 20, Loss: 0.0113\n",
      "Batch 30, Loss: 0.0052\n",
      "Batch 40, Loss: 0.0050\n",
      "Batch 50, Loss: 0.0069\n",
      "Batch 60, Loss: 0.0117\n",
      "Batch 70, Loss: 0.0035\n",
      "Batch 80, Loss: 0.0084\n",
      "Batch 90, Loss: 0.3714\n",
      "Epoch: 589 | Train loss: 0.02562 | Test loss: 0.18611 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0455\n",
      "Batch 10, Loss: 0.0029\n",
      "Batch 20, Loss: 0.0190\n",
      "Batch 30, Loss: 0.0741\n",
      "Batch 40, Loss: 0.0047\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 60, Loss: 0.0176\n",
      "Batch 70, Loss: 0.1211\n",
      "Batch 80, Loss: 0.0329\n",
      "Batch 90, Loss: 0.1198\n",
      "Epoch: 590 | Train loss: 0.07128 | Test loss: 0.70034 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0843\n",
      "Batch 10, Loss: 0.2718\n",
      "Batch 20, Loss: 0.0079\n",
      "Batch 30, Loss: 0.0842\n",
      "Batch 40, Loss: 0.0244\n",
      "Batch 50, Loss: 0.0484\n",
      "Batch 60, Loss: 0.0696\n",
      "Batch 70, Loss: 0.0204\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0159\n",
      "Epoch: 591 | Train loss: 0.03944 | Test loss: 0.12305 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.0308\n",
      "Batch 10, Loss: 0.0002\n",
      "Batch 20, Loss: 0.0655\n",
      "Batch 30, Loss: 0.0307\n",
      "Batch 40, Loss: 0.0241\n",
      "Batch 50, Loss: 0.0612\n",
      "Batch 60, Loss: 0.0023\n",
      "Batch 70, Loss: 0.0456\n",
      "Batch 80, Loss: 0.0199\n",
      "Batch 90, Loss: 0.0043\n",
      "Epoch: 592 | Train loss: 0.03484 | Test loss: 0.13550 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0155\n",
      "Batch 10, Loss: 0.0020\n",
      "Batch 20, Loss: 0.0309\n",
      "Batch 30, Loss: 0.3075\n",
      "Batch 40, Loss: 0.0722\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.2184\n",
      "Batch 70, Loss: 0.0053\n",
      "Batch 80, Loss: 0.0076\n",
      "Batch 90, Loss: 0.0989\n",
      "Epoch: 593 | Train loss: 0.08422 | Test loss: 0.13956 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1404\n",
      "Batch 10, Loss: 0.0236\n",
      "Batch 20, Loss: 0.0594\n",
      "Batch 30, Loss: 0.3676\n",
      "Batch 40, Loss: 0.0086\n",
      "Batch 50, Loss: 0.0609\n",
      "Batch 60, Loss: 0.0471\n",
      "Batch 70, Loss: 0.0021\n",
      "Batch 80, Loss: 0.1067\n",
      "Batch 90, Loss: 0.0129\n",
      "Epoch: 594 | Train loss: 0.07055 | Test loss: 0.31744 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0238\n",
      "Batch 10, Loss: 0.0155\n",
      "Batch 20, Loss: 0.0040\n",
      "Batch 30, Loss: 0.0165\n",
      "Batch 40, Loss: 0.0135\n",
      "Batch 50, Loss: 0.0103\n",
      "Batch 60, Loss: 0.3065\n",
      "Batch 70, Loss: 0.0042\n",
      "Batch 80, Loss: 0.1222\n",
      "Batch 90, Loss: 0.0172\n",
      "Epoch: 595 | Train loss: 0.04081 | Test loss: 0.18221 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0402\n",
      "Batch 10, Loss: 0.0249\n",
      "Batch 20, Loss: 0.0050\n",
      "Batch 30, Loss: 0.0578\n",
      "Batch 40, Loss: 0.0136\n",
      "Batch 50, Loss: 0.0084\n",
      "Batch 60, Loss: 0.0314\n",
      "Batch 70, Loss: 0.0081\n",
      "Batch 80, Loss: 0.0242\n",
      "Batch 90, Loss: 0.0990\n",
      "Epoch: 596 | Train loss: 0.03614 | Test loss: 0.12475 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0076\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0077\n",
      "Batch 30, Loss: 0.1976\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0114\n",
      "Batch 60, Loss: 0.0008\n",
      "Batch 70, Loss: 0.0838\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0324\n",
      "Epoch: 597 | Train loss: 0.02828 | Test loss: 0.04613 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0032\n",
      "Batch 10, Loss: 0.1270\n",
      "Batch 20, Loss: 0.0088\n",
      "Batch 30, Loss: 0.2081\n",
      "Batch 40, Loss: 0.0175\n",
      "Batch 50, Loss: 0.0280\n",
      "Batch 60, Loss: 0.0099\n",
      "Batch 70, Loss: 0.0259\n",
      "Batch 80, Loss: 0.0012\n",
      "Batch 90, Loss: 0.0476\n",
      "Epoch: 598 | Train loss: 0.03807 | Test loss: 0.24658 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0003\n",
      "Batch 10, Loss: 0.0859\n",
      "Batch 20, Loss: 0.0162\n",
      "Batch 30, Loss: 0.0412\n",
      "Batch 40, Loss: 0.0259\n",
      "Batch 50, Loss: 0.0050\n",
      "Batch 60, Loss: 0.0277\n",
      "Batch 70, Loss: 0.0202\n",
      "Batch 80, Loss: 0.1721\n",
      "Batch 90, Loss: 0.0050\n",
      "Epoch: 599 | Train loss: 0.03946 | Test loss: 0.11346 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0215\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.0233\n",
      "Batch 40, Loss: 0.0465\n",
      "Batch 50, Loss: 0.0064\n",
      "Batch 60, Loss: 0.0122\n",
      "Batch 70, Loss: 0.1409\n",
      "Batch 80, Loss: 0.0660\n",
      "Batch 90, Loss: 0.0489\n",
      "Epoch: 600 | Train loss: 0.06546 | Test loss: 0.26058 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0147\n",
      "Batch 20, Loss: 0.0305\n",
      "Batch 30, Loss: 0.3428\n",
      "Batch 40, Loss: 0.0028\n",
      "Batch 50, Loss: 0.0118\n",
      "Batch 60, Loss: 0.0209\n",
      "Batch 70, Loss: 0.6473\n",
      "Batch 80, Loss: 0.0713\n",
      "Batch 90, Loss: 0.1411\n",
      "Epoch: 601 | Train loss: 0.08453 | Test loss: 0.15765 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0445\n",
      "Batch 10, Loss: 0.0438\n",
      "Batch 20, Loss: 0.0520\n",
      "Batch 30, Loss: 0.0025\n",
      "Batch 40, Loss: 0.0188\n",
      "Batch 50, Loss: 0.0127\n",
      "Batch 60, Loss: 0.9444\n",
      "Batch 70, Loss: 0.0221\n",
      "Batch 80, Loss: 0.0032\n",
      "Batch 90, Loss: 0.0423\n",
      "Epoch: 602 | Train loss: 0.05493 | Test loss: 0.18239 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0160\n",
      "Batch 10, Loss: 0.0396\n",
      "Batch 20, Loss: 0.0519\n",
      "Batch 30, Loss: 0.0046\n",
      "Batch 40, Loss: 0.0402\n",
      "Batch 50, Loss: 0.0106\n",
      "Batch 60, Loss: 0.0165\n",
      "Batch 70, Loss: 0.0033\n",
      "Batch 80, Loss: 0.0287\n",
      "Batch 90, Loss: 0.0074\n",
      "Epoch: 603 | Train loss: 0.05640 | Test loss: 0.93024 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0148\n",
      "Batch 10, Loss: 0.0191\n",
      "Batch 20, Loss: 0.0117\n",
      "Batch 30, Loss: 0.0026\n",
      "Batch 40, Loss: 0.0054\n",
      "Batch 50, Loss: 0.0016\n",
      "Batch 60, Loss: 0.0225\n",
      "Batch 70, Loss: 0.0093\n",
      "Batch 80, Loss: 0.5995\n",
      "Batch 90, Loss: 0.0667\n",
      "Epoch: 604 | Train loss: 0.04334 | Test loss: 0.37372 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0023\n",
      "Batch 10, Loss: 0.0141\n",
      "Batch 20, Loss: 0.0108\n",
      "Batch 30, Loss: 0.0064\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.0020\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.0917\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0009\n",
      "Epoch: 605 | Train loss: 0.02073 | Test loss: 0.16583 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0046\n",
      "Batch 10, Loss: 0.0282\n",
      "Batch 20, Loss: 0.0212\n",
      "Batch 30, Loss: 0.0158\n",
      "Batch 40, Loss: 0.0098\n",
      "Batch 50, Loss: 0.0640\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0031\n",
      "Batch 80, Loss: 0.0204\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 606 | Train loss: 0.02214 | Test loss: 0.09164 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.0244\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0919\n",
      "Batch 40, Loss: 0.0341\n",
      "Batch 50, Loss: 0.0432\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.1286\n",
      "Batch 80, Loss: 0.0134\n",
      "Batch 90, Loss: 0.0253\n",
      "Epoch: 607 | Train loss: 0.04270 | Test loss: 0.21593 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0102\n",
      "Batch 10, Loss: 0.0129\n",
      "Batch 20, Loss: 0.0261\n",
      "Batch 30, Loss: 0.0166\n",
      "Batch 40, Loss: 0.0730\n",
      "Batch 50, Loss: 0.0084\n",
      "Batch 60, Loss: 0.0158\n",
      "Batch 70, Loss: 0.0012\n",
      "Batch 80, Loss: 0.0085\n",
      "Batch 90, Loss: 0.0342\n",
      "Epoch: 608 | Train loss: 0.02633 | Test loss: 0.08130 | Test accuracy: 0.51546\n",
      "Batch 0, Loss: 0.0053\n",
      "Batch 10, Loss: 0.0083\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.3547\n",
      "Batch 40, Loss: 0.0079\n",
      "Batch 50, Loss: 0.9567\n",
      "Batch 60, Loss: 0.0023\n",
      "Batch 70, Loss: 0.0282\n",
      "Batch 80, Loss: 0.1884\n",
      "Batch 90, Loss: 0.0180\n",
      "Epoch: 609 | Train loss: 0.07066 | Test loss: 0.33730 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0397\n",
      "Batch 10, Loss: 0.0228\n",
      "Batch 20, Loss: 0.0044\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0385\n",
      "Batch 60, Loss: 0.0966\n",
      "Batch 70, Loss: 0.0667\n",
      "Batch 80, Loss: 0.0052\n",
      "Batch 90, Loss: 0.0248\n",
      "Epoch: 610 | Train loss: 0.04917 | Test loss: 0.35913 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0561\n",
      "Batch 20, Loss: 0.0050\n",
      "Batch 30, Loss: 0.0580\n",
      "Batch 40, Loss: 0.0148\n",
      "Batch 50, Loss: 0.0098\n",
      "Batch 60, Loss: 0.0262\n",
      "Batch 70, Loss: 0.0074\n",
      "Batch 80, Loss: 0.1285\n",
      "Batch 90, Loss: 0.0858\n",
      "Epoch: 611 | Train loss: 0.08272 | Test loss: 0.04531 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0140\n",
      "Batch 10, Loss: 0.0159\n",
      "Batch 20, Loss: 0.0515\n",
      "Batch 30, Loss: 0.0590\n",
      "Batch 40, Loss: 0.0025\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.0027\n",
      "Batch 70, Loss: 0.1357\n",
      "Batch 80, Loss: 0.0303\n",
      "Batch 90, Loss: 0.0173\n",
      "Epoch: 612 | Train loss: 0.05832 | Test loss: 0.15321 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0091\n",
      "Batch 10, Loss: 0.0137\n",
      "Batch 20, Loss: 0.0231\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.0021\n",
      "Batch 50, Loss: 0.3717\n",
      "Batch 60, Loss: 0.0100\n",
      "Batch 70, Loss: 0.0100\n",
      "Batch 80, Loss: 0.0112\n",
      "Batch 90, Loss: 0.0084\n",
      "Epoch: 613 | Train loss: 0.04942 | Test loss: 0.09185 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0770\n",
      "Batch 10, Loss: 0.0062\n",
      "Batch 20, Loss: 0.0590\n",
      "Batch 30, Loss: 0.0037\n",
      "Batch 40, Loss: 0.0126\n",
      "Batch 50, Loss: 0.0003\n",
      "Batch 60, Loss: 0.0682\n",
      "Batch 70, Loss: 0.0029\n",
      "Batch 80, Loss: 0.0091\n",
      "Batch 90, Loss: 0.1647\n",
      "Epoch: 614 | Train loss: 0.04298 | Test loss: 0.10430 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0402\n",
      "Batch 10, Loss: 0.0124\n",
      "Batch 20, Loss: 0.0373\n",
      "Batch 30, Loss: 0.0008\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0252\n",
      "Batch 60, Loss: 0.1098\n",
      "Batch 70, Loss: 0.0071\n",
      "Batch 80, Loss: 0.0290\n",
      "Batch 90, Loss: 0.0097\n",
      "Epoch: 615 | Train loss: 0.05166 | Test loss: 0.06337 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0086\n",
      "Batch 10, Loss: 0.0433\n",
      "Batch 20, Loss: 0.0223\n",
      "Batch 30, Loss: 0.0125\n",
      "Batch 40, Loss: 0.0259\n",
      "Batch 50, Loss: 0.0291\n",
      "Batch 60, Loss: 0.0098\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0121\n",
      "Batch 90, Loss: 0.0008\n",
      "Epoch: 616 | Train loss: 0.03594 | Test loss: 0.11974 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0341\n",
      "Batch 10, Loss: 0.0012\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0478\n",
      "Batch 40, Loss: 0.2073\n",
      "Batch 50, Loss: 0.0988\n",
      "Batch 60, Loss: 0.0095\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.1978\n",
      "Batch 90, Loss: 0.0010\n",
      "Epoch: 617 | Train loss: 0.04327 | Test loss: 0.12800 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0203\n",
      "Batch 10, Loss: 0.0017\n",
      "Batch 20, Loss: 0.1094\n",
      "Batch 30, Loss: 0.0072\n",
      "Batch 40, Loss: 0.0593\n",
      "Batch 50, Loss: 0.0073\n",
      "Batch 60, Loss: 0.0012\n",
      "Batch 70, Loss: 0.0007\n",
      "Batch 80, Loss: 0.0181\n",
      "Batch 90, Loss: 0.0348\n",
      "Epoch: 618 | Train loss: 0.05684 | Test loss: 0.61546 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0032\n",
      "Batch 10, Loss: 0.2012\n",
      "Batch 20, Loss: 0.0040\n",
      "Batch 30, Loss: 0.0132\n",
      "Batch 40, Loss: 0.0110\n",
      "Batch 50, Loss: 0.0049\n",
      "Batch 60, Loss: 0.0228\n",
      "Batch 70, Loss: 0.0180\n",
      "Batch 80, Loss: 0.0575\n",
      "Batch 90, Loss: 0.0164\n",
      "Epoch: 619 | Train loss: 0.05166 | Test loss: 0.29186 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0808\n",
      "Batch 20, Loss: 0.0464\n",
      "Batch 30, Loss: 0.0261\n",
      "Batch 40, Loss: 0.0149\n",
      "Batch 50, Loss: 0.0127\n",
      "Batch 60, Loss: 0.0113\n",
      "Batch 70, Loss: 0.4431\n",
      "Batch 80, Loss: 0.5667\n",
      "Batch 90, Loss: 0.0513\n",
      "Epoch: 620 | Train loss: 0.03581 | Test loss: 1.42787 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.3268\n",
      "Batch 10, Loss: 0.0112\n",
      "Batch 20, Loss: 0.0337\n",
      "Batch 30, Loss: 0.0014\n",
      "Batch 40, Loss: 0.0074\n",
      "Batch 50, Loss: 0.3272\n",
      "Batch 60, Loss: 0.0371\n",
      "Batch 70, Loss: 0.0467\n",
      "Batch 80, Loss: 0.0214\n",
      "Batch 90, Loss: 0.0032\n",
      "Epoch: 621 | Train loss: 0.06277 | Test loss: 0.04938 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.1360\n",
      "Batch 10, Loss: 0.0313\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0131\n",
      "Batch 40, Loss: 0.0652\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.0160\n",
      "Batch 70, Loss: 0.0078\n",
      "Batch 80, Loss: 0.0600\n",
      "Batch 90, Loss: 0.0010\n",
      "Epoch: 622 | Train loss: 0.05657 | Test loss: 0.10992 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0264\n",
      "Batch 10, Loss: 0.1908\n",
      "Batch 20, Loss: 0.0080\n",
      "Batch 30, Loss: 0.0126\n",
      "Batch 40, Loss: 0.0509\n",
      "Batch 50, Loss: 0.0273\n",
      "Batch 60, Loss: 0.0399\n",
      "Batch 70, Loss: 0.0343\n",
      "Batch 80, Loss: 0.0200\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 623 | Train loss: 0.03606 | Test loss: 0.31985 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0126\n",
      "Batch 10, Loss: 0.0341\n",
      "Batch 20, Loss: 0.0092\n",
      "Batch 30, Loss: 0.0921\n",
      "Batch 40, Loss: 0.0056\n",
      "Batch 50, Loss: 0.1193\n",
      "Batch 60, Loss: 0.1710\n",
      "Batch 70, Loss: 0.0348\n",
      "Batch 80, Loss: 0.0147\n",
      "Batch 90, Loss: 0.0173\n",
      "Epoch: 624 | Train loss: 0.09020 | Test loss: 0.04480 | Test accuracy: 0.49485\n",
      "Batch 0, Loss: 0.0058\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0113\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0954\n",
      "Batch 50, Loss: 0.0034\n",
      "Batch 60, Loss: 0.0055\n",
      "Batch 70, Loss: 0.0047\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 625 | Train loss: 0.03906 | Test loss: 0.05633 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.9500\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.0092\n",
      "Batch 40, Loss: 0.0467\n",
      "Batch 50, Loss: 0.0108\n",
      "Batch 60, Loss: 0.0249\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0185\n",
      "Batch 90, Loss: 0.0755\n",
      "Epoch: 626 | Train loss: 0.04453 | Test loss: 0.09616 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0015\n",
      "Batch 10, Loss: 0.0055\n",
      "Batch 20, Loss: 0.1306\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.0097\n",
      "Batch 50, Loss: 0.0435\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.1062\n",
      "Batch 80, Loss: 0.0021\n",
      "Batch 90, Loss: 0.0090\n",
      "Epoch: 627 | Train loss: 0.04123 | Test loss: 0.06469 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.0058\n",
      "Batch 20, Loss: 0.0651\n",
      "Batch 30, Loss: 0.0211\n",
      "Batch 40, Loss: 0.0199\n",
      "Batch 50, Loss: 0.4527\n",
      "Batch 60, Loss: 0.0300\n",
      "Batch 70, Loss: 0.0307\n",
      "Batch 80, Loss: 0.0044\n",
      "Batch 90, Loss: 0.0195\n",
      "Epoch: 628 | Train loss: 0.04496 | Test loss: 0.67000 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0470\n",
      "Batch 10, Loss: 0.0213\n",
      "Batch 20, Loss: 0.1277\n",
      "Batch 30, Loss: 0.0808\n",
      "Batch 40, Loss: 0.1262\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0264\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0025\n",
      "Epoch: 629 | Train loss: 0.04568 | Test loss: 0.24511 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0047\n",
      "Batch 10, Loss: 0.0818\n",
      "Batch 20, Loss: 0.0005\n",
      "Batch 30, Loss: 0.0106\n",
      "Batch 40, Loss: 0.0674\n",
      "Batch 50, Loss: 0.0135\n",
      "Batch 60, Loss: 0.0180\n",
      "Batch 70, Loss: 0.0021\n",
      "Batch 80, Loss: 0.1230\n",
      "Batch 90, Loss: 0.0122\n",
      "Epoch: 630 | Train loss: 0.03207 | Test loss: 0.27554 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1224\n",
      "Batch 10, Loss: 0.0106\n",
      "Batch 20, Loss: 0.0848\n",
      "Batch 30, Loss: 0.0618\n",
      "Batch 40, Loss: 0.0043\n",
      "Batch 50, Loss: 0.0047\n",
      "Batch 60, Loss: 0.0154\n",
      "Batch 70, Loss: 0.0549\n",
      "Batch 80, Loss: 0.0187\n",
      "Batch 90, Loss: 0.0057\n",
      "Epoch: 631 | Train loss: 0.04225 | Test loss: 0.16950 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0173\n",
      "Batch 10, Loss: 0.0140\n",
      "Batch 20, Loss: 0.0140\n",
      "Batch 30, Loss: 0.2678\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0121\n",
      "Batch 60, Loss: 0.4877\n",
      "Batch 70, Loss: 0.2257\n",
      "Batch 80, Loss: 0.5894\n",
      "Batch 90, Loss: 0.0094\n",
      "Epoch: 632 | Train loss: 0.08689 | Test loss: 0.10272 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0540\n",
      "Batch 10, Loss: 0.0040\n",
      "Batch 20, Loss: 0.0718\n",
      "Batch 30, Loss: 0.0635\n",
      "Batch 40, Loss: 0.0110\n",
      "Batch 50, Loss: 0.0012\n",
      "Batch 60, Loss: 0.0015\n",
      "Batch 70, Loss: 0.0032\n",
      "Batch 80, Loss: 0.0106\n",
      "Batch 90, Loss: 0.0191\n",
      "Epoch: 633 | Train loss: 0.04375 | Test loss: 0.06045 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0299\n",
      "Batch 20, Loss: 0.0774\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.0055\n",
      "Batch 50, Loss: 0.0071\n",
      "Batch 60, Loss: 0.0153\n",
      "Batch 70, Loss: 0.0081\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.0772\n",
      "Epoch: 634 | Train loss: 0.04549 | Test loss: 0.13783 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2290\n",
      "Batch 10, Loss: 0.0072\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.2171\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.1314\n",
      "Batch 60, Loss: 0.0180\n",
      "Batch 70, Loss: 0.0022\n",
      "Batch 80, Loss: 0.1249\n",
      "Batch 90, Loss: 0.0595\n",
      "Epoch: 635 | Train loss: 0.04055 | Test loss: 0.05441 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0321\n",
      "Batch 10, Loss: 0.0064\n",
      "Batch 20, Loss: 0.1155\n",
      "Batch 30, Loss: 0.0060\n",
      "Batch 40, Loss: 0.0326\n",
      "Batch 50, Loss: 0.0058\n",
      "Batch 60, Loss: 0.0497\n",
      "Batch 70, Loss: 0.1128\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0063\n",
      "Epoch: 636 | Train loss: 0.03612 | Test loss: 0.04203 | Test accuracy: 0.57990\n",
      "Batch 0, Loss: 0.0328\n",
      "Batch 10, Loss: 0.0050\n",
      "Batch 20, Loss: 0.0016\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.0035\n",
      "Batch 50, Loss: 0.0068\n",
      "Batch 60, Loss: 0.0779\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0082\n",
      "Epoch: 637 | Train loss: 0.02791 | Test loss: 0.21214 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.0035\n",
      "Batch 10, Loss: 0.0542\n",
      "Batch 20, Loss: 0.0242\n",
      "Batch 30, Loss: 0.0091\n",
      "Batch 40, Loss: 0.1901\n",
      "Batch 50, Loss: 0.0074\n",
      "Batch 60, Loss: 0.0391\n",
      "Batch 70, Loss: 0.0408\n",
      "Batch 80, Loss: 0.4318\n",
      "Batch 90, Loss: 0.0203\n",
      "Epoch: 638 | Train loss: 0.04907 | Test loss: 0.09614 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0799\n",
      "Batch 10, Loss: 0.0081\n",
      "Batch 20, Loss: 0.0224\n",
      "Batch 30, Loss: 0.1166\n",
      "Batch 40, Loss: 0.0151\n",
      "Batch 50, Loss: 0.0033\n",
      "Batch 60, Loss: 0.0027\n",
      "Batch 70, Loss: 0.0268\n",
      "Batch 80, Loss: 0.1111\n",
      "Batch 90, Loss: 0.0066\n",
      "Epoch: 639 | Train loss: 0.04009 | Test loss: 0.43358 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0011\n",
      "Batch 10, Loss: 0.0055\n",
      "Batch 20, Loss: 0.0017\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0176\n",
      "Batch 60, Loss: 0.0772\n",
      "Batch 70, Loss: 0.0477\n",
      "Batch 80, Loss: 0.0496\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 640 | Train loss: 0.06407 | Test loss: 0.14842 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0181\n",
      "Batch 10, Loss: 0.2970\n",
      "Batch 20, Loss: 0.0476\n",
      "Batch 30, Loss: 0.0157\n",
      "Batch 40, Loss: 0.0367\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.0083\n",
      "Batch 70, Loss: 0.0315\n",
      "Batch 80, Loss: 0.0095\n",
      "Batch 90, Loss: 0.0495\n",
      "Epoch: 641 | Train loss: 0.04725 | Test loss: 0.14483 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2241\n",
      "Batch 10, Loss: 0.2041\n",
      "Batch 20, Loss: 0.0444\n",
      "Batch 30, Loss: 0.0360\n",
      "Batch 40, Loss: 0.0204\n",
      "Batch 50, Loss: 0.0063\n",
      "Batch 60, Loss: 0.0098\n",
      "Batch 70, Loss: 0.0203\n",
      "Batch 80, Loss: 0.0055\n",
      "Batch 90, Loss: 0.0328\n",
      "Epoch: 642 | Train loss: 0.06615 | Test loss: 0.08129 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0592\n",
      "Batch 10, Loss: 0.0056\n",
      "Batch 20, Loss: 0.0011\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0097\n",
      "Batch 50, Loss: 0.0460\n",
      "Batch 60, Loss: 0.1776\n",
      "Batch 70, Loss: 0.0350\n",
      "Batch 80, Loss: 0.0294\n",
      "Batch 90, Loss: 0.0017\n",
      "Epoch: 643 | Train loss: 0.02340 | Test loss: 0.42490 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0738\n",
      "Batch 20, Loss: 0.6793\n",
      "Batch 30, Loss: 0.0793\n",
      "Batch 40, Loss: 0.0117\n",
      "Batch 50, Loss: 0.0164\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0317\n",
      "Epoch: 644 | Train loss: 0.04484 | Test loss: 0.06452 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.0199\n",
      "Batch 20, Loss: 0.0202\n",
      "Batch 30, Loss: 0.0047\n",
      "Batch 40, Loss: 0.0036\n",
      "Batch 50, Loss: 0.0043\n",
      "Batch 60, Loss: 0.0039\n",
      "Batch 70, Loss: 0.0059\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.2601\n",
      "Epoch: 645 | Train loss: 0.03865 | Test loss: 0.26170 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0407\n",
      "Batch 10, Loss: 0.0107\n",
      "Batch 20, Loss: 0.0292\n",
      "Batch 30, Loss: 0.0109\n",
      "Batch 40, Loss: 0.0325\n",
      "Batch 50, Loss: 0.0082\n",
      "Batch 60, Loss: 0.0269\n",
      "Batch 70, Loss: 0.0074\n",
      "Batch 80, Loss: 0.1035\n",
      "Batch 90, Loss: 0.0345\n",
      "Epoch: 646 | Train loss: 0.08243 | Test loss: 0.08260 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0973\n",
      "Batch 10, Loss: 0.0182\n",
      "Batch 20, Loss: 0.0515\n",
      "Batch 30, Loss: 0.0187\n",
      "Batch 40, Loss: 0.1386\n",
      "Batch 50, Loss: 0.0383\n",
      "Batch 60, Loss: 0.0009\n",
      "Batch 70, Loss: 0.0021\n",
      "Batch 80, Loss: 0.6064\n",
      "Batch 90, Loss: 0.0682\n",
      "Epoch: 647 | Train loss: 0.04557 | Test loss: 0.55245 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0257\n",
      "Batch 10, Loss: 0.0075\n",
      "Batch 20, Loss: 0.0108\n",
      "Batch 30, Loss: 0.2084\n",
      "Batch 40, Loss: 0.0038\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0830\n",
      "Batch 70, Loss: 0.0038\n",
      "Batch 80, Loss: 0.0149\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 648 | Train loss: 0.04343 | Test loss: 0.07903 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0280\n",
      "Batch 10, Loss: 0.0152\n",
      "Batch 20, Loss: 0.0188\n",
      "Batch 30, Loss: 0.0437\n",
      "Batch 40, Loss: 0.0031\n",
      "Batch 50, Loss: 0.0089\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.1066\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.0347\n",
      "Epoch: 649 | Train loss: 0.07630 | Test loss: 0.06813 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0145\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.6047\n",
      "Batch 30, Loss: 0.4549\n",
      "Batch 40, Loss: 0.0379\n",
      "Batch 50, Loss: 0.0164\n",
      "Batch 60, Loss: 0.0524\n",
      "Batch 70, Loss: 0.0067\n",
      "Batch 80, Loss: 0.1450\n",
      "Batch 90, Loss: 0.1385\n",
      "Epoch: 650 | Train loss: 0.06972 | Test loss: 0.06874 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0075\n",
      "Batch 20, Loss: 0.0869\n",
      "Batch 30, Loss: 0.0191\n",
      "Batch 40, Loss: 0.0035\n",
      "Batch 50, Loss: 0.0750\n",
      "Batch 60, Loss: 0.0434\n",
      "Batch 70, Loss: 0.1442\n",
      "Batch 80, Loss: 0.0170\n",
      "Batch 90, Loss: 0.0052\n",
      "Epoch: 651 | Train loss: 0.06286 | Test loss: 0.05872 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0350\n",
      "Batch 10, Loss: 0.0250\n",
      "Batch 20, Loss: 0.0483\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.0198\n",
      "Batch 50, Loss: 0.0110\n",
      "Batch 60, Loss: 0.0018\n",
      "Batch 70, Loss: 0.0442\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0021\n",
      "Epoch: 652 | Train loss: 0.03916 | Test loss: 0.38675 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0007\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.1054\n",
      "Batch 30, Loss: 0.0821\n",
      "Batch 40, Loss: 0.0023\n",
      "Batch 50, Loss: 0.0019\n",
      "Batch 60, Loss: 0.0187\n",
      "Batch 70, Loss: 0.0040\n",
      "Batch 80, Loss: 0.0082\n",
      "Batch 90, Loss: 0.0057\n",
      "Epoch: 653 | Train loss: 0.02397 | Test loss: 0.10971 | Test accuracy: 0.51546\n",
      "Batch 0, Loss: 0.0556\n",
      "Batch 10, Loss: 0.0068\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0642\n",
      "Batch 40, Loss: 0.0151\n",
      "Batch 50, Loss: 0.0082\n",
      "Batch 60, Loss: 0.0291\n",
      "Batch 70, Loss: 0.0254\n",
      "Batch 80, Loss: 0.0102\n",
      "Batch 90, Loss: 0.0284\n",
      "Epoch: 654 | Train loss: 0.05309 | Test loss: 0.15004 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0233\n",
      "Batch 10, Loss: 0.0045\n",
      "Batch 20, Loss: 0.0197\n",
      "Batch 30, Loss: 0.0663\n",
      "Batch 40, Loss: 0.0288\n",
      "Batch 50, Loss: 0.0516\n",
      "Batch 60, Loss: 0.0142\n",
      "Batch 70, Loss: 0.0089\n",
      "Batch 80, Loss: 0.0271\n",
      "Batch 90, Loss: 0.0634\n",
      "Epoch: 655 | Train loss: 0.03421 | Test loss: 0.07813 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0107\n",
      "Batch 10, Loss: 0.0022\n",
      "Batch 20, Loss: 0.1698\n",
      "Batch 30, Loss: 0.0935\n",
      "Batch 40, Loss: 0.0350\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0696\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0016\n",
      "Batch 90, Loss: 0.1029\n",
      "Epoch: 656 | Train loss: 0.01733 | Test loss: 0.11290 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0211\n",
      "Batch 20, Loss: 0.0318\n",
      "Batch 30, Loss: 0.0022\n",
      "Batch 40, Loss: 0.0534\n",
      "Batch 50, Loss: 0.0075\n",
      "Batch 60, Loss: 0.0619\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0223\n",
      "Batch 90, Loss: 0.0261\n",
      "Epoch: 657 | Train loss: 0.02875 | Test loss: 0.05713 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.2527\n",
      "Batch 10, Loss: 0.0209\n",
      "Batch 20, Loss: 0.0090\n",
      "Batch 30, Loss: 0.2008\n",
      "Batch 40, Loss: 0.0643\n",
      "Batch 50, Loss: 0.0527\n",
      "Batch 60, Loss: 0.0179\n",
      "Batch 70, Loss: 0.0320\n",
      "Batch 80, Loss: 0.0413\n",
      "Batch 90, Loss: 0.0290\n",
      "Epoch: 658 | Train loss: 0.09817 | Test loss: 0.28330 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0801\n",
      "Batch 10, Loss: 0.0152\n",
      "Batch 20, Loss: 0.0873\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.0098\n",
      "Batch 60, Loss: 0.0186\n",
      "Batch 70, Loss: 0.0029\n",
      "Batch 80, Loss: 0.2584\n",
      "Batch 90, Loss: 0.0085\n",
      "Epoch: 659 | Train loss: 0.05590 | Test loss: 0.51054 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1246\n",
      "Batch 10, Loss: 0.0049\n",
      "Batch 20, Loss: 0.0163\n",
      "Batch 30, Loss: 0.0098\n",
      "Batch 40, Loss: 0.2330\n",
      "Batch 50, Loss: 0.3586\n",
      "Batch 60, Loss: 0.0360\n",
      "Batch 70, Loss: 0.2012\n",
      "Batch 80, Loss: 0.3705\n",
      "Batch 90, Loss: 0.0465\n",
      "Epoch: 660 | Train loss: 0.08388 | Test loss: 0.10598 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0323\n",
      "Batch 10, Loss: 0.0158\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.0068\n",
      "Batch 40, Loss: 0.0007\n",
      "Batch 50, Loss: 0.0026\n",
      "Batch 60, Loss: 0.0384\n",
      "Batch 70, Loss: 0.0267\n",
      "Batch 80, Loss: 0.0211\n",
      "Batch 90, Loss: 0.0192\n",
      "Epoch: 661 | Train loss: 0.03630 | Test loss: 0.19697 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0998\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0179\n",
      "Batch 30, Loss: 0.0300\n",
      "Batch 40, Loss: 0.0391\n",
      "Batch 50, Loss: 0.0425\n",
      "Batch 60, Loss: 0.0153\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0216\n",
      "Batch 90, Loss: 0.0148\n",
      "Epoch: 662 | Train loss: 0.05104 | Test loss: 0.48359 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0666\n",
      "Batch 10, Loss: 0.0031\n",
      "Batch 20, Loss: 0.0280\n",
      "Batch 30, Loss: 0.0041\n",
      "Batch 40, Loss: 0.0216\n",
      "Batch 50, Loss: 0.0009\n",
      "Batch 60, Loss: 0.0875\n",
      "Batch 70, Loss: 0.0191\n",
      "Batch 80, Loss: 0.0259\n",
      "Batch 90, Loss: 0.0102\n",
      "Epoch: 663 | Train loss: 0.03792 | Test loss: 0.09573 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0028\n",
      "Batch 10, Loss: 0.2021\n",
      "Batch 20, Loss: 0.0074\n",
      "Batch 30, Loss: 0.0069\n",
      "Batch 40, Loss: 0.0925\n",
      "Batch 50, Loss: 0.0048\n",
      "Batch 60, Loss: 0.0377\n",
      "Batch 70, Loss: 0.2746\n",
      "Batch 80, Loss: 0.0016\n",
      "Batch 90, Loss: 0.0175\n",
      "Epoch: 664 | Train loss: 0.03745 | Test loss: 0.08730 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0966\n",
      "Batch 10, Loss: 0.0008\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.2061\n",
      "Batch 40, Loss: 0.0052\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0232\n",
      "Batch 70, Loss: 0.0071\n",
      "Batch 80, Loss: 0.1050\n",
      "Batch 90, Loss: 0.3017\n",
      "Epoch: 665 | Train loss: 0.04130 | Test loss: 0.77681 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0045\n",
      "Batch 10, Loss: 0.1076\n",
      "Batch 20, Loss: 0.0676\n",
      "Batch 30, Loss: 0.0071\n",
      "Batch 40, Loss: 0.0299\n",
      "Batch 50, Loss: 0.0008\n",
      "Batch 60, Loss: 0.0141\n",
      "Batch 70, Loss: 0.0057\n",
      "Batch 80, Loss: 0.1051\n",
      "Batch 90, Loss: 0.0202\n",
      "Epoch: 666 | Train loss: 0.03985 | Test loss: 0.05377 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0036\n",
      "Batch 10, Loss: 0.0099\n",
      "Batch 20, Loss: 0.0816\n",
      "Batch 30, Loss: 0.0509\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0247\n",
      "Batch 70, Loss: 0.0146\n",
      "Batch 80, Loss: 0.0020\n",
      "Batch 90, Loss: 0.0127\n",
      "Epoch: 667 | Train loss: 0.05943 | Test loss: 0.04544 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0077\n",
      "Batch 20, Loss: 0.0196\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.0770\n",
      "Batch 50, Loss: 0.0035\n",
      "Batch 60, Loss: 0.1014\n",
      "Batch 70, Loss: 0.1314\n",
      "Batch 80, Loss: 0.0192\n",
      "Batch 90, Loss: 0.0240\n",
      "Epoch: 668 | Train loss: 0.03702 | Test loss: 0.48223 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0061\n",
      "Batch 10, Loss: 0.0155\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0169\n",
      "Batch 40, Loss: 0.0039\n",
      "Batch 50, Loss: 0.0053\n",
      "Batch 60, Loss: 0.0032\n",
      "Batch 70, Loss: 0.0044\n",
      "Batch 80, Loss: 0.0067\n",
      "Batch 90, Loss: 0.0896\n",
      "Epoch: 669 | Train loss: 0.02825 | Test loss: 0.05070 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0111\n",
      "Batch 10, Loss: 0.1768\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0016\n",
      "Batch 40, Loss: 0.0084\n",
      "Batch 50, Loss: 0.2081\n",
      "Batch 60, Loss: 0.1814\n",
      "Batch 70, Loss: 0.2279\n",
      "Batch 80, Loss: 0.0031\n",
      "Batch 90, Loss: 0.0196\n",
      "Epoch: 670 | Train loss: 0.03964 | Test loss: 0.05433 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0047\n",
      "Batch 10, Loss: 0.0302\n",
      "Batch 20, Loss: 0.0205\n",
      "Batch 30, Loss: 0.0038\n",
      "Batch 40, Loss: 0.0038\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0541\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0406\n",
      "Epoch: 671 | Train loss: 0.03042 | Test loss: 0.21551 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1094\n",
      "Batch 10, Loss: 0.0070\n",
      "Batch 20, Loss: 0.0020\n",
      "Batch 30, Loss: 0.0091\n",
      "Batch 40, Loss: 0.0099\n",
      "Batch 50, Loss: 0.0056\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.0284\n",
      "Batch 90, Loss: 0.0058\n",
      "Epoch: 672 | Train loss: 0.01970 | Test loss: 0.04802 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0107\n",
      "Batch 10, Loss: 0.0374\n",
      "Batch 20, Loss: 0.0047\n",
      "Batch 30, Loss: 0.0021\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.0063\n",
      "Batch 60, Loss: 0.0834\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0207\n",
      "Batch 90, Loss: 0.0444\n",
      "Epoch: 673 | Train loss: 0.03776 | Test loss: 0.04759 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0041\n",
      "Batch 10, Loss: 0.1001\n",
      "Batch 20, Loss: 0.1539\n",
      "Batch 30, Loss: 0.0858\n",
      "Batch 40, Loss: 0.1202\n",
      "Batch 50, Loss: 0.3133\n",
      "Batch 60, Loss: 0.1119\n",
      "Batch 70, Loss: 0.5293\n",
      "Batch 80, Loss: 0.0042\n",
      "Batch 90, Loss: 0.0078\n",
      "Epoch: 674 | Train loss: 0.08085 | Test loss: 0.06900 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0374\n",
      "Batch 10, Loss: 0.0080\n",
      "Batch 20, Loss: 0.2660\n",
      "Batch 30, Loss: 0.0138\n",
      "Batch 40, Loss: 0.0068\n",
      "Batch 50, Loss: 0.0625\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.0044\n",
      "Batch 80, Loss: 0.0587\n",
      "Batch 90, Loss: 0.2229\n",
      "Epoch: 675 | Train loss: 0.02934 | Test loss: 0.55574 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0087\n",
      "Batch 10, Loss: 0.0018\n",
      "Batch 20, Loss: 0.0272\n",
      "Batch 30, Loss: 0.2214\n",
      "Batch 40, Loss: 0.0151\n",
      "Batch 50, Loss: 0.0372\n",
      "Batch 60, Loss: 0.0108\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0110\n",
      "Batch 90, Loss: 0.0806\n",
      "Epoch: 676 | Train loss: 0.06199 | Test loss: 0.63019 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0144\n",
      "Batch 10, Loss: 0.0103\n",
      "Batch 20, Loss: 0.0059\n",
      "Batch 30, Loss: 0.0450\n",
      "Batch 40, Loss: 0.0209\n",
      "Batch 50, Loss: 0.0047\n",
      "Batch 60, Loss: 0.0427\n",
      "Batch 70, Loss: 0.0079\n",
      "Batch 80, Loss: 0.0665\n",
      "Batch 90, Loss: 0.7395\n",
      "Epoch: 677 | Train loss: 0.05910 | Test loss: 0.17011 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0228\n",
      "Batch 10, Loss: 0.0145\n",
      "Batch 20, Loss: 0.0062\n",
      "Batch 30, Loss: 0.2450\n",
      "Batch 40, Loss: 0.0454\n",
      "Batch 50, Loss: 0.0036\n",
      "Batch 60, Loss: 0.0620\n",
      "Batch 70, Loss: 0.0171\n",
      "Batch 80, Loss: 0.0105\n",
      "Batch 90, Loss: 0.0840\n",
      "Epoch: 678 | Train loss: 0.04436 | Test loss: 0.16702 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0335\n",
      "Batch 10, Loss: 0.0020\n",
      "Batch 20, Loss: 0.0055\n",
      "Batch 30, Loss: 0.0813\n",
      "Batch 40, Loss: 0.0595\n",
      "Batch 50, Loss: 0.0039\n",
      "Batch 60, Loss: 0.0190\n",
      "Batch 70, Loss: 0.0295\n",
      "Batch 80, Loss: 0.0096\n",
      "Batch 90, Loss: 0.0359\n",
      "Epoch: 679 | Train loss: 0.07894 | Test loss: 0.08431 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0882\n",
      "Batch 10, Loss: 0.0356\n",
      "Batch 20, Loss: 0.0234\n",
      "Batch 30, Loss: 0.0157\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0537\n",
      "Batch 60, Loss: 0.0135\n",
      "Batch 70, Loss: 0.2992\n",
      "Batch 80, Loss: 0.0462\n",
      "Batch 90, Loss: 0.1249\n",
      "Epoch: 680 | Train loss: 0.09135 | Test loss: 0.08182 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.2011\n",
      "Batch 10, Loss: 0.0215\n",
      "Batch 20, Loss: 0.0328\n",
      "Batch 30, Loss: 0.0151\n",
      "Batch 40, Loss: 0.2859\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0004\n",
      "Batch 70, Loss: 0.1662\n",
      "Batch 80, Loss: 0.4216\n",
      "Batch 90, Loss: 0.0905\n",
      "Epoch: 681 | Train loss: 0.04165 | Test loss: 0.59939 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0017\n",
      "Batch 10, Loss: 0.0326\n",
      "Batch 20, Loss: 0.0077\n",
      "Batch 30, Loss: 0.1346\n",
      "Batch 40, Loss: 0.0123\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.1004\n",
      "Batch 70, Loss: 0.0452\n",
      "Batch 80, Loss: 0.3118\n",
      "Batch 90, Loss: 0.4963\n",
      "Epoch: 682 | Train loss: 0.05678 | Test loss: 0.05091 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2009\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.1003\n",
      "Batch 30, Loss: 0.0344\n",
      "Batch 40, Loss: 0.0092\n",
      "Batch 50, Loss: 0.0110\n",
      "Batch 60, Loss: 0.0208\n",
      "Batch 70, Loss: 0.3820\n",
      "Batch 80, Loss: 0.0336\n",
      "Batch 90, Loss: 0.0989\n",
      "Epoch: 683 | Train loss: 0.06944 | Test loss: 1.40169 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1076\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0121\n",
      "Batch 40, Loss: 0.0105\n",
      "Batch 50, Loss: 0.0033\n",
      "Batch 60, Loss: 0.0117\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0123\n",
      "Batch 90, Loss: 0.0048\n",
      "Epoch: 684 | Train loss: 0.04621 | Test loss: 0.25682 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0620\n",
      "Batch 10, Loss: 0.0221\n",
      "Batch 20, Loss: 0.0067\n",
      "Batch 30, Loss: 0.0471\n",
      "Batch 40, Loss: 0.0097\n",
      "Batch 50, Loss: 0.0142\n",
      "Batch 60, Loss: 0.0030\n",
      "Batch 70, Loss: 0.0170\n",
      "Batch 80, Loss: 0.0709\n",
      "Batch 90, Loss: 0.0076\n",
      "Epoch: 685 | Train loss: 0.03735 | Test loss: 0.15263 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0506\n",
      "Batch 10, Loss: 0.0210\n",
      "Batch 20, Loss: 0.0094\n",
      "Batch 30, Loss: 0.0756\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0029\n",
      "Batch 60, Loss: 0.0293\n",
      "Batch 70, Loss: 0.0105\n",
      "Batch 80, Loss: 0.0030\n",
      "Batch 90, Loss: 0.0022\n",
      "Epoch: 686 | Train loss: 0.03399 | Test loss: 0.04748 | Test accuracy: 0.51546\n",
      "Batch 0, Loss: 0.0259\n",
      "Batch 10, Loss: 0.1042\n",
      "Batch 20, Loss: 0.0106\n",
      "Batch 30, Loss: 0.0103\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.1591\n",
      "Batch 70, Loss: 0.0660\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0011\n",
      "Epoch: 687 | Train loss: 0.02884 | Test loss: 0.06099 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0157\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.1247\n",
      "Batch 30, Loss: 0.0198\n",
      "Batch 40, Loss: 0.0183\n",
      "Batch 50, Loss: 0.0040\n",
      "Batch 60, Loss: 0.0061\n",
      "Batch 70, Loss: 0.0139\n",
      "Batch 80, Loss: 0.0593\n",
      "Batch 90, Loss: 0.0077\n",
      "Epoch: 688 | Train loss: 0.02555 | Test loss: 0.25569 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0261\n",
      "Batch 10, Loss: 0.0091\n",
      "Batch 20, Loss: 0.1256\n",
      "Batch 30, Loss: 0.0110\n",
      "Batch 40, Loss: 0.0030\n",
      "Batch 50, Loss: 0.0108\n",
      "Batch 60, Loss: 0.0088\n",
      "Batch 70, Loss: 0.0086\n",
      "Batch 80, Loss: 0.0904\n",
      "Batch 90, Loss: 0.0018\n",
      "Epoch: 689 | Train loss: 0.02643 | Test loss: 0.06386 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0058\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.1664\n",
      "Batch 40, Loss: 0.0055\n",
      "Batch 50, Loss: 0.0034\n",
      "Batch 60, Loss: 0.0240\n",
      "Batch 70, Loss: 0.0238\n",
      "Batch 80, Loss: 0.1451\n",
      "Batch 90, Loss: 0.0122\n",
      "Epoch: 690 | Train loss: 0.04146 | Test loss: 0.26976 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0203\n",
      "Batch 10, Loss: 0.0957\n",
      "Batch 20, Loss: 0.0058\n",
      "Batch 30, Loss: 0.0036\n",
      "Batch 40, Loss: 0.0050\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0455\n",
      "Batch 70, Loss: 0.0560\n",
      "Batch 80, Loss: 0.0349\n",
      "Batch 90, Loss: 0.0047\n",
      "Epoch: 691 | Train loss: 0.03653 | Test loss: 0.14214 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0019\n",
      "Batch 10, Loss: 0.0299\n",
      "Batch 20, Loss: 0.0037\n",
      "Batch 30, Loss: 0.0010\n",
      "Batch 40, Loss: 0.0148\n",
      "Batch 50, Loss: 0.0023\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0015\n",
      "Batch 80, Loss: 0.0282\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 692 | Train loss: 0.03751 | Test loss: 0.13581 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.3449\n",
      "Batch 20, Loss: 0.2863\n",
      "Batch 30, Loss: 0.0071\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0256\n",
      "Batch 60, Loss: 0.0065\n",
      "Batch 70, Loss: 0.1139\n",
      "Batch 80, Loss: 0.0105\n",
      "Batch 90, Loss: 0.0098\n",
      "Epoch: 693 | Train loss: 0.05937 | Test loss: 0.12241 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0032\n",
      "Batch 10, Loss: 0.0134\n",
      "Batch 20, Loss: 0.0014\n",
      "Batch 30, Loss: 0.0029\n",
      "Batch 40, Loss: 0.0224\n",
      "Batch 50, Loss: 0.0069\n",
      "Batch 60, Loss: 0.0183\n",
      "Batch 70, Loss: 0.0004\n",
      "Batch 80, Loss: 0.0042\n",
      "Batch 90, Loss: 0.0407\n",
      "Epoch: 694 | Train loss: 0.05008 | Test loss: 0.07838 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0216\n",
      "Batch 10, Loss: 0.0277\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.0419\n",
      "Batch 40, Loss: 0.0189\n",
      "Batch 50, Loss: 0.0707\n",
      "Batch 60, Loss: 0.1107\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0110\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 695 | Train loss: 0.04939 | Test loss: 0.06529 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0246\n",
      "Batch 10, Loss: 0.9009\n",
      "Batch 20, Loss: 0.0904\n",
      "Batch 30, Loss: 0.0068\n",
      "Batch 40, Loss: 0.0088\n",
      "Batch 50, Loss: 0.0619\n",
      "Batch 60, Loss: 0.0170\n",
      "Batch 70, Loss: 0.0395\n",
      "Batch 80, Loss: 0.0040\n",
      "Batch 90, Loss: 0.0072\n",
      "Epoch: 696 | Train loss: 0.05053 | Test loss: 0.05988 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.0132\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0508\n",
      "Batch 30, Loss: 0.0059\n",
      "Batch 40, Loss: 0.0356\n",
      "Batch 50, Loss: 0.0705\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0028\n",
      "Batch 90, Loss: 0.0137\n",
      "Epoch: 697 | Train loss: 0.03415 | Test loss: 0.07435 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0005\n",
      "Batch 10, Loss: 0.1292\n",
      "Batch 20, Loss: 0.0046\n",
      "Batch 30, Loss: 0.0260\n",
      "Batch 40, Loss: 0.0045\n",
      "Batch 50, Loss: 0.0102\n",
      "Batch 60, Loss: 0.0732\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0020\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 698 | Train loss: 0.03086 | Test loss: 0.19433 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1538\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.0309\n",
      "Batch 30, Loss: 0.0169\n",
      "Batch 40, Loss: 0.0415\n",
      "Batch 50, Loss: 0.2483\n",
      "Batch 60, Loss: 0.1155\n",
      "Batch 70, Loss: 0.0953\n",
      "Batch 80, Loss: 0.0250\n",
      "Batch 90, Loss: 0.2068\n",
      "Epoch: 699 | Train loss: 0.06345 | Test loss: 0.09366 | Test accuracy: 0.38918\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0185\n",
      "Batch 20, Loss: 0.0089\n",
      "Batch 30, Loss: 0.2166\n",
      "Batch 40, Loss: 0.0227\n",
      "Batch 50, Loss: 0.0039\n",
      "Batch 60, Loss: 0.0744\n",
      "Batch 70, Loss: 0.0147\n",
      "Batch 80, Loss: 0.0164\n",
      "Batch 90, Loss: 0.0042\n",
      "Epoch: 700 | Train loss: 0.05278 | Test loss: 0.10905 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0147\n",
      "Batch 10, Loss: 0.0130\n",
      "Batch 20, Loss: 0.0088\n",
      "Batch 30, Loss: 0.0235\n",
      "Batch 40, Loss: 0.0193\n",
      "Batch 50, Loss: 0.0250\n",
      "Batch 60, Loss: 0.0509\n",
      "Batch 70, Loss: 0.0184\n",
      "Batch 80, Loss: 0.0741\n",
      "Batch 90, Loss: 0.0072\n",
      "Epoch: 701 | Train loss: 0.08066 | Test loss: 0.04941 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0925\n",
      "Batch 10, Loss: 0.0478\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.1164\n",
      "Batch 40, Loss: 0.0035\n",
      "Batch 50, Loss: 0.0054\n",
      "Batch 60, Loss: 0.0443\n",
      "Batch 70, Loss: 0.0536\n",
      "Batch 80, Loss: 0.0413\n",
      "Batch 90, Loss: 0.0271\n",
      "Epoch: 702 | Train loss: 0.04369 | Test loss: 16.11559 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0485\n",
      "Batch 30, Loss: 0.0020\n",
      "Batch 40, Loss: 0.1225\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.2961\n",
      "Batch 70, Loss: 0.1278\n",
      "Batch 80, Loss: 0.0013\n",
      "Batch 90, Loss: 0.0515\n",
      "Epoch: 703 | Train loss: 0.04459 | Test loss: 0.44597 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.2063\n",
      "Batch 10, Loss: 0.0165\n",
      "Batch 20, Loss: 0.1691\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0070\n",
      "Batch 60, Loss: 0.0033\n",
      "Batch 70, Loss: 0.0411\n",
      "Batch 80, Loss: 0.0144\n",
      "Batch 90, Loss: 0.0074\n",
      "Epoch: 704 | Train loss: 0.05525 | Test loss: 0.09851 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0404\n",
      "Batch 10, Loss: 0.0090\n",
      "Batch 20, Loss: 0.0172\n",
      "Batch 30, Loss: 0.0436\n",
      "Batch 40, Loss: 0.0213\n",
      "Batch 50, Loss: 0.0500\n",
      "Batch 60, Loss: 0.0016\n",
      "Batch 70, Loss: 0.0078\n",
      "Batch 80, Loss: 0.1743\n",
      "Batch 90, Loss: 0.2772\n",
      "Epoch: 705 | Train loss: 0.03852 | Test loss: 0.37766 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0257\n",
      "Batch 10, Loss: 0.1081\n",
      "Batch 20, Loss: 0.0002\n",
      "Batch 30, Loss: 0.0710\n",
      "Batch 40, Loss: 0.0617\n",
      "Batch 50, Loss: 0.0050\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0175\n",
      "Batch 80, Loss: 0.0199\n",
      "Batch 90, Loss: 0.0093\n",
      "Epoch: 706 | Train loss: 0.05936 | Test loss: 0.18079 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0097\n",
      "Batch 10, Loss: 0.0091\n",
      "Batch 20, Loss: 0.1070\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0039\n",
      "Batch 50, Loss: 0.0213\n",
      "Batch 60, Loss: 0.0514\n",
      "Batch 70, Loss: 0.0182\n",
      "Batch 80, Loss: 0.0185\n",
      "Batch 90, Loss: 0.1493\n",
      "Epoch: 707 | Train loss: 0.04740 | Test loss: 0.04420 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.1130\n",
      "Batch 10, Loss: 0.0082\n",
      "Batch 20, Loss: 0.0064\n",
      "Batch 30, Loss: 0.2966\n",
      "Batch 40, Loss: 0.0034\n",
      "Batch 50, Loss: 0.0018\n",
      "Batch 60, Loss: 0.0018\n",
      "Batch 70, Loss: 0.1058\n",
      "Batch 80, Loss: 0.0227\n",
      "Batch 90, Loss: 0.0431\n",
      "Epoch: 708 | Train loss: 0.03815 | Test loss: 0.12612 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0081\n",
      "Batch 20, Loss: 0.0574\n",
      "Batch 30, Loss: 0.0195\n",
      "Batch 40, Loss: 0.0451\n",
      "Batch 50, Loss: 0.0018\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.1560\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0105\n",
      "Epoch: 709 | Train loss: 0.03452 | Test loss: 0.75756 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.1300\n",
      "Batch 20, Loss: 0.0117\n",
      "Batch 30, Loss: 0.0709\n",
      "Batch 40, Loss: 0.1030\n",
      "Batch 50, Loss: 0.0406\n",
      "Batch 60, Loss: 0.0125\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0176\n",
      "Batch 90, Loss: 0.0008\n",
      "Epoch: 710 | Train loss: 0.05135 | Test loss: 0.10115 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.2090\n",
      "Batch 10, Loss: 0.0173\n",
      "Batch 20, Loss: 0.0123\n",
      "Batch 30, Loss: 0.0461\n",
      "Batch 40, Loss: 0.0029\n",
      "Batch 50, Loss: 0.0869\n",
      "Batch 60, Loss: 0.0254\n",
      "Batch 70, Loss: 0.0026\n",
      "Batch 80, Loss: 0.0563\n",
      "Batch 90, Loss: 0.3474\n",
      "Epoch: 711 | Train loss: 0.03661 | Test loss: 0.05501 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0019\n",
      "Batch 10, Loss: 0.0517\n",
      "Batch 20, Loss: 0.0031\n",
      "Batch 30, Loss: 0.0044\n",
      "Batch 40, Loss: 0.0015\n",
      "Batch 50, Loss: 0.1829\n",
      "Batch 60, Loss: 0.0066\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0674\n",
      "Batch 90, Loss: 0.2050\n",
      "Epoch: 712 | Train loss: 0.03125 | Test loss: 0.04460 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0057\n",
      "Batch 10, Loss: 0.0058\n",
      "Batch 20, Loss: 0.0023\n",
      "Batch 30, Loss: 0.0417\n",
      "Batch 40, Loss: 0.0145\n",
      "Batch 50, Loss: 0.0117\n",
      "Batch 60, Loss: 0.0025\n",
      "Batch 70, Loss: 0.0092\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 713 | Train loss: 0.03546 | Test loss: 0.05031 | Test accuracy: 0.55412\n",
      "Batch 0, Loss: 0.0007\n",
      "Batch 10, Loss: 0.0113\n",
      "Batch 20, Loss: 0.0011\n",
      "Batch 30, Loss: 0.2021\n",
      "Batch 40, Loss: 0.0049\n",
      "Batch 50, Loss: 0.0018\n",
      "Batch 60, Loss: 0.0405\n",
      "Batch 70, Loss: 0.0049\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0059\n",
      "Epoch: 714 | Train loss: 0.04046 | Test loss: 0.13228 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.0050\n",
      "Batch 20, Loss: 0.0052\n",
      "Batch 30, Loss: 0.0405\n",
      "Batch 40, Loss: 0.0008\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0131\n",
      "Batch 70, Loss: 0.0120\n",
      "Batch 80, Loss: 0.0021\n",
      "Batch 90, Loss: 0.0131\n",
      "Epoch: 715 | Train loss: 0.05653 | Test loss: 0.06203 | Test accuracy: 0.57732\n",
      "Batch 0, Loss: 0.0089\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0023\n",
      "Batch 30, Loss: 0.0101\n",
      "Batch 40, Loss: 0.0117\n",
      "Batch 50, Loss: 0.0711\n",
      "Batch 60, Loss: 0.2496\n",
      "Batch 70, Loss: 0.0191\n",
      "Batch 80, Loss: 0.0028\n",
      "Batch 90, Loss: 0.0298\n",
      "Epoch: 716 | Train loss: 0.05611 | Test loss: 0.07165 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0054\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0201\n",
      "Batch 70, Loss: 0.0048\n",
      "Batch 80, Loss: 0.1757\n",
      "Batch 90, Loss: 0.0244\n",
      "Epoch: 717 | Train loss: 0.03599 | Test loss: 0.30445 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0504\n",
      "Batch 10, Loss: 0.0745\n",
      "Batch 20, Loss: 0.0014\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0313\n",
      "Batch 50, Loss: 0.0214\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0340\n",
      "Batch 80, Loss: 0.1213\n",
      "Batch 90, Loss: 0.0457\n",
      "Epoch: 718 | Train loss: 0.05802 | Test loss: 0.04945 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.4024\n",
      "Batch 10, Loss: 0.0511\n",
      "Batch 20, Loss: 0.3010\n",
      "Batch 30, Loss: 0.0259\n",
      "Batch 40, Loss: 0.0104\n",
      "Batch 50, Loss: 0.0038\n",
      "Batch 60, Loss: 0.0183\n",
      "Batch 70, Loss: 0.0196\n",
      "Batch 80, Loss: 0.0047\n",
      "Batch 90, Loss: 0.0107\n",
      "Epoch: 719 | Train loss: 0.08294 | Test loss: 0.21949 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0112\n",
      "Batch 10, Loss: 0.0201\n",
      "Batch 20, Loss: 0.0496\n",
      "Batch 30, Loss: 0.0100\n",
      "Batch 40, Loss: 0.0011\n",
      "Batch 50, Loss: 0.0018\n",
      "Batch 60, Loss: 0.0928\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0280\n",
      "Batch 90, Loss: 0.0238\n",
      "Epoch: 720 | Train loss: 0.03487 | Test loss: 0.05584 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0242\n",
      "Batch 10, Loss: 0.0108\n",
      "Batch 20, Loss: 0.0008\n",
      "Batch 30, Loss: 0.0266\n",
      "Batch 40, Loss: 0.0007\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 60, Loss: 0.0012\n",
      "Batch 70, Loss: 0.0481\n",
      "Batch 80, Loss: 0.0022\n",
      "Batch 90, Loss: 0.0137\n",
      "Epoch: 721 | Train loss: 0.02461 | Test loss: 0.05167 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.1722\n",
      "Batch 10, Loss: 0.0029\n",
      "Batch 20, Loss: 0.1845\n",
      "Batch 30, Loss: 0.0429\n",
      "Batch 40, Loss: 0.0141\n",
      "Batch 50, Loss: 0.0399\n",
      "Batch 60, Loss: 0.0131\n",
      "Batch 70, Loss: 0.0044\n",
      "Batch 80, Loss: 0.0182\n",
      "Batch 90, Loss: 0.0016\n",
      "Epoch: 722 | Train loss: 0.05733 | Test loss: 0.04352 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0021\n",
      "Batch 10, Loss: 0.0017\n",
      "Batch 20, Loss: 0.0551\n",
      "Batch 30, Loss: 0.0375\n",
      "Batch 40, Loss: 0.0187\n",
      "Batch 50, Loss: 0.0680\n",
      "Batch 60, Loss: 0.0089\n",
      "Batch 70, Loss: 0.1173\n",
      "Batch 80, Loss: 0.1205\n",
      "Batch 90, Loss: 0.0413\n",
      "Epoch: 723 | Train loss: 0.05717 | Test loss: 0.04039 | Test accuracy: 0.50000\n",
      "Batch 0, Loss: 0.1164\n",
      "Batch 10, Loss: 0.0057\n",
      "Batch 20, Loss: 0.0507\n",
      "Batch 30, Loss: 0.0342\n",
      "Batch 40, Loss: 0.0273\n",
      "Batch 50, Loss: 0.0263\n",
      "Batch 60, Loss: 0.0054\n",
      "Batch 70, Loss: 0.0096\n",
      "Batch 80, Loss: 0.0113\n",
      "Batch 90, Loss: 0.0933\n",
      "Epoch: 724 | Train loss: 0.06305 | Test loss: 0.13856 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0104\n",
      "Batch 20, Loss: 0.0003\n",
      "Batch 30, Loss: 0.0003\n",
      "Batch 40, Loss: 0.0369\n",
      "Batch 50, Loss: 0.0013\n",
      "Batch 60, Loss: 0.0432\n",
      "Batch 70, Loss: 0.4253\n",
      "Batch 80, Loss: 0.0002\n",
      "Batch 90, Loss: 0.0103\n",
      "Epoch: 725 | Train loss: 0.04501 | Test loss: 0.12709 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0100\n",
      "Batch 10, Loss: 0.1868\n",
      "Batch 20, Loss: 0.0090\n",
      "Batch 30, Loss: 0.0313\n",
      "Batch 40, Loss: 0.0713\n",
      "Batch 50, Loss: 0.0360\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0155\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 726 | Train loss: 0.02868 | Test loss: 0.04573 | Test accuracy: 0.49227\n",
      "Batch 0, Loss: 0.0006\n",
      "Batch 10, Loss: 0.0169\n",
      "Batch 20, Loss: 0.0245\n",
      "Batch 30, Loss: 0.0001\n",
      "Batch 40, Loss: 0.1204\n",
      "Batch 50, Loss: 0.0295\n",
      "Batch 60, Loss: 0.0015\n",
      "Batch 70, Loss: 0.0048\n",
      "Batch 80, Loss: 0.0070\n",
      "Batch 90, Loss: 0.0560\n",
      "Epoch: 727 | Train loss: 0.03012 | Test loss: 1.05039 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0065\n",
      "Batch 10, Loss: 0.1252\n",
      "Batch 20, Loss: 0.0088\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0281\n",
      "Batch 60, Loss: 0.0165\n",
      "Batch 70, Loss: 0.0096\n",
      "Batch 80, Loss: 0.1931\n",
      "Batch 90, Loss: 0.0254\n",
      "Epoch: 728 | Train loss: 0.04667 | Test loss: 0.06540 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0767\n",
      "Batch 10, Loss: 0.0024\n",
      "Batch 20, Loss: 0.0131\n",
      "Batch 30, Loss: 0.0456\n",
      "Batch 40, Loss: 0.0039\n",
      "Batch 50, Loss: 0.0122\n",
      "Batch 60, Loss: 0.0032\n",
      "Batch 70, Loss: 0.0069\n",
      "Batch 80, Loss: 0.0188\n",
      "Batch 90, Loss: 0.0525\n",
      "Epoch: 729 | Train loss: 0.04648 | Test loss: 0.04966 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0090\n",
      "Batch 20, Loss: 0.2942\n",
      "Batch 30, Loss: 0.0145\n",
      "Batch 40, Loss: 0.0115\n",
      "Batch 50, Loss: 0.1568\n",
      "Batch 60, Loss: 0.0081\n",
      "Batch 70, Loss: 0.0218\n",
      "Batch 80, Loss: 0.0103\n",
      "Batch 90, Loss: 0.0241\n",
      "Epoch: 730 | Train loss: 0.06012 | Test loss: 0.04949 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0507\n",
      "Batch 10, Loss: 0.0033\n",
      "Batch 20, Loss: 0.0921\n",
      "Batch 30, Loss: 0.0729\n",
      "Batch 40, Loss: 0.0257\n",
      "Batch 50, Loss: 0.1550\n",
      "Batch 60, Loss: 0.0113\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0012\n",
      "Epoch: 731 | Train loss: 0.07076 | Test loss: 0.21142 | Test accuracy: 0.51546\n",
      "Batch 0, Loss: 0.0724\n",
      "Batch 10, Loss: 0.0032\n",
      "Batch 20, Loss: 0.0047\n",
      "Batch 30, Loss: 0.0495\n",
      "Batch 40, Loss: 0.0172\n",
      "Batch 50, Loss: 0.0106\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0019\n",
      "Batch 80, Loss: 0.0065\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 732 | Train loss: 0.04826 | Test loss: 1.72125 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0422\n",
      "Batch 10, Loss: 0.0347\n",
      "Batch 20, Loss: 0.0303\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0379\n",
      "Batch 50, Loss: 0.3920\n",
      "Batch 60, Loss: 0.0798\n",
      "Batch 70, Loss: 0.1427\n",
      "Batch 80, Loss: 0.0114\n",
      "Batch 90, Loss: 0.0118\n",
      "Epoch: 733 | Train loss: 0.05257 | Test loss: 0.11162 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0203\n",
      "Batch 10, Loss: 0.0754\n",
      "Batch 20, Loss: 0.0419\n",
      "Batch 30, Loss: 0.0205\n",
      "Batch 40, Loss: 0.0939\n",
      "Batch 50, Loss: 0.0023\n",
      "Batch 60, Loss: 0.0011\n",
      "Batch 70, Loss: 0.1370\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0972\n",
      "Epoch: 734 | Train loss: 0.04554 | Test loss: 0.08872 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0698\n",
      "Batch 10, Loss: 0.1102\n",
      "Batch 20, Loss: 0.2676\n",
      "Batch 30, Loss: 0.0006\n",
      "Batch 40, Loss: 0.0130\n",
      "Batch 50, Loss: 0.0460\n",
      "Batch 60, Loss: 0.0418\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0013\n",
      "Epoch: 735 | Train loss: 0.03257 | Test loss: 0.62250 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0017\n",
      "Batch 10, Loss: 0.0060\n",
      "Batch 20, Loss: 0.0356\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0014\n",
      "Batch 50, Loss: 0.0522\n",
      "Batch 60, Loss: 0.0082\n",
      "Batch 70, Loss: 0.0087\n",
      "Batch 80, Loss: 0.0321\n",
      "Batch 90, Loss: 0.0492\n",
      "Epoch: 736 | Train loss: 0.03110 | Test loss: 0.21527 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0139\n",
      "Batch 10, Loss: 0.0319\n",
      "Batch 20, Loss: 0.0107\n",
      "Batch 30, Loss: 0.0374\n",
      "Batch 40, Loss: 0.0125\n",
      "Batch 50, Loss: 0.0065\n",
      "Batch 60, Loss: 0.0237\n",
      "Batch 70, Loss: 0.0412\n",
      "Batch 80, Loss: 0.0197\n",
      "Batch 90, Loss: 0.0079\n",
      "Epoch: 737 | Train loss: 0.03969 | Test loss: 0.04550 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0133\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.0616\n",
      "Batch 30, Loss: 0.0002\n",
      "Batch 40, Loss: 0.0164\n",
      "Batch 50, Loss: 0.0088\n",
      "Batch 60, Loss: 0.0419\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0111\n",
      "Epoch: 738 | Train loss: 0.02748 | Test loss: 0.20282 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.1195\n",
      "Batch 20, Loss: 0.0035\n",
      "Batch 30, Loss: 0.0020\n",
      "Batch 40, Loss: 0.0741\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.1487\n",
      "Batch 80, Loss: 0.0050\n",
      "Batch 90, Loss: 0.0103\n",
      "Epoch: 739 | Train loss: 0.02414 | Test loss: 0.08514 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0031\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.0047\n",
      "Batch 40, Loss: 0.0007\n",
      "Batch 50, Loss: 0.0025\n",
      "Batch 60, Loss: 0.0073\n",
      "Batch 70, Loss: 0.0322\n",
      "Batch 80, Loss: 0.0006\n",
      "Batch 90, Loss: 0.0009\n",
      "Epoch: 740 | Train loss: 0.02559 | Test loss: 0.08375 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0086\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0359\n",
      "Batch 50, Loss: 0.0053\n",
      "Batch 60, Loss: 0.0046\n",
      "Batch 70, Loss: 0.0207\n",
      "Batch 80, Loss: 0.0755\n",
      "Batch 90, Loss: 0.0103\n",
      "Epoch: 741 | Train loss: 0.03170 | Test loss: 0.06946 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0328\n",
      "Batch 10, Loss: 0.0007\n",
      "Batch 20, Loss: 0.0205\n",
      "Batch 30, Loss: 0.0816\n",
      "Batch 40, Loss: 0.0052\n",
      "Batch 50, Loss: 0.0084\n",
      "Batch 60, Loss: 0.3703\n",
      "Batch 70, Loss: 0.0154\n",
      "Batch 80, Loss: 0.0441\n",
      "Batch 90, Loss: 0.0007\n",
      "Epoch: 742 | Train loss: 0.04130 | Test loss: 0.04197 | Test accuracy: 0.50000\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.0329\n",
      "Batch 20, Loss: 0.0189\n",
      "Batch 30, Loss: 0.0173\n",
      "Batch 40, Loss: 0.0047\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.0152\n",
      "Batch 70, Loss: 0.0758\n",
      "Batch 80, Loss: 0.0372\n",
      "Batch 90, Loss: 0.0213\n",
      "Epoch: 743 | Train loss: 0.03017 | Test loss: 1.65563 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0096\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0002\n",
      "Batch 30, Loss: 0.1457\n",
      "Batch 40, Loss: 0.0126\n",
      "Batch 50, Loss: 0.0060\n",
      "Batch 60, Loss: 0.0213\n",
      "Batch 70, Loss: 0.0155\n",
      "Batch 80, Loss: 0.0144\n",
      "Batch 90, Loss: 0.0175\n",
      "Epoch: 744 | Train loss: 0.03044 | Test loss: 0.18669 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0024\n",
      "Batch 10, Loss: 0.0638\n",
      "Batch 20, Loss: 0.0010\n",
      "Batch 30, Loss: 0.0176\n",
      "Batch 40, Loss: 0.0005\n",
      "Batch 50, Loss: 0.0061\n",
      "Batch 60, Loss: 0.0007\n",
      "Batch 70, Loss: 0.0279\n",
      "Batch 80, Loss: 0.3492\n",
      "Batch 90, Loss: 0.1187\n",
      "Epoch: 745 | Train loss: 0.03937 | Test loss: 0.13150 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0405\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0077\n",
      "Batch 30, Loss: 0.0449\n",
      "Batch 40, Loss: 0.0071\n",
      "Batch 50, Loss: 0.0027\n",
      "Batch 60, Loss: 0.0607\n",
      "Batch 70, Loss: 0.0039\n",
      "Batch 80, Loss: 0.0002\n",
      "Batch 90, Loss: 0.0109\n",
      "Epoch: 746 | Train loss: 0.03187 | Test loss: 0.92132 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0069\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.0004\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0300\n",
      "Batch 60, Loss: 0.2582\n",
      "Batch 70, Loss: 0.0810\n",
      "Batch 80, Loss: 0.0582\n",
      "Batch 90, Loss: 0.0412\n",
      "Epoch: 747 | Train loss: 0.10120 | Test loss: 0.14843 | Test accuracy: 0.43814\n",
      "Batch 0, Loss: 0.0054\n",
      "Batch 10, Loss: 0.0081\n",
      "Batch 20, Loss: 0.0016\n",
      "Batch 30, Loss: 0.0098\n",
      "Batch 40, Loss: 0.0287\n",
      "Batch 50, Loss: 0.0303\n",
      "Batch 60, Loss: 0.0041\n",
      "Batch 70, Loss: 0.0083\n",
      "Batch 80, Loss: 0.0327\n",
      "Batch 90, Loss: 0.0207\n",
      "Epoch: 748 | Train loss: 0.06799 | Test loss: 0.04838 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.1725\n",
      "Batch 10, Loss: 0.1194\n",
      "Batch 20, Loss: 0.2443\n",
      "Batch 30, Loss: 0.0065\n",
      "Batch 40, Loss: 0.0417\n",
      "Batch 50, Loss: 0.0565\n",
      "Batch 60, Loss: 0.2162\n",
      "Batch 70, Loss: 0.0113\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0051\n",
      "Epoch: 749 | Train loss: 0.09461 | Test loss: 1.17903 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1802\n",
      "Batch 10, Loss: 0.0015\n",
      "Batch 20, Loss: 0.1775\n",
      "Batch 30, Loss: 0.0022\n",
      "Batch 40, Loss: 0.1623\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.0118\n",
      "Batch 70, Loss: 0.0140\n",
      "Batch 80, Loss: 0.0073\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 750 | Train loss: 0.04719 | Test loss: 1.60743 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0021\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.1141\n",
      "Batch 30, Loss: 0.1151\n",
      "Batch 40, Loss: 0.1109\n",
      "Batch 50, Loss: 0.3641\n",
      "Batch 60, Loss: 0.0739\n",
      "Batch 70, Loss: 0.0134\n",
      "Batch 80, Loss: 0.0118\n",
      "Batch 90, Loss: 0.0237\n",
      "Epoch: 751 | Train loss: 0.09563 | Test loss: 0.05131 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0814\n",
      "Batch 10, Loss: 0.0044\n",
      "Batch 20, Loss: 0.0167\n",
      "Batch 30, Loss: 0.0339\n",
      "Batch 40, Loss: 0.6921\n",
      "Batch 50, Loss: 0.0121\n",
      "Batch 60, Loss: 0.0161\n",
      "Batch 70, Loss: 0.0107\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.0305\n",
      "Epoch: 752 | Train loss: 0.05872 | Test loss: 0.17624 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0486\n",
      "Batch 20, Loss: 0.0041\n",
      "Batch 30, Loss: 0.0236\n",
      "Batch 40, Loss: 0.0088\n",
      "Batch 50, Loss: 0.0341\n",
      "Batch 60, Loss: 0.0722\n",
      "Batch 70, Loss: 0.0076\n",
      "Batch 80, Loss: 0.0020\n",
      "Batch 90, Loss: 0.0228\n",
      "Epoch: 753 | Train loss: 0.03144 | Test loss: 0.10940 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0169\n",
      "Batch 10, Loss: 0.1169\n",
      "Batch 20, Loss: 0.2145\n",
      "Batch 30, Loss: 0.0076\n",
      "Batch 40, Loss: 0.0020\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.1736\n",
      "Batch 70, Loss: 0.0006\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0191\n",
      "Epoch: 754 | Train loss: 0.03682 | Test loss: 0.05813 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0059\n",
      "Batch 20, Loss: 0.0046\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.1001\n",
      "Batch 50, Loss: 0.0204\n",
      "Batch 60, Loss: 0.0064\n",
      "Batch 70, Loss: 0.0681\n",
      "Batch 80, Loss: 0.0003\n",
      "Batch 90, Loss: 0.0408\n",
      "Epoch: 755 | Train loss: 0.03521 | Test loss: 0.24515 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0007\n",
      "Batch 30, Loss: 0.0005\n",
      "Batch 40, Loss: 0.1762\n",
      "Batch 50, Loss: 0.0373\n",
      "Batch 60, Loss: 0.0193\n",
      "Batch 70, Loss: 0.0068\n",
      "Batch 80, Loss: 0.0002\n",
      "Batch 90, Loss: 0.0204\n",
      "Epoch: 756 | Train loss: 0.02799 | Test loss: 0.05927 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0227\n",
      "Batch 10, Loss: 0.0081\n",
      "Batch 20, Loss: 0.0072\n",
      "Batch 30, Loss: 0.0044\n",
      "Batch 40, Loss: 0.0183\n",
      "Batch 50, Loss: 0.0166\n",
      "Batch 60, Loss: 0.1028\n",
      "Batch 70, Loss: 0.0164\n",
      "Batch 80, Loss: 0.0021\n",
      "Batch 90, Loss: 0.0117\n",
      "Epoch: 757 | Train loss: 0.05066 | Test loss: 0.08614 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0132\n",
      "Batch 10, Loss: 0.0015\n",
      "Batch 20, Loss: 0.0286\n",
      "Batch 30, Loss: 0.0018\n",
      "Batch 40, Loss: 0.0117\n",
      "Batch 50, Loss: 0.2288\n",
      "Batch 60, Loss: 0.0108\n",
      "Batch 70, Loss: 0.0013\n",
      "Batch 80, Loss: 0.0251\n",
      "Batch 90, Loss: 0.0145\n",
      "Epoch: 758 | Train loss: 0.03680 | Test loss: 0.11148 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3004\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0094\n",
      "Batch 30, Loss: 0.0158\n",
      "Batch 40, Loss: 0.0518\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0017\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0005\n",
      "Batch 90, Loss: 0.0886\n",
      "Epoch: 759 | Train loss: 0.03167 | Test loss: 0.15960 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1166\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.1240\n",
      "Batch 30, Loss: 0.0059\n",
      "Batch 40, Loss: 0.0094\n",
      "Batch 50, Loss: 0.0162\n",
      "Batch 60, Loss: 0.0013\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.1000\n",
      "Batch 90, Loss: 0.0206\n",
      "Epoch: 760 | Train loss: 0.04202 | Test loss: 1.39205 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0217\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.0526\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.0857\n",
      "Batch 60, Loss: 0.2092\n",
      "Batch 70, Loss: 0.0261\n",
      "Batch 80, Loss: 0.0047\n",
      "Batch 90, Loss: 0.0066\n",
      "Epoch: 761 | Train loss: 0.05912 | Test loss: 1.91150 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2054\n",
      "Batch 10, Loss: 0.0316\n",
      "Batch 20, Loss: 0.1849\n",
      "Batch 30, Loss: 0.0085\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0056\n",
      "Batch 60, Loss: 0.0053\n",
      "Batch 70, Loss: 0.0119\n",
      "Batch 80, Loss: 0.0017\n",
      "Batch 90, Loss: 0.0015\n",
      "Epoch: 762 | Train loss: 0.03731 | Test loss: 0.27557 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.5791\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0083\n",
      "Batch 30, Loss: 0.0117\n",
      "Batch 40, Loss: 0.0021\n",
      "Batch 50, Loss: 0.0036\n",
      "Batch 60, Loss: 0.2933\n",
      "Batch 70, Loss: 0.0106\n",
      "Batch 80, Loss: 0.0272\n",
      "Batch 90, Loss: 0.0478\n",
      "Epoch: 763 | Train loss: 0.07538 | Test loss: 0.08154 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0151\n",
      "Batch 10, Loss: 0.0241\n",
      "Batch 20, Loss: 0.1787\n",
      "Batch 30, Loss: 0.0473\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.0949\n",
      "Batch 60, Loss: 0.1882\n",
      "Batch 70, Loss: 0.0366\n",
      "Batch 80, Loss: 0.0537\n",
      "Batch 90, Loss: 0.0497\n",
      "Epoch: 764 | Train loss: 0.06722 | Test loss: 1.59736 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1079\n",
      "Batch 10, Loss: 0.0051\n",
      "Batch 20, Loss: 0.0854\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.0067\n",
      "Batch 50, Loss: 0.0779\n",
      "Batch 60, Loss: 0.0097\n",
      "Batch 70, Loss: 0.0111\n",
      "Batch 80, Loss: 0.0145\n",
      "Batch 90, Loss: 0.0591\n",
      "Epoch: 765 | Train loss: 0.04324 | Test loss: 0.04833 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0348\n",
      "Batch 10, Loss: 0.0255\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.1073\n",
      "Batch 40, Loss: 0.0268\n",
      "Batch 50, Loss: 0.0007\n",
      "Batch 60, Loss: 0.2264\n",
      "Batch 70, Loss: 0.0069\n",
      "Batch 80, Loss: 0.0044\n",
      "Batch 90, Loss: 0.0054\n",
      "Epoch: 766 | Train loss: 0.07279 | Test loss: 0.28230 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0036\n",
      "Batch 10, Loss: 0.0286\n",
      "Batch 20, Loss: 0.1682\n",
      "Batch 30, Loss: 0.1344\n",
      "Batch 40, Loss: 0.0182\n",
      "Batch 50, Loss: 0.0338\n",
      "Batch 60, Loss: 0.0088\n",
      "Batch 70, Loss: 0.0013\n",
      "Batch 80, Loss: 0.0270\n",
      "Batch 90, Loss: 0.0103\n",
      "Epoch: 767 | Train loss: 0.03158 | Test loss: 0.05456 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0655\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0093\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0030\n",
      "Batch 60, Loss: 0.0009\n",
      "Batch 70, Loss: 0.0073\n",
      "Batch 80, Loss: 0.0863\n",
      "Batch 90, Loss: 0.0072\n",
      "Epoch: 768 | Train loss: 0.02998 | Test loss: 0.07842 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0160\n",
      "Batch 10, Loss: 0.0081\n",
      "Batch 20, Loss: 0.0183\n",
      "Batch 30, Loss: 0.0748\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0185\n",
      "Batch 60, Loss: 0.0171\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0072\n",
      "Batch 90, Loss: 0.0162\n",
      "Epoch: 769 | Train loss: 0.02629 | Test loss: 0.13498 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0025\n",
      "Batch 10, Loss: 0.0006\n",
      "Batch 20, Loss: 0.1054\n",
      "Batch 30, Loss: 0.0438\n",
      "Batch 40, Loss: 0.0168\n",
      "Batch 50, Loss: 0.0339\n",
      "Batch 60, Loss: 0.0001\n",
      "Batch 70, Loss: 0.0124\n",
      "Batch 80, Loss: 0.0681\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 770 | Train loss: 0.06987 | Test loss: 0.11127 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0143\n",
      "Batch 10, Loss: 0.0736\n",
      "Batch 20, Loss: 0.0078\n",
      "Batch 30, Loss: 0.0050\n",
      "Batch 40, Loss: 0.0135\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.1497\n",
      "Batch 70, Loss: 0.2595\n",
      "Batch 80, Loss: 0.0667\n",
      "Batch 90, Loss: 0.1198\n",
      "Epoch: 771 | Train loss: 0.03858 | Test loss: 0.05761 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0004\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.0884\n",
      "Batch 40, Loss: 0.0168\n",
      "Batch 50, Loss: 0.0825\n",
      "Batch 60, Loss: 0.0126\n",
      "Batch 70, Loss: 0.0033\n",
      "Batch 80, Loss: 0.0131\n",
      "Batch 90, Loss: 0.0008\n",
      "Epoch: 772 | Train loss: 0.04403 | Test loss: 0.08761 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0272\n",
      "Batch 10, Loss: 0.0041\n",
      "Batch 20, Loss: 0.0224\n",
      "Batch 30, Loss: 0.0194\n",
      "Batch 40, Loss: 0.0349\n",
      "Batch 50, Loss: 0.0606\n",
      "Batch 60, Loss: 0.4867\n",
      "Batch 70, Loss: 0.0098\n",
      "Batch 80, Loss: 0.0424\n",
      "Batch 90, Loss: 0.0510\n",
      "Epoch: 773 | Train loss: 0.06321 | Test loss: 0.04836 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0088\n",
      "Batch 10, Loss: 0.1613\n",
      "Batch 20, Loss: 0.0078\n",
      "Batch 30, Loss: 0.0077\n",
      "Batch 40, Loss: 0.0036\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.0883\n",
      "Batch 70, Loss: 0.0566\n",
      "Batch 80, Loss: 0.0340\n",
      "Batch 90, Loss: 0.0050\n",
      "Epoch: 774 | Train loss: 0.04170 | Test loss: 0.49312 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0188\n",
      "Batch 20, Loss: 0.0155\n",
      "Batch 30, Loss: 0.0465\n",
      "Batch 40, Loss: 0.0340\n",
      "Batch 50, Loss: 0.0192\n",
      "Batch 60, Loss: 0.0187\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.1324\n",
      "Batch 90, Loss: 0.0014\n",
      "Epoch: 775 | Train loss: 0.03250 | Test loss: 0.07130 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0413\n",
      "Batch 10, Loss: 0.0158\n",
      "Batch 20, Loss: 0.1141\n",
      "Batch 30, Loss: 0.0166\n",
      "Batch 40, Loss: 0.0013\n",
      "Batch 50, Loss: 0.1090\n",
      "Batch 60, Loss: 0.0570\n",
      "Batch 70, Loss: 0.0244\n",
      "Batch 80, Loss: 0.0026\n",
      "Batch 90, Loss: 0.0089\n",
      "Epoch: 776 | Train loss: 0.05310 | Test loss: 2.70961 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0053\n",
      "Batch 10, Loss: 0.0401\n",
      "Batch 20, Loss: 0.0282\n",
      "Batch 30, Loss: 0.0238\n",
      "Batch 40, Loss: 0.3042\n",
      "Batch 50, Loss: 0.0098\n",
      "Batch 60, Loss: 0.4133\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0102\n",
      "Batch 90, Loss: 0.0400\n",
      "Epoch: 777 | Train loss: 0.08078 | Test loss: 0.06743 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0182\n",
      "Batch 10, Loss: 0.0116\n",
      "Batch 20, Loss: 0.7273\n",
      "Batch 30, Loss: 0.0646\n",
      "Batch 40, Loss: 0.0037\n",
      "Batch 50, Loss: 0.0098\n",
      "Batch 60, Loss: 0.0037\n",
      "Batch 70, Loss: 0.0192\n",
      "Batch 80, Loss: 0.0025\n",
      "Batch 90, Loss: 0.1048\n",
      "Epoch: 778 | Train loss: 0.05059 | Test loss: 0.04791 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0022\n",
      "Batch 10, Loss: 0.0071\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.4766\n",
      "Batch 40, Loss: 0.1327\n",
      "Batch 50, Loss: 0.0875\n",
      "Batch 60, Loss: 0.0121\n",
      "Batch 70, Loss: 0.0184\n",
      "Batch 80, Loss: 0.0215\n",
      "Batch 90, Loss: 0.0058\n",
      "Epoch: 779 | Train loss: 0.03385 | Test loss: 0.44434 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1084\n",
      "Batch 10, Loss: 0.0100\n",
      "Batch 20, Loss: 0.0086\n",
      "Batch 30, Loss: 0.0031\n",
      "Batch 40, Loss: 0.0257\n",
      "Batch 50, Loss: 0.0003\n",
      "Batch 60, Loss: 0.0014\n",
      "Batch 70, Loss: 0.0043\n",
      "Batch 80, Loss: 0.0698\n",
      "Batch 90, Loss: 0.0185\n",
      "Epoch: 780 | Train loss: 0.02716 | Test loss: 0.06257 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0211\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0041\n",
      "Batch 30, Loss: 0.1665\n",
      "Batch 40, Loss: 0.0075\n",
      "Batch 50, Loss: 0.0023\n",
      "Batch 60, Loss: 0.0099\n",
      "Batch 70, Loss: 0.0012\n",
      "Batch 80, Loss: 0.0737\n",
      "Batch 90, Loss: 0.0071\n",
      "Epoch: 781 | Train loss: 0.04144 | Test loss: 0.07998 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.0204\n",
      "Batch 20, Loss: 0.0075\n",
      "Batch 30, Loss: 0.1612\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.0600\n",
      "Batch 60, Loss: 0.0063\n",
      "Batch 70, Loss: 0.0151\n",
      "Batch 80, Loss: 0.0061\n",
      "Batch 90, Loss: 0.0302\n",
      "Epoch: 782 | Train loss: 0.04447 | Test loss: 0.16166 | Test accuracy: 0.45619\n",
      "Batch 0, Loss: 0.0404\n",
      "Batch 10, Loss: 0.0482\n",
      "Batch 20, Loss: 0.0011\n",
      "Batch 30, Loss: 0.0187\n",
      "Batch 40, Loss: 0.0197\n",
      "Batch 50, Loss: 0.0109\n",
      "Batch 60, Loss: 0.0203\n",
      "Batch 70, Loss: 0.0114\n",
      "Batch 80, Loss: 0.0010\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 783 | Train loss: 0.04590 | Test loss: 0.12345 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0041\n",
      "Batch 10, Loss: 0.0171\n",
      "Batch 20, Loss: 0.0566\n",
      "Batch 30, Loss: 0.0436\n",
      "Batch 40, Loss: 0.1586\n",
      "Batch 50, Loss: 0.0386\n",
      "Batch 60, Loss: 0.0002\n",
      "Batch 70, Loss: 0.0027\n",
      "Batch 80, Loss: 0.1310\n",
      "Batch 90, Loss: 0.0048\n",
      "Epoch: 784 | Train loss: 0.04018 | Test loss: 2.24279 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1757\n",
      "Batch 10, Loss: 0.0136\n",
      "Batch 20, Loss: 0.4474\n",
      "Batch 30, Loss: 0.0128\n",
      "Batch 40, Loss: 0.0105\n",
      "Batch 50, Loss: 0.0718\n",
      "Batch 60, Loss: 0.0868\n",
      "Batch 70, Loss: 0.1401\n",
      "Batch 80, Loss: 0.0336\n",
      "Batch 90, Loss: 0.0396\n",
      "Epoch: 785 | Train loss: 0.05383 | Test loss: 0.37814 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.2489\n",
      "Batch 10, Loss: 0.0346\n",
      "Batch 20, Loss: 0.0101\n",
      "Batch 30, Loss: 0.0084\n",
      "Batch 40, Loss: 0.0148\n",
      "Batch 50, Loss: 0.0119\n",
      "Batch 60, Loss: 0.0234\n",
      "Batch 70, Loss: 0.0082\n",
      "Batch 80, Loss: 0.0262\n",
      "Batch 90, Loss: 0.0127\n",
      "Epoch: 786 | Train loss: 0.04465 | Test loss: 0.08267 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0311\n",
      "Batch 10, Loss: 0.0090\n",
      "Batch 20, Loss: 0.0541\n",
      "Batch 30, Loss: 0.0186\n",
      "Batch 40, Loss: 0.0481\n",
      "Batch 50, Loss: 0.0196\n",
      "Batch 60, Loss: 0.0143\n",
      "Batch 70, Loss: 0.0032\n",
      "Batch 80, Loss: 0.0087\n",
      "Batch 90, Loss: 0.0219\n",
      "Epoch: 787 | Train loss: 0.03318 | Test loss: 1.26540 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0012\n",
      "Batch 20, Loss: 0.0074\n",
      "Batch 30, Loss: 0.0079\n",
      "Batch 40, Loss: 0.0036\n",
      "Batch 50, Loss: 0.0039\n",
      "Batch 60, Loss: 0.0060\n",
      "Batch 70, Loss: 0.0124\n",
      "Batch 80, Loss: 0.0382\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 788 | Train loss: 0.03121 | Test loss: 0.07944 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1189\n",
      "Batch 10, Loss: 0.0133\n",
      "Batch 20, Loss: 0.0212\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.0076\n",
      "Batch 50, Loss: 0.0324\n",
      "Batch 60, Loss: 0.0254\n",
      "Batch 70, Loss: 0.0180\n",
      "Batch 80, Loss: 0.0433\n",
      "Batch 90, Loss: 0.0124\n",
      "Epoch: 789 | Train loss: 0.05120 | Test loss: 0.04777 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0041\n",
      "Batch 10, Loss: 0.0234\n",
      "Batch 20, Loss: 0.0051\n",
      "Batch 30, Loss: 0.0539\n",
      "Batch 40, Loss: 0.0078\n",
      "Batch 50, Loss: 0.1539\n",
      "Batch 60, Loss: 0.0471\n",
      "Batch 70, Loss: 0.0002\n",
      "Batch 80, Loss: 0.0442\n",
      "Batch 90, Loss: 0.0249\n",
      "Epoch: 790 | Train loss: 0.06265 | Test loss: 0.11203 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1946\n",
      "Batch 10, Loss: 0.1102\n",
      "Batch 20, Loss: 0.1955\n",
      "Batch 30, Loss: 0.8288\n",
      "Batch 40, Loss: 0.0933\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 60, Loss: 0.1681\n",
      "Batch 70, Loss: 0.0090\n",
      "Batch 80, Loss: 0.0407\n",
      "Batch 90, Loss: 0.0358\n",
      "Epoch: 791 | Train loss: 0.08424 | Test loss: 0.84374 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0512\n",
      "Batch 10, Loss: 0.0070\n",
      "Batch 20, Loss: 0.0050\n",
      "Batch 30, Loss: 0.0583\n",
      "Batch 40, Loss: 0.0043\n",
      "Batch 50, Loss: 0.0040\n",
      "Batch 60, Loss: 0.1578\n",
      "Batch 70, Loss: 0.0152\n",
      "Batch 80, Loss: 0.0035\n",
      "Batch 90, Loss: 0.2856\n",
      "Epoch: 792 | Train loss: 0.04567 | Test loss: 0.28206 | Test accuracy: 0.55412\n",
      "Batch 0, Loss: 0.0268\n",
      "Batch 10, Loss: 0.0573\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0962\n",
      "Batch 50, Loss: 0.0592\n",
      "Batch 60, Loss: 0.0590\n",
      "Batch 70, Loss: 0.0257\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0467\n",
      "Epoch: 793 | Train loss: 0.05203 | Test loss: 0.05666 | Test accuracy: 0.55155\n",
      "Batch 0, Loss: 0.0475\n",
      "Batch 10, Loss: 0.0493\n",
      "Batch 20, Loss: 0.0482\n",
      "Batch 30, Loss: 0.0095\n",
      "Batch 40, Loss: 0.3384\n",
      "Batch 50, Loss: 0.0322\n",
      "Batch 60, Loss: 0.1531\n",
      "Batch 70, Loss: 0.0066\n",
      "Batch 80, Loss: 0.0555\n",
      "Batch 90, Loss: 0.0121\n",
      "Epoch: 794 | Train loss: 0.03839 | Test loss: 2.95331 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.0466\n",
      "Batch 20, Loss: 0.1345\n",
      "Batch 30, Loss: 0.0107\n",
      "Batch 40, Loss: 0.0008\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0029\n",
      "Batch 70, Loss: 0.0328\n",
      "Batch 80, Loss: 0.0086\n",
      "Batch 90, Loss: 0.0248\n",
      "Epoch: 795 | Train loss: 0.02317 | Test loss: 0.13429 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0009\n",
      "Batch 20, Loss: 0.0132\n",
      "Batch 30, Loss: 0.0016\n",
      "Batch 40, Loss: 0.0280\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0071\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.2543\n",
      "Batch 90, Loss: 0.0036\n",
      "Epoch: 796 | Train loss: 0.02065 | Test loss: 0.17031 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0190\n",
      "Batch 10, Loss: 0.0528\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0154\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.1322\n",
      "Batch 60, Loss: 0.0702\n",
      "Batch 70, Loss: 0.0498\n",
      "Batch 80, Loss: 0.0121\n",
      "Batch 90, Loss: 0.0016\n",
      "Epoch: 797 | Train loss: 0.05416 | Test loss: 0.14214 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.0035\n",
      "Batch 20, Loss: 0.0021\n",
      "Batch 30, Loss: 0.0035\n",
      "Batch 40, Loss: 0.0134\n",
      "Batch 50, Loss: 0.0919\n",
      "Batch 60, Loss: 0.0138\n",
      "Batch 70, Loss: 0.0904\n",
      "Batch 80, Loss: 0.1319\n",
      "Batch 90, Loss: 0.0221\n",
      "Epoch: 798 | Train loss: 0.08574 | Test loss: 0.05197 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0281\n",
      "Batch 10, Loss: 0.0242\n",
      "Batch 20, Loss: 0.0089\n",
      "Batch 30, Loss: 0.0653\n",
      "Batch 40, Loss: 0.0098\n",
      "Batch 50, Loss: 0.0209\n",
      "Batch 60, Loss: 0.0588\n",
      "Batch 70, Loss: 0.0097\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 799 | Train loss: 0.04979 | Test loss: 1.37699 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1076\n",
      "Batch 10, Loss: 0.0198\n",
      "Batch 20, Loss: 0.0511\n",
      "Batch 30, Loss: 0.0163\n",
      "Batch 40, Loss: 0.0162\n",
      "Batch 50, Loss: 0.0083\n",
      "Batch 60, Loss: 0.0129\n",
      "Batch 70, Loss: 0.0022\n",
      "Batch 80, Loss: 0.0729\n",
      "Batch 90, Loss: 0.0069\n",
      "Epoch: 800 | Train loss: 0.07084 | Test loss: 0.11101 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0036\n",
      "Batch 10, Loss: 0.4336\n",
      "Batch 20, Loss: 0.0040\n",
      "Batch 30, Loss: 0.0697\n",
      "Batch 40, Loss: 0.0838\n",
      "Batch 50, Loss: 0.0037\n",
      "Batch 60, Loss: 0.4407\n",
      "Batch 70, Loss: 0.0184\n",
      "Batch 80, Loss: 0.0958\n",
      "Batch 90, Loss: 0.0431\n",
      "Epoch: 801 | Train loss: 0.05588 | Test loss: 0.08584 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0708\n",
      "Batch 20, Loss: 0.0172\n",
      "Batch 30, Loss: 0.0094\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.0136\n",
      "Batch 60, Loss: 0.0119\n",
      "Batch 70, Loss: 0.0098\n",
      "Batch 80, Loss: 0.0302\n",
      "Batch 90, Loss: 0.0060\n",
      "Epoch: 802 | Train loss: 0.03871 | Test loss: 0.04577 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0241\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.1153\n",
      "Batch 40, Loss: 0.1849\n",
      "Batch 50, Loss: 0.0449\n",
      "Batch 60, Loss: 0.0608\n",
      "Batch 70, Loss: 0.0381\n",
      "Batch 80, Loss: 0.0281\n",
      "Batch 90, Loss: 0.0240\n",
      "Epoch: 803 | Train loss: 0.03689 | Test loss: 0.33466 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0008\n",
      "Batch 10, Loss: 0.0034\n",
      "Batch 20, Loss: 0.0310\n",
      "Batch 30, Loss: 0.0411\n",
      "Batch 40, Loss: 0.0398\n",
      "Batch 50, Loss: 0.0080\n",
      "Batch 60, Loss: 0.0065\n",
      "Batch 70, Loss: 0.0020\n",
      "Batch 80, Loss: 0.0703\n",
      "Batch 90, Loss: 0.0468\n",
      "Epoch: 804 | Train loss: 0.03140 | Test loss: 0.10172 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0163\n",
      "Batch 10, Loss: 0.0009\n",
      "Batch 20, Loss: 0.0002\n",
      "Batch 30, Loss: 0.0046\n",
      "Batch 40, Loss: 0.0116\n",
      "Batch 50, Loss: 0.0283\n",
      "Batch 60, Loss: 0.0011\n",
      "Batch 70, Loss: 0.1647\n",
      "Batch 80, Loss: 0.0027\n",
      "Batch 90, Loss: 0.0697\n",
      "Epoch: 805 | Train loss: 0.02400 | Test loss: 0.04903 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0661\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.0048\n",
      "Batch 30, Loss: 0.0014\n",
      "Batch 40, Loss: 0.0116\n",
      "Batch 50, Loss: 0.0045\n",
      "Batch 60, Loss: 0.0060\n",
      "Batch 70, Loss: 0.0819\n",
      "Batch 80, Loss: 0.0098\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 806 | Train loss: 0.04455 | Test loss: 0.60766 | Test accuracy: 0.54897\n",
      "Batch 0, Loss: 0.1590\n",
      "Batch 10, Loss: 0.1358\n",
      "Batch 20, Loss: 0.0105\n",
      "Batch 30, Loss: 0.0038\n",
      "Batch 40, Loss: 0.0019\n",
      "Batch 50, Loss: 0.0192\n",
      "Batch 60, Loss: 0.0074\n",
      "Batch 70, Loss: 0.0085\n",
      "Batch 80, Loss: 0.0637\n",
      "Batch 90, Loss: 0.0219\n",
      "Epoch: 807 | Train loss: 0.05891 | Test loss: 0.05534 | Test accuracy: 0.54639\n",
      "Batch 0, Loss: 0.1086\n",
      "Batch 10, Loss: 0.1217\n",
      "Batch 20, Loss: 0.0016\n",
      "Batch 30, Loss: 0.2274\n",
      "Batch 40, Loss: 0.1808\n",
      "Batch 50, Loss: 0.0151\n",
      "Batch 60, Loss: 0.0028\n",
      "Batch 70, Loss: 0.0141\n",
      "Batch 80, Loss: 0.0002\n",
      "Batch 90, Loss: 0.0258\n",
      "Epoch: 808 | Train loss: 0.04291 | Test loss: 0.04685 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.1768\n",
      "Batch 10, Loss: 0.0432\n",
      "Batch 20, Loss: 0.0019\n",
      "Batch 30, Loss: 0.0042\n",
      "Batch 40, Loss: 0.1440\n",
      "Batch 50, Loss: 0.0239\n",
      "Batch 60, Loss: 0.0472\n",
      "Batch 70, Loss: 0.0116\n",
      "Batch 80, Loss: 0.0360\n",
      "Batch 90, Loss: 0.0213\n",
      "Epoch: 809 | Train loss: 0.06775 | Test loss: 1.29530 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0221\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0056\n",
      "Batch 50, Loss: 0.0171\n",
      "Batch 60, Loss: 0.0244\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0130\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 810 | Train loss: 0.04053 | Test loss: 0.07955 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0106\n",
      "Batch 10, Loss: 0.0300\n",
      "Batch 20, Loss: 0.0317\n",
      "Batch 30, Loss: 0.0053\n",
      "Batch 40, Loss: 0.0026\n",
      "Batch 50, Loss: 0.0021\n",
      "Batch 60, Loss: 0.1426\n",
      "Batch 70, Loss: 0.0094\n",
      "Batch 80, Loss: 0.0065\n",
      "Batch 90, Loss: 0.0013\n",
      "Epoch: 811 | Train loss: 0.03999 | Test loss: 0.21918 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0115\n",
      "Batch 10, Loss: 0.0231\n",
      "Batch 20, Loss: 0.0141\n",
      "Batch 30, Loss: 0.0298\n",
      "Batch 40, Loss: 0.0923\n",
      "Batch 50, Loss: 0.0542\n",
      "Batch 60, Loss: 0.0066\n",
      "Batch 70, Loss: 0.0716\n",
      "Batch 80, Loss: 0.0355\n",
      "Batch 90, Loss: 0.0804\n",
      "Epoch: 812 | Train loss: 0.05129 | Test loss: 0.56971 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0626\n",
      "Batch 10, Loss: 0.0962\n",
      "Batch 20, Loss: 0.0735\n",
      "Batch 30, Loss: 0.2707\n",
      "Batch 40, Loss: 0.0127\n",
      "Batch 50, Loss: 0.0007\n",
      "Batch 60, Loss: 0.1007\n",
      "Batch 70, Loss: 0.0005\n",
      "Batch 80, Loss: 0.0210\n",
      "Batch 90, Loss: 0.0107\n",
      "Epoch: 813 | Train loss: 0.03873 | Test loss: 0.05506 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0111\n",
      "Batch 10, Loss: 0.0004\n",
      "Batch 20, Loss: 0.0042\n",
      "Batch 30, Loss: 0.0024\n",
      "Batch 40, Loss: 0.0113\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0750\n",
      "Batch 70, Loss: 0.0511\n",
      "Batch 80, Loss: 0.0293\n",
      "Batch 90, Loss: 0.1347\n",
      "Epoch: 814 | Train loss: 0.09306 | Test loss: 0.75819 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0395\n",
      "Batch 10, Loss: 0.0124\n",
      "Batch 20, Loss: 0.0147\n",
      "Batch 30, Loss: 0.1064\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0216\n",
      "Batch 60, Loss: 0.0156\n",
      "Batch 70, Loss: 0.0660\n",
      "Batch 80, Loss: 0.1352\n",
      "Batch 90, Loss: 0.0127\n",
      "Epoch: 815 | Train loss: 0.05725 | Test loss: 0.07697 | Test accuracy: 0.55928\n",
      "Batch 0, Loss: 0.0031\n",
      "Batch 10, Loss: 0.0053\n",
      "Batch 20, Loss: 0.1556\n",
      "Batch 30, Loss: 0.1203\n",
      "Batch 40, Loss: 0.0044\n",
      "Batch 50, Loss: 0.1732\n",
      "Batch 60, Loss: 0.0334\n",
      "Batch 70, Loss: 0.0549\n",
      "Batch 80, Loss: 0.0177\n",
      "Batch 90, Loss: 0.0145\n",
      "Epoch: 816 | Train loss: 0.03942 | Test loss: 0.04350 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0484\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.2232\n",
      "Batch 30, Loss: 0.0595\n",
      "Batch 40, Loss: 0.0051\n",
      "Batch 50, Loss: 0.0065\n",
      "Batch 60, Loss: 0.0021\n",
      "Batch 70, Loss: 0.0186\n",
      "Batch 80, Loss: 0.0063\n",
      "Batch 90, Loss: 0.0150\n",
      "Epoch: 817 | Train loss: 0.03690 | Test loss: 0.13912 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0014\n",
      "Batch 10, Loss: 0.0418\n",
      "Batch 20, Loss: 0.0063\n",
      "Batch 30, Loss: 0.0452\n",
      "Batch 40, Loss: 0.1684\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0197\n",
      "Batch 70, Loss: 0.0057\n",
      "Batch 80, Loss: 0.0340\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 818 | Train loss: 0.04199 | Test loss: 0.04987 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0363\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0164\n",
      "Batch 40, Loss: 0.0040\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.0346\n",
      "Batch 70, Loss: 0.0019\n",
      "Batch 80, Loss: 0.3877\n",
      "Batch 90, Loss: 0.0302\n",
      "Epoch: 819 | Train loss: 0.02368 | Test loss: 0.04447 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0003\n",
      "Batch 30, Loss: 0.0352\n",
      "Batch 40, Loss: 0.0036\n",
      "Batch 50, Loss: 0.0090\n",
      "Batch 60, Loss: 0.0307\n",
      "Batch 70, Loss: 0.0133\n",
      "Batch 80, Loss: 0.0589\n",
      "Batch 90, Loss: 0.0008\n",
      "Epoch: 820 | Train loss: 0.03185 | Test loss: 0.47971 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0070\n",
      "Batch 10, Loss: 0.0159\n",
      "Batch 20, Loss: 0.0131\n",
      "Batch 30, Loss: 0.0020\n",
      "Batch 40, Loss: 0.0101\n",
      "Batch 50, Loss: 0.0496\n",
      "Batch 60, Loss: 0.0127\n",
      "Batch 70, Loss: 0.0470\n",
      "Batch 80, Loss: 0.0650\n",
      "Batch 90, Loss: 0.0257\n",
      "Epoch: 821 | Train loss: 0.04337 | Test loss: 0.06923 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.2529\n",
      "Batch 10, Loss: 0.3555\n",
      "Batch 20, Loss: 0.0159\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0143\n",
      "Batch 50, Loss: 0.0421\n",
      "Batch 60, Loss: 0.0048\n",
      "Batch 70, Loss: 0.0016\n",
      "Batch 80, Loss: 0.0049\n",
      "Batch 90, Loss: 0.0198\n",
      "Epoch: 822 | Train loss: 0.03208 | Test loss: 0.08458 | Test accuracy: 0.56186\n",
      "Batch 0, Loss: 0.0339\n",
      "Batch 10, Loss: 0.0022\n",
      "Batch 20, Loss: 0.0003\n",
      "Batch 30, Loss: 0.0480\n",
      "Batch 40, Loss: 0.0072\n",
      "Batch 50, Loss: 0.0004\n",
      "Batch 60, Loss: 0.0072\n",
      "Batch 70, Loss: 0.0480\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0358\n",
      "Epoch: 823 | Train loss: 0.06337 | Test loss: 0.27124 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0326\n",
      "Batch 10, Loss: 0.0251\n",
      "Batch 20, Loss: 0.0101\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0068\n",
      "Batch 50, Loss: 0.0298\n",
      "Batch 60, Loss: 0.0095\n",
      "Batch 70, Loss: 0.0068\n",
      "Batch 80, Loss: 0.0060\n",
      "Batch 90, Loss: 0.0015\n",
      "Epoch: 824 | Train loss: 0.02628 | Test loss: 0.07387 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.0179\n",
      "Batch 20, Loss: 0.0831\n",
      "Batch 30, Loss: 0.0288\n",
      "Batch 40, Loss: 0.0365\n",
      "Batch 50, Loss: 0.0137\n",
      "Batch 60, Loss: 0.3641\n",
      "Batch 70, Loss: 0.0525\n",
      "Batch 80, Loss: 0.0054\n",
      "Batch 90, Loss: 0.0750\n",
      "Epoch: 825 | Train loss: 0.03961 | Test loss: 0.04652 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0192\n",
      "Batch 10, Loss: 0.1051\n",
      "Batch 20, Loss: 0.0195\n",
      "Batch 30, Loss: 0.0366\n",
      "Batch 40, Loss: 0.0578\n",
      "Batch 50, Loss: 0.0251\n",
      "Batch 60, Loss: 0.0027\n",
      "Batch 70, Loss: 0.0707\n",
      "Batch 80, Loss: 0.0243\n",
      "Batch 90, Loss: 0.0043\n",
      "Epoch: 826 | Train loss: 0.03053 | Test loss: 1.41630 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0027\n",
      "Batch 10, Loss: 0.0033\n",
      "Batch 20, Loss: 0.0064\n",
      "Batch 30, Loss: 0.0538\n",
      "Batch 40, Loss: 0.0354\n",
      "Batch 50, Loss: 0.1507\n",
      "Batch 60, Loss: 0.0360\n",
      "Batch 70, Loss: 0.0626\n",
      "Batch 80, Loss: 0.0482\n",
      "Batch 90, Loss: 0.0323\n",
      "Epoch: 827 | Train loss: 0.05548 | Test loss: 0.19046 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0105\n",
      "Batch 10, Loss: 0.5437\n",
      "Batch 20, Loss: 0.0356\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0404\n",
      "Batch 50, Loss: 0.5493\n",
      "Batch 60, Loss: 0.0217\n",
      "Batch 70, Loss: 0.0391\n",
      "Batch 80, Loss: 0.0413\n",
      "Batch 90, Loss: 0.0378\n",
      "Epoch: 828 | Train loss: 0.07203 | Test loss: 0.05316 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0703\n",
      "Batch 10, Loss: 0.0248\n",
      "Batch 20, Loss: 0.0926\n",
      "Batch 30, Loss: 0.0179\n",
      "Batch 40, Loss: 0.1019\n",
      "Batch 50, Loss: 0.1107\n",
      "Batch 60, Loss: 0.0048\n",
      "Batch 70, Loss: 0.0666\n",
      "Batch 80, Loss: 0.0087\n",
      "Batch 90, Loss: 0.0176\n",
      "Epoch: 829 | Train loss: 0.06783 | Test loss: 0.04401 | Test accuracy: 0.55412\n",
      "Batch 0, Loss: 0.0096\n",
      "Batch 10, Loss: 0.0026\n",
      "Batch 20, Loss: 0.0144\n",
      "Batch 30, Loss: 0.1400\n",
      "Batch 40, Loss: 0.0253\n",
      "Batch 50, Loss: 0.0922\n",
      "Batch 60, Loss: 0.0371\n",
      "Batch 70, Loss: 0.0177\n",
      "Batch 80, Loss: 0.1466\n",
      "Batch 90, Loss: 0.0995\n",
      "Epoch: 830 | Train loss: 0.09483 | Test loss: 0.11312 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3029\n",
      "Batch 10, Loss: 0.0060\n",
      "Batch 20, Loss: 0.7402\n",
      "Batch 30, Loss: 0.0680\n",
      "Batch 40, Loss: 0.0361\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.1006\n",
      "Batch 70, Loss: 0.0131\n",
      "Batch 80, Loss: 0.0444\n",
      "Batch 90, Loss: 0.1832\n",
      "Epoch: 831 | Train loss: 0.05394 | Test loss: 0.04791 | Test accuracy: 0.70876\n",
      "Batch 0, Loss: 0.0156\n",
      "Batch 10, Loss: 0.0148\n",
      "Batch 20, Loss: 0.0497\n",
      "Batch 30, Loss: 0.0028\n",
      "Batch 40, Loss: 0.0069\n",
      "Batch 50, Loss: 0.0455\n",
      "Batch 60, Loss: 0.1409\n",
      "Batch 70, Loss: 0.0510\n",
      "Batch 80, Loss: 0.0190\n",
      "Batch 90, Loss: 0.0055\n",
      "Epoch: 832 | Train loss: 0.06544 | Test loss: 0.13908 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0113\n",
      "Batch 10, Loss: 0.0097\n",
      "Batch 20, Loss: 0.0023\n",
      "Batch 30, Loss: 0.0286\n",
      "Batch 40, Loss: 0.0543\n",
      "Batch 50, Loss: 0.0004\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.0081\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 833 | Train loss: 0.03714 | Test loss: 0.06262 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0021\n",
      "Batch 10, Loss: 0.0302\n",
      "Batch 20, Loss: 0.0151\n",
      "Batch 30, Loss: 0.9612\n",
      "Batch 40, Loss: 0.0013\n",
      "Batch 50, Loss: 0.0265\n",
      "Batch 60, Loss: 0.0450\n",
      "Batch 70, Loss: 0.0914\n",
      "Batch 80, Loss: 0.0344\n",
      "Batch 90, Loss: 0.2216\n",
      "Epoch: 834 | Train loss: 0.06270 | Test loss: 0.04142 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0287\n",
      "Batch 10, Loss: 0.0110\n",
      "Batch 20, Loss: 0.0263\n",
      "Batch 30, Loss: 0.0200\n",
      "Batch 40, Loss: 0.0259\n",
      "Batch 50, Loss: 0.0318\n",
      "Batch 60, Loss: 0.0465\n",
      "Batch 70, Loss: 0.0092\n",
      "Batch 80, Loss: 0.0221\n",
      "Batch 90, Loss: 0.0379\n",
      "Epoch: 835 | Train loss: 0.05218 | Test loss: 0.05976 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0291\n",
      "Batch 10, Loss: 0.0065\n",
      "Batch 20, Loss: 0.0204\n",
      "Batch 30, Loss: 0.0568\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.0024\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0364\n",
      "Batch 80, Loss: 0.0036\n",
      "Batch 90, Loss: 0.0239\n",
      "Epoch: 836 | Train loss: 0.03117 | Test loss: 0.05465 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0442\n",
      "Batch 10, Loss: 0.0072\n",
      "Batch 20, Loss: 0.0016\n",
      "Batch 30, Loss: 0.0111\n",
      "Batch 40, Loss: 0.0040\n",
      "Batch 50, Loss: 0.1428\n",
      "Batch 60, Loss: 0.0095\n",
      "Batch 70, Loss: 0.0035\n",
      "Batch 80, Loss: 0.0127\n",
      "Batch 90, Loss: 0.0082\n",
      "Epoch: 837 | Train loss: 0.02286 | Test loss: 0.05319 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0073\n",
      "Batch 10, Loss: 0.0969\n",
      "Batch 20, Loss: 0.0657\n",
      "Batch 30, Loss: 0.0215\n",
      "Batch 40, Loss: 0.0329\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.0759\n",
      "Batch 80, Loss: 0.0257\n",
      "Batch 90, Loss: 0.0422\n",
      "Epoch: 838 | Train loss: 0.02685 | Test loss: 0.04515 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0056\n",
      "Batch 20, Loss: 0.0058\n",
      "Batch 30, Loss: 0.0026\n",
      "Batch 40, Loss: 0.0142\n",
      "Batch 50, Loss: 0.0857\n",
      "Batch 60, Loss: 0.0144\n",
      "Batch 70, Loss: 0.0048\n",
      "Batch 80, Loss: 0.0046\n",
      "Batch 90, Loss: 0.0410\n",
      "Epoch: 839 | Train loss: 0.03926 | Test loss: 0.14563 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0038\n",
      "Batch 10, Loss: 0.0268\n",
      "Batch 20, Loss: 0.0005\n",
      "Batch 30, Loss: 0.0002\n",
      "Batch 40, Loss: 0.0038\n",
      "Batch 50, Loss: 0.0189\n",
      "Batch 60, Loss: 0.0199\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.2811\n",
      "Batch 90, Loss: 0.0347\n",
      "Epoch: 840 | Train loss: 0.05081 | Test loss: 0.06010 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0148\n",
      "Batch 10, Loss: 0.0931\n",
      "Batch 20, Loss: 0.0109\n",
      "Batch 30, Loss: 0.0201\n",
      "Batch 40, Loss: 0.1840\n",
      "Batch 50, Loss: 0.0058\n",
      "Batch 60, Loss: 0.0290\n",
      "Batch 70, Loss: 0.0401\n",
      "Batch 80, Loss: 0.0019\n",
      "Batch 90, Loss: 0.0062\n",
      "Epoch: 841 | Train loss: 0.04838 | Test loss: 0.11862 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0328\n",
      "Batch 10, Loss: 0.0495\n",
      "Batch 20, Loss: 0.0419\n",
      "Batch 30, Loss: 0.0299\n",
      "Batch 40, Loss: 0.0112\n",
      "Batch 50, Loss: 0.0007\n",
      "Batch 60, Loss: 0.1127\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.0352\n",
      "Batch 90, Loss: 0.0047\n",
      "Epoch: 842 | Train loss: 0.06600 | Test loss: 0.05632 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.0365\n",
      "Batch 20, Loss: 0.0368\n",
      "Batch 30, Loss: 0.1135\n",
      "Batch 40, Loss: 0.0687\n",
      "Batch 50, Loss: 0.0178\n",
      "Batch 60, Loss: 0.0459\n",
      "Batch 70, Loss: 0.1063\n",
      "Batch 80, Loss: 0.0059\n",
      "Batch 90, Loss: 0.0467\n",
      "Epoch: 843 | Train loss: 0.06038 | Test loss: 0.06633 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0202\n",
      "Batch 10, Loss: 0.0006\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.2039\n",
      "Batch 40, Loss: 0.0213\n",
      "Batch 50, Loss: 0.0053\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.0087\n",
      "Batch 80, Loss: 0.0007\n",
      "Batch 90, Loss: 0.0002\n",
      "Epoch: 844 | Train loss: 0.02558 | Test loss: 0.10978 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0010\n",
      "Batch 10, Loss: 0.0393\n",
      "Batch 20, Loss: 0.0025\n",
      "Batch 30, Loss: 0.0073\n",
      "Batch 40, Loss: 0.0059\n",
      "Batch 50, Loss: 0.0678\n",
      "Batch 60, Loss: 0.0027\n",
      "Batch 70, Loss: 0.0162\n",
      "Batch 80, Loss: 0.0493\n",
      "Batch 90, Loss: 0.0057\n",
      "Epoch: 845 | Train loss: 0.02621 | Test loss: 0.05687 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0242\n",
      "Batch 10, Loss: 0.0020\n",
      "Batch 20, Loss: 0.0207\n",
      "Batch 30, Loss: 0.0112\n",
      "Batch 40, Loss: 0.0107\n",
      "Batch 50, Loss: 0.0007\n",
      "Batch 60, Loss: 0.1799\n",
      "Batch 70, Loss: 0.0298\n",
      "Batch 80, Loss: 0.0045\n",
      "Batch 90, Loss: 0.0151\n",
      "Epoch: 846 | Train loss: 0.02796 | Test loss: 0.07432 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0051\n",
      "Batch 10, Loss: 0.0384\n",
      "Batch 20, Loss: 0.0110\n",
      "Batch 30, Loss: 0.0005\n",
      "Batch 40, Loss: 0.0125\n",
      "Batch 50, Loss: 0.0211\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.0725\n",
      "Batch 80, Loss: 0.0051\n",
      "Batch 90, Loss: 0.0026\n",
      "Epoch: 847 | Train loss: 0.04973 | Test loss: 0.56805 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0386\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0017\n",
      "Batch 30, Loss: 0.0210\n",
      "Batch 40, Loss: 0.0569\n",
      "Batch 50, Loss: 0.0226\n",
      "Batch 60, Loss: 0.0092\n",
      "Batch 70, Loss: 0.0145\n",
      "Batch 80, Loss: 0.0538\n",
      "Batch 90, Loss: 0.0227\n",
      "Epoch: 848 | Train loss: 0.04490 | Test loss: 2.46890 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0264\n",
      "Batch 10, Loss: 0.0624\n",
      "Batch 20, Loss: 0.0213\n",
      "Batch 30, Loss: 0.0157\n",
      "Batch 40, Loss: 0.2499\n",
      "Batch 50, Loss: 0.0125\n",
      "Batch 60, Loss: 0.0016\n",
      "Batch 70, Loss: 0.0022\n",
      "Batch 80, Loss: 0.3614\n",
      "Batch 90, Loss: 0.1292\n",
      "Epoch: 849 | Train loss: 0.06126 | Test loss: 0.06190 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0043\n",
      "Batch 10, Loss: 0.0898\n",
      "Batch 20, Loss: 0.0099\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.1321\n",
      "Batch 50, Loss: 0.0086\n",
      "Batch 60, Loss: 0.0835\n",
      "Batch 70, Loss: 0.0019\n",
      "Batch 80, Loss: 0.2906\n",
      "Batch 90, Loss: 0.0302\n",
      "Epoch: 850 | Train loss: 0.07562 | Test loss: 0.06088 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0592\n",
      "Batch 10, Loss: 0.0515\n",
      "Batch 20, Loss: 0.1970\n",
      "Batch 30, Loss: 0.1555\n",
      "Batch 40, Loss: 0.0130\n",
      "Batch 50, Loss: 0.0239\n",
      "Batch 60, Loss: 0.0476\n",
      "Batch 70, Loss: 0.0081\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0302\n",
      "Epoch: 851 | Train loss: 0.08172 | Test loss: 0.05630 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0965\n",
      "Batch 10, Loss: 0.0016\n",
      "Batch 20, Loss: 0.0888\n",
      "Batch 30, Loss: 0.0261\n",
      "Batch 40, Loss: 0.0047\n",
      "Batch 50, Loss: 0.1785\n",
      "Batch 60, Loss: 0.0178\n",
      "Batch 70, Loss: 0.1475\n",
      "Batch 80, Loss: 0.0275\n",
      "Batch 90, Loss: 0.0488\n",
      "Epoch: 852 | Train loss: 0.04113 | Test loss: 0.04572 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0023\n",
      "Batch 10, Loss: 0.0043\n",
      "Batch 20, Loss: 0.0047\n",
      "Batch 30, Loss: 0.0576\n",
      "Batch 40, Loss: 0.0383\n",
      "Batch 50, Loss: 0.0030\n",
      "Batch 60, Loss: 0.0432\n",
      "Batch 70, Loss: 0.0155\n",
      "Batch 80, Loss: 0.0238\n",
      "Batch 90, Loss: 0.0176\n",
      "Epoch: 853 | Train loss: 0.04376 | Test loss: 0.07238 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.3169\n",
      "Batch 20, Loss: 0.0437\n",
      "Batch 30, Loss: 0.0248\n",
      "Batch 40, Loss: 0.0227\n",
      "Batch 50, Loss: 0.0607\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0036\n",
      "Batch 80, Loss: 0.0593\n",
      "Batch 90, Loss: 0.0090\n",
      "Epoch: 854 | Train loss: 0.06434 | Test loss: 0.16208 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0049\n",
      "Batch 20, Loss: 0.0605\n",
      "Batch 30, Loss: 0.0591\n",
      "Batch 40, Loss: 0.0139\n",
      "Batch 50, Loss: 0.2669\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.0328\n",
      "Batch 80, Loss: 0.0604\n",
      "Batch 90, Loss: 0.0235\n",
      "Epoch: 855 | Train loss: 0.06834 | Test loss: 0.10733 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0125\n",
      "Batch 10, Loss: 0.3174\n",
      "Batch 20, Loss: 0.0412\n",
      "Batch 30, Loss: 0.5449\n",
      "Batch 40, Loss: 0.0069\n",
      "Batch 50, Loss: 0.0127\n",
      "Batch 60, Loss: 0.0042\n",
      "Batch 70, Loss: 0.0138\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.0106\n",
      "Epoch: 856 | Train loss: 0.08393 | Test loss: 0.29883 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0033\n",
      "Batch 20, Loss: 0.2361\n",
      "Batch 30, Loss: 0.0062\n",
      "Batch 40, Loss: 0.0139\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.1848\n",
      "Batch 70, Loss: 0.0142\n",
      "Batch 80, Loss: 0.3599\n",
      "Batch 90, Loss: 0.0622\n",
      "Epoch: 857 | Train loss: 0.06561 | Test loss: 0.06725 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0114\n",
      "Batch 10, Loss: 0.0141\n",
      "Batch 20, Loss: 0.0550\n",
      "Batch 30, Loss: 0.0070\n",
      "Batch 40, Loss: 0.0581\n",
      "Batch 50, Loss: 0.0117\n",
      "Batch 60, Loss: 0.0012\n",
      "Batch 70, Loss: 0.0570\n",
      "Batch 80, Loss: 0.0583\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 858 | Train loss: 0.07255 | Test loss: 0.86856 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0946\n",
      "Batch 10, Loss: 0.0274\n",
      "Batch 20, Loss: 0.1220\n",
      "Batch 30, Loss: 0.0161\n",
      "Batch 40, Loss: 0.2905\n",
      "Batch 50, Loss: 0.0139\n",
      "Batch 60, Loss: 0.0102\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0202\n",
      "Batch 90, Loss: 0.0121\n",
      "Epoch: 859 | Train loss: 0.05717 | Test loss: 0.04871 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0041\n",
      "Batch 20, Loss: 0.0292\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0337\n",
      "Batch 50, Loss: 0.0096\n",
      "Batch 60, Loss: 0.0047\n",
      "Batch 70, Loss: 0.0103\n",
      "Batch 80, Loss: 0.0098\n",
      "Batch 90, Loss: 0.0025\n",
      "Epoch: 860 | Train loss: 0.04820 | Test loss: 0.07779 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0362\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0163\n",
      "Batch 30, Loss: 0.0094\n",
      "Batch 40, Loss: 0.0145\n",
      "Batch 50, Loss: 0.0022\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0312\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0024\n",
      "Epoch: 861 | Train loss: 0.02499 | Test loss: 0.05191 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0679\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0820\n",
      "Batch 30, Loss: 0.0238\n",
      "Batch 40, Loss: 0.0196\n",
      "Batch 50, Loss: 0.0124\n",
      "Batch 60, Loss: 0.0687\n",
      "Batch 70, Loss: 0.0673\n",
      "Batch 80, Loss: 0.0041\n",
      "Batch 90, Loss: 0.0275\n",
      "Epoch: 862 | Train loss: 0.04206 | Test loss: 0.04500 | Test accuracy: 0.57474\n",
      "Batch 0, Loss: 0.0023\n",
      "Batch 10, Loss: 0.0013\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0059\n",
      "Batch 40, Loss: 0.0414\n",
      "Batch 50, Loss: 0.0095\n",
      "Batch 60, Loss: 0.0430\n",
      "Batch 70, Loss: 0.0054\n",
      "Batch 80, Loss: 0.0061\n",
      "Batch 90, Loss: 0.0105\n",
      "Epoch: 863 | Train loss: 0.03009 | Test loss: 0.37192 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0008\n",
      "Batch 10, Loss: 0.0240\n",
      "Batch 20, Loss: 0.0009\n",
      "Batch 30, Loss: 0.0198\n",
      "Batch 40, Loss: 0.0082\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 60, Loss: 0.0119\n",
      "Batch 70, Loss: 0.0198\n",
      "Batch 80, Loss: 0.0152\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 864 | Train loss: 0.03957 | Test loss: 0.05246 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0103\n",
      "Batch 10, Loss: 0.0020\n",
      "Batch 20, Loss: 0.1201\n",
      "Batch 30, Loss: 0.0367\n",
      "Batch 40, Loss: 0.0484\n",
      "Batch 50, Loss: 0.0099\n",
      "Batch 60, Loss: 0.1156\n",
      "Batch 70, Loss: 0.0195\n",
      "Batch 80, Loss: 0.0597\n",
      "Batch 90, Loss: 0.2467\n",
      "Epoch: 865 | Train loss: 0.04195 | Test loss: 0.06963 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0545\n",
      "Batch 10, Loss: 0.2717\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.1565\n",
      "Batch 40, Loss: 0.0659\n",
      "Batch 50, Loss: 0.0060\n",
      "Batch 60, Loss: 0.0513\n",
      "Batch 70, Loss: 0.0153\n",
      "Batch 80, Loss: 0.0031\n",
      "Batch 90, Loss: 0.0005\n",
      "Epoch: 866 | Train loss: 0.04327 | Test loss: 0.06816 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0020\n",
      "Batch 10, Loss: 0.0251\n",
      "Batch 20, Loss: 0.0012\n",
      "Batch 30, Loss: 0.0282\n",
      "Batch 40, Loss: 0.2547\n",
      "Batch 50, Loss: 0.0122\n",
      "Batch 60, Loss: 0.0235\n",
      "Batch 70, Loss: 0.0000\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0735\n",
      "Epoch: 867 | Train loss: 0.03278 | Test loss: 0.04497 | Test accuracy: 0.49485\n",
      "Batch 0, Loss: 0.0497\n",
      "Batch 10, Loss: 0.0115\n",
      "Batch 20, Loss: 0.0092\n",
      "Batch 30, Loss: 0.1032\n",
      "Batch 40, Loss: 0.2194\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.0287\n",
      "Batch 70, Loss: 0.0053\n",
      "Batch 80, Loss: 0.0040\n",
      "Batch 90, Loss: 0.0073\n",
      "Epoch: 868 | Train loss: 0.03652 | Test loss: 0.59355 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0013\n",
      "Batch 10, Loss: 0.1061\n",
      "Batch 20, Loss: 0.0124\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.0244\n",
      "Batch 50, Loss: 0.0173\n",
      "Batch 60, Loss: 0.0007\n",
      "Batch 70, Loss: 0.0621\n",
      "Batch 80, Loss: 0.0053\n",
      "Batch 90, Loss: 0.0027\n",
      "Epoch: 869 | Train loss: 0.04461 | Test loss: 0.23530 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0507\n",
      "Batch 10, Loss: 0.0066\n",
      "Batch 20, Loss: 0.0482\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0069\n",
      "Batch 50, Loss: 0.0011\n",
      "Batch 60, Loss: 0.0065\n",
      "Batch 70, Loss: 0.0163\n",
      "Batch 80, Loss: 0.0016\n",
      "Batch 90, Loss: 0.0021\n",
      "Epoch: 870 | Train loss: 0.02565 | Test loss: 0.09423 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0038\n",
      "Batch 10, Loss: 0.0227\n",
      "Batch 20, Loss: 0.0142\n",
      "Batch 30, Loss: 0.4368\n",
      "Batch 40, Loss: 0.0184\n",
      "Batch 50, Loss: 0.0337\n",
      "Batch 60, Loss: 0.0126\n",
      "Batch 70, Loss: 0.1374\n",
      "Batch 80, Loss: 0.0299\n",
      "Batch 90, Loss: 1.2032\n",
      "Epoch: 871 | Train loss: 0.09039 | Test loss: 0.14032 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0435\n",
      "Batch 10, Loss: 0.0225\n",
      "Batch 20, Loss: 0.0479\n",
      "Batch 30, Loss: 0.0188\n",
      "Batch 40, Loss: 0.0107\n",
      "Batch 50, Loss: 0.0142\n",
      "Batch 60, Loss: 0.0435\n",
      "Batch 70, Loss: 0.0392\n",
      "Batch 80, Loss: 0.0239\n",
      "Batch 90, Loss: 0.0602\n",
      "Epoch: 872 | Train loss: 0.05496 | Test loss: 0.21691 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0385\n",
      "Batch 10, Loss: 0.0024\n",
      "Batch 20, Loss: 0.0507\n",
      "Batch 30, Loss: 0.0073\n",
      "Batch 40, Loss: 0.0473\n",
      "Batch 50, Loss: 0.0113\n",
      "Batch 60, Loss: 0.0316\n",
      "Batch 70, Loss: 0.0078\n",
      "Batch 80, Loss: 0.0028\n",
      "Batch 90, Loss: 0.0251\n",
      "Epoch: 873 | Train loss: 0.03361 | Test loss: 0.06821 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0298\n",
      "Batch 10, Loss: 0.0360\n",
      "Batch 20, Loss: 0.0027\n",
      "Batch 30, Loss: 0.0132\n",
      "Batch 40, Loss: 0.5277\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0351\n",
      "Batch 70, Loss: 0.0137\n",
      "Batch 80, Loss: 0.0236\n",
      "Batch 90, Loss: 0.1298\n",
      "Epoch: 874 | Train loss: 0.06090 | Test loss: 0.04307 | Test accuracy: 0.49485\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.0105\n",
      "Batch 20, Loss: 0.1390\n",
      "Batch 30, Loss: 0.0064\n",
      "Batch 40, Loss: 0.0300\n",
      "Batch 50, Loss: 0.0077\n",
      "Batch 60, Loss: 0.0154\n",
      "Batch 70, Loss: 0.0105\n",
      "Batch 80, Loss: 0.0007\n",
      "Batch 90, Loss: 0.0012\n",
      "Epoch: 875 | Train loss: 0.06710 | Test loss: 0.46097 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0158\n",
      "Batch 10, Loss: 0.0044\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0033\n",
      "Batch 40, Loss: 0.0093\n",
      "Batch 50, Loss: 0.0294\n",
      "Batch 60, Loss: 0.0916\n",
      "Batch 70, Loss: 0.0373\n",
      "Batch 80, Loss: 0.0051\n",
      "Batch 90, Loss: 0.0082\n",
      "Epoch: 876 | Train loss: 0.05018 | Test loss: 0.16594 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0038\n",
      "Batch 10, Loss: 0.0084\n",
      "Batch 20, Loss: 0.0022\n",
      "Batch 30, Loss: 0.0004\n",
      "Batch 40, Loss: 0.0572\n",
      "Batch 50, Loss: 0.0138\n",
      "Batch 60, Loss: 0.0043\n",
      "Batch 70, Loss: 0.1939\n",
      "Batch 80, Loss: 0.1035\n",
      "Batch 90, Loss: 0.0217\n",
      "Epoch: 877 | Train loss: 0.05010 | Test loss: 0.04434 | Test accuracy: 0.53866\n",
      "Batch 0, Loss: 0.0179\n",
      "Batch 10, Loss: 0.0046\n",
      "Batch 20, Loss: 0.0701\n",
      "Batch 30, Loss: 0.1010\n",
      "Batch 40, Loss: 0.0023\n",
      "Batch 50, Loss: 0.2729\n",
      "Batch 60, Loss: 0.0191\n",
      "Batch 70, Loss: 0.0272\n",
      "Batch 80, Loss: 0.0017\n",
      "Batch 90, Loss: 0.0077\n",
      "Epoch: 878 | Train loss: 0.03220 | Test loss: 0.44712 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0210\n",
      "Batch 10, Loss: 0.0041\n",
      "Batch 20, Loss: 0.1464\n",
      "Batch 30, Loss: 0.0319\n",
      "Batch 40, Loss: 0.0095\n",
      "Batch 50, Loss: 0.0004\n",
      "Batch 60, Loss: 0.0705\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0337\n",
      "Batch 90, Loss: 0.0145\n",
      "Epoch: 879 | Train loss: 0.05968 | Test loss: 0.05500 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0165\n",
      "Batch 10, Loss: 0.0136\n",
      "Batch 20, Loss: 0.0138\n",
      "Batch 30, Loss: 0.0053\n",
      "Batch 40, Loss: 0.0177\n",
      "Batch 50, Loss: 0.0831\n",
      "Batch 60, Loss: 0.1236\n",
      "Batch 70, Loss: 0.0102\n",
      "Batch 80, Loss: 0.0311\n",
      "Batch 90, Loss: 0.2541\n",
      "Epoch: 880 | Train loss: 0.05146 | Test loss: 0.07895 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0274\n",
      "Batch 10, Loss: 0.0015\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.0016\n",
      "Batch 40, Loss: 0.0101\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0030\n",
      "Batch 70, Loss: 0.0009\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.0126\n",
      "Epoch: 881 | Train loss: 0.05132 | Test loss: 0.79325 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0108\n",
      "Batch 10, Loss: 0.0055\n",
      "Batch 20, Loss: 0.0938\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.0009\n",
      "Batch 50, Loss: 0.0667\n",
      "Batch 60, Loss: 0.0030\n",
      "Batch 70, Loss: 0.0045\n",
      "Batch 80, Loss: 0.0056\n",
      "Batch 90, Loss: 0.2651\n",
      "Epoch: 882 | Train loss: 0.05521 | Test loss: 0.07544 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.3573\n",
      "Batch 10, Loss: 0.0008\n",
      "Batch 20, Loss: 0.1361\n",
      "Batch 30, Loss: 0.0218\n",
      "Batch 40, Loss: 0.0089\n",
      "Batch 50, Loss: 0.0119\n",
      "Batch 60, Loss: 0.0353\n",
      "Batch 70, Loss: 0.0058\n",
      "Batch 80, Loss: 0.0107\n",
      "Batch 90, Loss: 0.2026\n",
      "Epoch: 883 | Train loss: 0.03202 | Test loss: 0.17097 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0223\n",
      "Batch 10, Loss: 0.0080\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.0024\n",
      "Batch 40, Loss: 0.0170\n",
      "Batch 50, Loss: 0.0279\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.1444\n",
      "Batch 80, Loss: 0.0604\n",
      "Batch 90, Loss: 0.0102\n",
      "Epoch: 884 | Train loss: 0.03074 | Test loss: 0.04507 | Test accuracy: 0.49742\n",
      "Batch 0, Loss: 0.2201\n",
      "Batch 10, Loss: 0.0046\n",
      "Batch 20, Loss: 0.0926\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.0127\n",
      "Batch 60, Loss: 0.1581\n",
      "Batch 70, Loss: 0.0075\n",
      "Batch 80, Loss: 0.0356\n",
      "Batch 90, Loss: 0.1253\n",
      "Epoch: 885 | Train loss: 0.03811 | Test loss: 0.04857 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0036\n",
      "Batch 10, Loss: 0.0492\n",
      "Batch 20, Loss: 0.0193\n",
      "Batch 30, Loss: 0.0006\n",
      "Batch 40, Loss: 0.0019\n",
      "Batch 50, Loss: 0.0189\n",
      "Batch 60, Loss: 0.0418\n",
      "Batch 70, Loss: 0.5662\n",
      "Batch 80, Loss: 0.0082\n",
      "Batch 90, Loss: 0.0115\n",
      "Epoch: 886 | Train loss: 0.07025 | Test loss: 0.04419 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0161\n",
      "Batch 10, Loss: 0.4833\n",
      "Batch 20, Loss: 0.0173\n",
      "Batch 30, Loss: 0.0068\n",
      "Batch 40, Loss: 0.0647\n",
      "Batch 50, Loss: 0.0503\n",
      "Batch 60, Loss: 0.2578\n",
      "Batch 70, Loss: 0.1675\n",
      "Batch 80, Loss: 0.0647\n",
      "Batch 90, Loss: 0.0039\n",
      "Epoch: 887 | Train loss: 0.06894 | Test loss: 0.06995 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0084\n",
      "Batch 10, Loss: 0.1737\n",
      "Batch 20, Loss: 0.0185\n",
      "Batch 30, Loss: 0.0058\n",
      "Batch 40, Loss: 0.0163\n",
      "Batch 50, Loss: 0.2660\n",
      "Batch 60, Loss: 0.0676\n",
      "Batch 70, Loss: 0.0328\n",
      "Batch 80, Loss: 0.0293\n",
      "Batch 90, Loss: 0.0138\n",
      "Epoch: 888 | Train loss: 0.05849 | Test loss: 0.23502 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0039\n",
      "Batch 10, Loss: 0.0151\n",
      "Batch 20, Loss: 0.0038\n",
      "Batch 30, Loss: 0.0162\n",
      "Batch 40, Loss: 0.0890\n",
      "Batch 50, Loss: 0.0075\n",
      "Batch 60, Loss: 0.0709\n",
      "Batch 70, Loss: 0.0573\n",
      "Batch 80, Loss: 0.0566\n",
      "Batch 90, Loss: 0.0081\n",
      "Epoch: 889 | Train loss: 0.06610 | Test loss: 0.86945 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1035\n",
      "Batch 10, Loss: 0.0152\n",
      "Batch 20, Loss: 0.0208\n",
      "Batch 30, Loss: 0.0449\n",
      "Batch 40, Loss: 0.0159\n",
      "Batch 50, Loss: 0.0086\n",
      "Batch 60, Loss: 0.0251\n",
      "Batch 70, Loss: 0.0306\n",
      "Batch 80, Loss: 0.4691\n",
      "Batch 90, Loss: 0.2282\n",
      "Epoch: 890 | Train loss: 0.04211 | Test loss: 0.16873 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0102\n",
      "Batch 10, Loss: 0.0036\n",
      "Batch 20, Loss: 0.0289\n",
      "Batch 30, Loss: 0.0014\n",
      "Batch 40, Loss: 0.0043\n",
      "Batch 50, Loss: 0.0902\n",
      "Batch 60, Loss: 0.0433\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0035\n",
      "Batch 90, Loss: 0.4245\n",
      "Epoch: 891 | Train loss: 0.04175 | Test loss: 0.04880 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0089\n",
      "Batch 10, Loss: 0.0024\n",
      "Batch 20, Loss: 0.1703\n",
      "Batch 30, Loss: 0.1046\n",
      "Batch 40, Loss: 0.0028\n",
      "Batch 50, Loss: 0.0019\n",
      "Batch 60, Loss: 0.0206\n",
      "Batch 70, Loss: 0.0281\n",
      "Batch 80, Loss: 0.0886\n",
      "Batch 90, Loss: 0.1625\n",
      "Epoch: 892 | Train loss: 0.03207 | Test loss: 0.05595 | Test accuracy: 0.53351\n",
      "Batch 0, Loss: 0.0041\n",
      "Batch 10, Loss: 0.0936\n",
      "Batch 20, Loss: 0.0136\n",
      "Batch 30, Loss: 0.0372\n",
      "Batch 40, Loss: 0.0055\n",
      "Batch 50, Loss: 0.0236\n",
      "Batch 60, Loss: 0.0020\n",
      "Batch 70, Loss: 0.2521\n",
      "Batch 80, Loss: 0.0201\n",
      "Batch 90, Loss: 0.0019\n",
      "Epoch: 893 | Train loss: 0.05531 | Test loss: 0.11615 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0569\n",
      "Batch 10, Loss: 0.0218\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.0441\n",
      "Batch 40, Loss: 0.0123\n",
      "Batch 50, Loss: 0.1713\n",
      "Batch 60, Loss: 0.0242\n",
      "Batch 70, Loss: 0.0008\n",
      "Batch 80, Loss: 0.0071\n",
      "Batch 90, Loss: 0.0104\n",
      "Epoch: 894 | Train loss: 0.05610 | Test loss: 1.41545 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0363\n",
      "Batch 10, Loss: 0.0032\n",
      "Batch 20, Loss: 0.0398\n",
      "Batch 30, Loss: 0.0055\n",
      "Batch 40, Loss: 0.0370\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0866\n",
      "Batch 80, Loss: 0.1700\n",
      "Batch 90, Loss: 0.0159\n",
      "Epoch: 895 | Train loss: 0.03879 | Test loss: 0.22647 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.2776\n",
      "Batch 20, Loss: 0.0229\n",
      "Batch 30, Loss: 0.0034\n",
      "Batch 40, Loss: 0.0012\n",
      "Batch 50, Loss: 0.0230\n",
      "Batch 60, Loss: 0.0004\n",
      "Batch 70, Loss: 0.0088\n",
      "Batch 80, Loss: 0.0001\n",
      "Batch 90, Loss: 0.0022\n",
      "Epoch: 896 | Train loss: 0.02655 | Test loss: 0.04774 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0435\n",
      "Batch 10, Loss: 0.0006\n",
      "Batch 20, Loss: 0.0024\n",
      "Batch 30, Loss: 0.0165\n",
      "Batch 40, Loss: 0.0035\n",
      "Batch 50, Loss: 0.0062\n",
      "Batch 60, Loss: 0.0029\n",
      "Batch 70, Loss: 0.0092\n",
      "Batch 80, Loss: 0.0059\n",
      "Batch 90, Loss: 0.0224\n",
      "Epoch: 897 | Train loss: 0.04092 | Test loss: 0.71631 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0152\n",
      "Batch 10, Loss: 0.0008\n",
      "Batch 20, Loss: 0.0304\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.2780\n",
      "Batch 50, Loss: 0.0175\n",
      "Batch 60, Loss: 0.0146\n",
      "Batch 70, Loss: 0.0033\n",
      "Batch 80, Loss: 0.0117\n",
      "Batch 90, Loss: 0.0591\n",
      "Epoch: 898 | Train loss: 0.05545 | Test loss: 2.16684 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0161\n",
      "Batch 10, Loss: 0.1773\n",
      "Batch 20, Loss: 0.0333\n",
      "Batch 30, Loss: 0.0053\n",
      "Batch 40, Loss: 0.0969\n",
      "Batch 50, Loss: 0.0052\n",
      "Batch 60, Loss: 0.0073\n",
      "Batch 70, Loss: 0.0319\n",
      "Batch 80, Loss: 0.0919\n",
      "Batch 90, Loss: 0.0009\n",
      "Epoch: 899 | Train loss: 0.02872 | Test loss: 0.04505 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.0107\n",
      "Batch 10, Loss: 0.0570\n",
      "Batch 20, Loss: 0.0334\n",
      "Batch 30, Loss: 0.0369\n",
      "Batch 40, Loss: 0.0242\n",
      "Batch 50, Loss: 0.0032\n",
      "Batch 60, Loss: 0.2066\n",
      "Batch 70, Loss: 0.0103\n",
      "Batch 80, Loss: 0.0203\n",
      "Batch 90, Loss: 0.1299\n",
      "Epoch: 900 | Train loss: 0.06261 | Test loss: 0.04618 | Test accuracy: 0.47165\n",
      "Batch 0, Loss: 0.3229\n",
      "Batch 10, Loss: 0.0309\n",
      "Batch 20, Loss: 0.0029\n",
      "Batch 30, Loss: 0.0007\n",
      "Batch 40, Loss: 0.0066\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0219\n",
      "Batch 70, Loss: 0.0307\n",
      "Batch 80, Loss: 0.0023\n",
      "Batch 90, Loss: 0.0150\n",
      "Epoch: 901 | Train loss: 0.03135 | Test loss: 0.12425 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0224\n",
      "Batch 10, Loss: 0.0347\n",
      "Batch 20, Loss: 0.0251\n",
      "Batch 30, Loss: 0.0728\n",
      "Batch 40, Loss: 0.0001\n",
      "Batch 50, Loss: 0.0349\n",
      "Batch 60, Loss: 0.2558\n",
      "Batch 70, Loss: 0.0290\n",
      "Batch 80, Loss: 0.0320\n",
      "Batch 90, Loss: 0.0191\n",
      "Epoch: 902 | Train loss: 0.03792 | Test loss: 0.06316 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0006\n",
      "Batch 10, Loss: 0.0012\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.0913\n",
      "Batch 40, Loss: 0.0134\n",
      "Batch 50, Loss: 0.0012\n",
      "Batch 60, Loss: 0.1324\n",
      "Batch 70, Loss: 0.1797\n",
      "Batch 80, Loss: 0.0167\n",
      "Batch 90, Loss: 0.0151\n",
      "Epoch: 903 | Train loss: 0.02825 | Test loss: 1.95967 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0450\n",
      "Batch 10, Loss: 0.0777\n",
      "Batch 20, Loss: 0.0797\n",
      "Batch 30, Loss: 0.0013\n",
      "Batch 40, Loss: 0.0026\n",
      "Batch 50, Loss: 0.0368\n",
      "Batch 60, Loss: 0.0575\n",
      "Batch 70, Loss: 0.0162\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0017\n",
      "Epoch: 904 | Train loss: 0.02858 | Test loss: 0.06489 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0082\n",
      "Batch 10, Loss: 0.0247\n",
      "Batch 20, Loss: 0.0039\n",
      "Batch 30, Loss: 0.0068\n",
      "Batch 40, Loss: 0.0111\n",
      "Batch 50, Loss: 0.0492\n",
      "Batch 60, Loss: 0.0111\n",
      "Batch 70, Loss: 0.0079\n",
      "Batch 80, Loss: 0.0035\n",
      "Batch 90, Loss: 0.0536\n",
      "Epoch: 905 | Train loss: 0.03816 | Test loss: 0.07606 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.3671\n",
      "Batch 10, Loss: 0.0942\n",
      "Batch 20, Loss: 0.0168\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0090\n",
      "Batch 50, Loss: 0.0979\n",
      "Batch 60, Loss: 0.0219\n",
      "Batch 70, Loss: 0.0306\n",
      "Batch 80, Loss: 0.0048\n",
      "Batch 90, Loss: 0.0119\n",
      "Epoch: 906 | Train loss: 0.03539 | Test loss: 0.25795 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0162\n",
      "Batch 10, Loss: 0.0990\n",
      "Batch 20, Loss: 0.0314\n",
      "Batch 30, Loss: 0.0031\n",
      "Batch 40, Loss: 0.0154\n",
      "Batch 50, Loss: 0.0026\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0296\n",
      "Batch 80, Loss: 0.0003\n",
      "Batch 90, Loss: 0.0443\n",
      "Epoch: 907 | Train loss: 0.06597 | Test loss: 0.55431 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0256\n",
      "Batch 20, Loss: 0.0108\n",
      "Batch 30, Loss: 0.0044\n",
      "Batch 40, Loss: 0.1085\n",
      "Batch 50, Loss: 0.0081\n",
      "Batch 60, Loss: 0.4488\n",
      "Batch 70, Loss: 0.0056\n",
      "Batch 80, Loss: 0.0116\n",
      "Batch 90, Loss: 0.0032\n",
      "Epoch: 908 | Train loss: 0.07395 | Test loss: 0.05266 | Test accuracy: 0.49227\n",
      "Batch 0, Loss: 0.0536\n",
      "Batch 10, Loss: 0.1041\n",
      "Batch 20, Loss: 0.1017\n",
      "Batch 30, Loss: 0.1150\n",
      "Batch 40, Loss: 0.0329\n",
      "Batch 50, Loss: 0.0004\n",
      "Batch 60, Loss: 0.1087\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.0043\n",
      "Batch 90, Loss: 0.0055\n",
      "Epoch: 909 | Train loss: 0.04150 | Test loss: 0.06609 | Test accuracy: 0.47680\n",
      "Batch 0, Loss: 0.0108\n",
      "Batch 10, Loss: 0.1331\n",
      "Batch 20, Loss: 0.0339\n",
      "Batch 30, Loss: 0.0224\n",
      "Batch 40, Loss: 0.1864\n",
      "Batch 50, Loss: 0.0134\n",
      "Batch 60, Loss: 0.0308\n",
      "Batch 70, Loss: 0.0137\n",
      "Batch 80, Loss: 0.0025\n",
      "Batch 90, Loss: 0.0230\n",
      "Epoch: 910 | Train loss: 0.04378 | Test loss: 2.24967 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0017\n",
      "Batch 10, Loss: 0.0286\n",
      "Batch 20, Loss: 0.0043\n",
      "Batch 30, Loss: 0.0423\n",
      "Batch 40, Loss: 0.0049\n",
      "Batch 50, Loss: 0.0501\n",
      "Batch 60, Loss: 0.0047\n",
      "Batch 70, Loss: 0.0006\n",
      "Batch 80, Loss: 0.0335\n",
      "Batch 90, Loss: 0.0193\n",
      "Epoch: 911 | Train loss: 0.03696 | Test loss: 0.10968 | Test accuracy: 0.50515\n",
      "Batch 0, Loss: 0.0397\n",
      "Batch 10, Loss: 0.0412\n",
      "Batch 20, Loss: 0.1708\n",
      "Batch 30, Loss: 0.0321\n",
      "Batch 40, Loss: 0.0242\n",
      "Batch 50, Loss: 0.0523\n",
      "Batch 60, Loss: 0.1766\n",
      "Batch 70, Loss: 0.0012\n",
      "Batch 80, Loss: 0.0572\n",
      "Batch 90, Loss: 0.0410\n",
      "Epoch: 912 | Train loss: 0.06106 | Test loss: 0.06694 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0122\n",
      "Batch 10, Loss: 0.0145\n",
      "Batch 20, Loss: 0.0595\n",
      "Batch 30, Loss: 0.0030\n",
      "Batch 40, Loss: 0.0118\n",
      "Batch 50, Loss: 0.1532\n",
      "Batch 60, Loss: 0.5786\n",
      "Batch 70, Loss: 0.0122\n",
      "Batch 80, Loss: 0.0029\n",
      "Batch 90, Loss: 0.0047\n",
      "Epoch: 913 | Train loss: 0.07768 | Test loss: 0.04741 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0255\n",
      "Batch 10, Loss: 0.0009\n",
      "Batch 20, Loss: 0.0227\n",
      "Batch 30, Loss: 0.0067\n",
      "Batch 40, Loss: 0.0738\n",
      "Batch 50, Loss: 0.0261\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0139\n",
      "Batch 90, Loss: 0.0630\n",
      "Epoch: 914 | Train loss: 0.05529 | Test loss: 1.79561 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0211\n",
      "Batch 10, Loss: 0.0092\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.1424\n",
      "Batch 40, Loss: 0.7815\n",
      "Batch 50, Loss: 0.0216\n",
      "Batch 60, Loss: 0.0992\n",
      "Batch 70, Loss: 0.1284\n",
      "Batch 80, Loss: 0.0179\n",
      "Batch 90, Loss: 0.0037\n",
      "Epoch: 915 | Train loss: 0.07744 | Test loss: 0.04438 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0241\n",
      "Batch 10, Loss: 0.0079\n",
      "Batch 20, Loss: 0.0666\n",
      "Batch 30, Loss: 0.0230\n",
      "Batch 40, Loss: 0.0348\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0472\n",
      "Batch 70, Loss: 0.0067\n",
      "Batch 80, Loss: 0.0009\n",
      "Batch 90, Loss: 0.3410\n",
      "Epoch: 916 | Train loss: 0.03408 | Test loss: 0.13160 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0290\n",
      "Batch 10, Loss: 0.0097\n",
      "Batch 20, Loss: 0.0131\n",
      "Batch 30, Loss: 0.0256\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0031\n",
      "Batch 60, Loss: 0.0334\n",
      "Batch 70, Loss: 0.0161\n",
      "Batch 80, Loss: 0.0177\n",
      "Batch 90, Loss: 0.5353\n",
      "Epoch: 917 | Train loss: 0.04083 | Test loss: 1.75244 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0437\n",
      "Batch 10, Loss: 0.0053\n",
      "Batch 20, Loss: 0.0111\n",
      "Batch 30, Loss: 0.0099\n",
      "Batch 40, Loss: 0.0339\n",
      "Batch 50, Loss: 0.0326\n",
      "Batch 60, Loss: 0.0397\n",
      "Batch 70, Loss: 0.0085\n",
      "Batch 80, Loss: 0.0838\n",
      "Batch 90, Loss: 0.0195\n",
      "Epoch: 918 | Train loss: 0.06420 | Test loss: 0.26959 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0021\n",
      "Batch 10, Loss: 0.0120\n",
      "Batch 20, Loss: 0.0030\n",
      "Batch 30, Loss: 0.0627\n",
      "Batch 40, Loss: 0.0233\n",
      "Batch 50, Loss: 0.0403\n",
      "Batch 60, Loss: 0.1053\n",
      "Batch 70, Loss: 0.0208\n",
      "Batch 80, Loss: 0.0044\n",
      "Batch 90, Loss: 0.0150\n",
      "Epoch: 919 | Train loss: 0.04145 | Test loss: 0.06748 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0315\n",
      "Batch 10, Loss: 0.0029\n",
      "Batch 20, Loss: 0.0192\n",
      "Batch 30, Loss: 0.0368\n",
      "Batch 40, Loss: 0.0027\n",
      "Batch 50, Loss: 0.1757\n",
      "Batch 60, Loss: 0.0639\n",
      "Batch 70, Loss: 0.0024\n",
      "Batch 80, Loss: 0.4786\n",
      "Batch 90, Loss: 0.1080\n",
      "Epoch: 920 | Train loss: 0.03214 | Test loss: 0.04274 | Test accuracy: 0.56701\n",
      "Batch 0, Loss: 0.0007\n",
      "Batch 10, Loss: 0.0162\n",
      "Batch 20, Loss: 0.0691\n",
      "Batch 30, Loss: 0.0283\n",
      "Batch 40, Loss: 0.0062\n",
      "Batch 50, Loss: 0.0042\n",
      "Batch 60, Loss: 0.0089\n",
      "Batch 70, Loss: 0.0103\n",
      "Batch 80, Loss: 0.0047\n",
      "Batch 90, Loss: 0.0226\n",
      "Epoch: 921 | Train loss: 0.05711 | Test loss: 0.05453 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0062\n",
      "Batch 20, Loss: 0.5255\n",
      "Batch 30, Loss: 0.0085\n",
      "Batch 40, Loss: 0.0192\n",
      "Batch 50, Loss: 0.0291\n",
      "Batch 60, Loss: 0.0792\n",
      "Batch 70, Loss: 0.0448\n",
      "Batch 80, Loss: 0.0631\n",
      "Batch 90, Loss: 0.0313\n",
      "Epoch: 922 | Train loss: 0.08976 | Test loss: 0.11519 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0215\n",
      "Batch 10, Loss: 0.0752\n",
      "Batch 20, Loss: 0.0097\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.0507\n",
      "Batch 50, Loss: 0.0118\n",
      "Batch 60, Loss: 0.0191\n",
      "Batch 70, Loss: 0.0125\n",
      "Batch 80, Loss: 0.0032\n",
      "Batch 90, Loss: 0.0020\n",
      "Epoch: 923 | Train loss: 0.04223 | Test loss: 0.08971 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0342\n",
      "Batch 10, Loss: 0.0010\n",
      "Batch 20, Loss: 0.0343\n",
      "Batch 30, Loss: 0.0304\n",
      "Batch 40, Loss: 0.0023\n",
      "Batch 50, Loss: 0.0026\n",
      "Batch 60, Loss: 0.0611\n",
      "Batch 70, Loss: 0.0247\n",
      "Batch 80, Loss: 0.0076\n",
      "Batch 90, Loss: 0.0687\n",
      "Epoch: 924 | Train loss: 0.04255 | Test loss: 9.73774 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0187\n",
      "Batch 10, Loss: 0.0058\n",
      "Batch 20, Loss: 0.0081\n",
      "Batch 30, Loss: 0.0095\n",
      "Batch 40, Loss: 0.0077\n",
      "Batch 50, Loss: 0.0091\n",
      "Batch 60, Loss: 0.0016\n",
      "Batch 70, Loss: 0.0129\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0091\n",
      "Epoch: 925 | Train loss: 0.03605 | Test loss: 0.04939 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0006\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0111\n",
      "Batch 30, Loss: 0.0327\n",
      "Batch 40, Loss: 0.0022\n",
      "Batch 50, Loss: 0.0004\n",
      "Batch 60, Loss: 0.0026\n",
      "Batch 70, Loss: 0.0119\n",
      "Batch 80, Loss: 0.1875\n",
      "Batch 90, Loss: 0.0037\n",
      "Epoch: 926 | Train loss: 0.04357 | Test loss: 0.37416 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0159\n",
      "Batch 10, Loss: 0.1841\n",
      "Batch 20, Loss: 0.1023\n",
      "Batch 30, Loss: 0.0026\n",
      "Batch 40, Loss: 0.1288\n",
      "Batch 50, Loss: 0.0151\n",
      "Batch 60, Loss: 0.0287\n",
      "Batch 70, Loss: 0.2273\n",
      "Batch 80, Loss: 0.0390\n",
      "Batch 90, Loss: 0.0022\n",
      "Epoch: 927 | Train loss: 0.04515 | Test loss: 0.04042 | Test accuracy: 0.67268\n",
      "Batch 0, Loss: 0.0138\n",
      "Batch 10, Loss: 0.0404\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0203\n",
      "Batch 40, Loss: 0.0097\n",
      "Batch 50, Loss: 0.0010\n",
      "Batch 60, Loss: 0.0108\n",
      "Batch 70, Loss: 0.0125\n",
      "Batch 80, Loss: 0.0346\n",
      "Batch 90, Loss: 0.0298\n",
      "Epoch: 928 | Train loss: 0.04423 | Test loss: 0.04688 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0030\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0068\n",
      "Batch 30, Loss: 0.0279\n",
      "Batch 40, Loss: 0.0031\n",
      "Batch 50, Loss: 0.0313\n",
      "Batch 60, Loss: 0.0203\n",
      "Batch 70, Loss: 0.0171\n",
      "Batch 80, Loss: 0.0004\n",
      "Batch 90, Loss: 0.0015\n",
      "Epoch: 929 | Train loss: 0.02801 | Test loss: 0.04341 | Test accuracy: 0.75515\n",
      "Batch 0, Loss: 0.0075\n",
      "Batch 10, Loss: 0.0037\n",
      "Batch 20, Loss: 0.0390\n",
      "Batch 30, Loss: 0.0043\n",
      "Batch 40, Loss: 0.0058\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.1269\n",
      "Batch 80, Loss: 0.0186\n",
      "Batch 90, Loss: 0.0076\n",
      "Epoch: 930 | Train loss: 0.03068 | Test loss: 0.97994 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0010\n",
      "Batch 10, Loss: 0.0422\n",
      "Batch 20, Loss: 0.0121\n",
      "Batch 30, Loss: 0.0823\n",
      "Batch 40, Loss: 0.0135\n",
      "Batch 50, Loss: 0.0215\n",
      "Batch 60, Loss: 0.2471\n",
      "Batch 70, Loss: 0.0086\n",
      "Batch 80, Loss: 0.0012\n",
      "Batch 90, Loss: 0.0252\n",
      "Epoch: 931 | Train loss: 0.05517 | Test loss: 0.10460 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0562\n",
      "Batch 20, Loss: 0.0045\n",
      "Batch 30, Loss: 0.0077\n",
      "Batch 40, Loss: 0.0936\n",
      "Batch 50, Loss: 0.0033\n",
      "Batch 60, Loss: 0.0129\n",
      "Batch 70, Loss: 0.0128\n",
      "Batch 80, Loss: 0.0620\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 932 | Train loss: 0.04585 | Test loss: 0.39942 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.4532\n",
      "Batch 10, Loss: 0.0008\n",
      "Batch 20, Loss: 0.0012\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0250\n",
      "Batch 50, Loss: 0.0020\n",
      "Batch 60, Loss: 0.0005\n",
      "Batch 70, Loss: 0.0022\n",
      "Batch 80, Loss: 0.0038\n",
      "Batch 90, Loss: 0.0109\n",
      "Epoch: 933 | Train loss: 0.05652 | Test loss: 0.06678 | Test accuracy: 0.47423\n",
      "Batch 0, Loss: 0.0146\n",
      "Batch 10, Loss: 0.0116\n",
      "Batch 20, Loss: 0.0103\n",
      "Batch 30, Loss: 0.0075\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0027\n",
      "Batch 60, Loss: 0.0244\n",
      "Batch 70, Loss: 0.0595\n",
      "Batch 80, Loss: 0.0408\n",
      "Batch 90, Loss: 0.0076\n",
      "Epoch: 934 | Train loss: 0.02619 | Test loss: 0.21111 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0084\n",
      "Batch 10, Loss: 0.0017\n",
      "Batch 20, Loss: 0.0061\n",
      "Batch 30, Loss: 0.3236\n",
      "Batch 40, Loss: 0.0306\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0035\n",
      "Batch 70, Loss: 0.0146\n",
      "Batch 80, Loss: 0.0139\n",
      "Batch 90, Loss: 0.0082\n",
      "Epoch: 935 | Train loss: 0.03879 | Test loss: 0.04496 | Test accuracy: 0.56959\n",
      "Batch 0, Loss: 0.0022\n",
      "Batch 10, Loss: 0.0041\n",
      "Batch 20, Loss: 0.0056\n",
      "Batch 30, Loss: 0.5420\n",
      "Batch 40, Loss: 0.0811\n",
      "Batch 50, Loss: 0.0188\n",
      "Batch 60, Loss: 0.0487\n",
      "Batch 70, Loss: 0.0404\n",
      "Batch 80, Loss: 0.0052\n",
      "Batch 90, Loss: 0.0117\n",
      "Epoch: 936 | Train loss: 0.03412 | Test loss: 0.08820 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0063\n",
      "Batch 10, Loss: 0.0007\n",
      "Batch 20, Loss: 0.0053\n",
      "Batch 30, Loss: 0.0051\n",
      "Batch 40, Loss: 0.0249\n",
      "Batch 50, Loss: 0.0144\n",
      "Batch 60, Loss: 0.0277\n",
      "Batch 70, Loss: 0.0099\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0460\n",
      "Epoch: 937 | Train loss: 0.02079 | Test loss: 0.10108 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0090\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0088\n",
      "Batch 30, Loss: 0.7948\n",
      "Batch 40, Loss: 0.1483\n",
      "Batch 50, Loss: 0.0277\n",
      "Batch 60, Loss: 0.0227\n",
      "Batch 70, Loss: 0.0676\n",
      "Batch 80, Loss: 0.0092\n",
      "Batch 90, Loss: 0.0073\n",
      "Epoch: 938 | Train loss: 0.08296 | Test loss: 0.05807 | Test accuracy: 0.59278\n",
      "Batch 0, Loss: 0.3218\n",
      "Batch 10, Loss: 0.0076\n",
      "Batch 20, Loss: 0.0034\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0024\n",
      "Batch 50, Loss: 0.0312\n",
      "Batch 60, Loss: 0.0301\n",
      "Batch 70, Loss: 0.1391\n",
      "Batch 80, Loss: 0.0067\n",
      "Batch 90, Loss: 0.0024\n",
      "Epoch: 939 | Train loss: 0.06161 | Test loss: 0.06888 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0054\n",
      "Batch 10, Loss: 0.0094\n",
      "Batch 20, Loss: 0.0960\n",
      "Batch 30, Loss: 0.0051\n",
      "Batch 40, Loss: 0.0136\n",
      "Batch 50, Loss: 0.0059\n",
      "Batch 60, Loss: 0.0007\n",
      "Batch 70, Loss: 0.0899\n",
      "Batch 80, Loss: 0.0037\n",
      "Batch 90, Loss: 0.2120\n",
      "Epoch: 940 | Train loss: 0.04408 | Test loss: 0.08449 | Test accuracy: 0.52062\n",
      "Batch 0, Loss: 0.0002\n",
      "Batch 10, Loss: 0.0012\n",
      "Batch 20, Loss: 0.0504\n",
      "Batch 30, Loss: 0.0032\n",
      "Batch 40, Loss: 0.0061\n",
      "Batch 50, Loss: 0.0263\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.0181\n",
      "Batch 80, Loss: 0.0881\n",
      "Batch 90, Loss: 0.0063\n",
      "Epoch: 941 | Train loss: 0.04425 | Test loss: 0.10222 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0029\n",
      "Batch 10, Loss: 0.0083\n",
      "Batch 20, Loss: 0.0150\n",
      "Batch 30, Loss: 0.0785\n",
      "Batch 40, Loss: 0.0750\n",
      "Batch 50, Loss: 0.0019\n",
      "Batch 60, Loss: 0.0166\n",
      "Batch 70, Loss: 0.0357\n",
      "Batch 80, Loss: 0.1993\n",
      "Batch 90, Loss: 0.0045\n",
      "Epoch: 942 | Train loss: 0.05148 | Test loss: 0.06724 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0073\n",
      "Batch 10, Loss: 0.0167\n",
      "Batch 20, Loss: 0.5180\n",
      "Batch 30, Loss: 0.0024\n",
      "Batch 40, Loss: 0.0157\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.1824\n",
      "Batch 70, Loss: 0.1149\n",
      "Batch 80, Loss: 0.0018\n",
      "Batch 90, Loss: 0.4274\n",
      "Epoch: 943 | Train loss: 0.08059 | Test loss: 0.05860 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2599\n",
      "Batch 10, Loss: 0.0262\n",
      "Batch 20, Loss: 0.0414\n",
      "Batch 30, Loss: 0.0089\n",
      "Batch 40, Loss: 0.0045\n",
      "Batch 50, Loss: 0.0046\n",
      "Batch 60, Loss: 0.2460\n",
      "Batch 70, Loss: 0.0443\n",
      "Batch 80, Loss: 0.0015\n",
      "Batch 90, Loss: 0.0126\n",
      "Epoch: 944 | Train loss: 0.05139 | Test loss: 0.93571 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0273\n",
      "Batch 10, Loss: 0.0236\n",
      "Batch 20, Loss: 0.0480\n",
      "Batch 30, Loss: 0.0156\n",
      "Batch 40, Loss: 0.0048\n",
      "Batch 50, Loss: 0.0207\n",
      "Batch 60, Loss: 0.0574\n",
      "Batch 70, Loss: 0.0010\n",
      "Batch 80, Loss: 0.0005\n",
      "Batch 90, Loss: 0.0028\n",
      "Epoch: 945 | Train loss: 0.03793 | Test loss: 0.22490 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0455\n",
      "Batch 10, Loss: 0.0061\n",
      "Batch 20, Loss: 0.0197\n",
      "Batch 30, Loss: 0.0161\n",
      "Batch 40, Loss: 0.0006\n",
      "Batch 50, Loss: 0.0138\n",
      "Batch 60, Loss: 0.0853\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.3910\n",
      "Batch 90, Loss: 0.7134\n",
      "Epoch: 946 | Train loss: 0.06230 | Test loss: 0.31973 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0177\n",
      "Batch 10, Loss: 0.0938\n",
      "Batch 20, Loss: 0.2373\n",
      "Batch 30, Loss: 0.0537\n",
      "Batch 40, Loss: 0.0047\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0056\n",
      "Batch 70, Loss: 0.0089\n",
      "Batch 80, Loss: 0.0471\n",
      "Batch 90, Loss: 0.1645\n",
      "Epoch: 947 | Train loss: 0.07836 | Test loss: 0.78866 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.1174\n",
      "Batch 10, Loss: 0.1552\n",
      "Batch 20, Loss: 0.0067\n",
      "Batch 30, Loss: 0.0060\n",
      "Batch 40, Loss: 0.1479\n",
      "Batch 50, Loss: 0.0360\n",
      "Batch 60, Loss: 0.0481\n",
      "Batch 70, Loss: 0.0087\n",
      "Batch 80, Loss: 0.0132\n",
      "Batch 90, Loss: 0.0031\n",
      "Epoch: 948 | Train loss: 0.06256 | Test loss: 0.04882 | Test accuracy: 0.48969\n",
      "Batch 0, Loss: 0.0203\n",
      "Batch 10, Loss: 0.0030\n",
      "Batch 20, Loss: 0.0028\n",
      "Batch 30, Loss: 0.0023\n",
      "Batch 40, Loss: 0.0568\n",
      "Batch 50, Loss: 0.0051\n",
      "Batch 60, Loss: 0.0010\n",
      "Batch 70, Loss: 0.0032\n",
      "Batch 80, Loss: 0.0025\n",
      "Batch 90, Loss: 0.0140\n",
      "Epoch: 949 | Train loss: 0.02454 | Test loss: 0.05147 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0130\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0500\n",
      "Batch 40, Loss: 0.0538\n",
      "Batch 50, Loss: 0.0067\n",
      "Batch 60, Loss: 0.0015\n",
      "Batch 70, Loss: 0.0098\n",
      "Batch 80, Loss: 0.0477\n",
      "Batch 90, Loss: 0.0056\n",
      "Epoch: 950 | Train loss: 0.02570 | Test loss: 0.45952 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0018\n",
      "Batch 30, Loss: 0.0152\n",
      "Batch 40, Loss: 0.0007\n",
      "Batch 50, Loss: 0.0278\n",
      "Batch 60, Loss: 0.0398\n",
      "Batch 70, Loss: 0.0457\n",
      "Batch 80, Loss: 0.1367\n",
      "Batch 90, Loss: 0.0063\n",
      "Epoch: 951 | Train loss: 0.05146 | Test loss: 0.09727 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0241\n",
      "Batch 10, Loss: 0.0223\n",
      "Batch 20, Loss: 0.0061\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0077\n",
      "Batch 50, Loss: 0.0274\n",
      "Batch 60, Loss: 0.0367\n",
      "Batch 70, Loss: 0.0060\n",
      "Batch 80, Loss: 0.2686\n",
      "Batch 90, Loss: 0.0498\n",
      "Epoch: 952 | Train loss: 0.04275 | Test loss: 0.58021 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0133\n",
      "Batch 10, Loss: 0.0090\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0027\n",
      "Batch 40, Loss: 0.0104\n",
      "Batch 50, Loss: 0.0110\n",
      "Batch 60, Loss: 0.0038\n",
      "Batch 70, Loss: 0.0241\n",
      "Batch 80, Loss: 0.0006\n",
      "Batch 90, Loss: 0.0005\n",
      "Epoch: 953 | Train loss: 0.03423 | Test loss: 0.07221 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.1132\n",
      "Batch 10, Loss: 0.0745\n",
      "Batch 20, Loss: 0.0012\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.0154\n",
      "Batch 60, Loss: 0.1069\n",
      "Batch 70, Loss: 0.0073\n",
      "Batch 80, Loss: 0.0226\n",
      "Batch 90, Loss: 0.0068\n",
      "Epoch: 954 | Train loss: 0.03758 | Test loss: 0.13461 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1711\n",
      "Batch 10, Loss: 0.1808\n",
      "Batch 20, Loss: 0.0414\n",
      "Batch 30, Loss: 0.1992\n",
      "Batch 40, Loss: 0.0055\n",
      "Batch 50, Loss: 0.0170\n",
      "Batch 60, Loss: 0.0362\n",
      "Batch 70, Loss: 0.0305\n",
      "Batch 80, Loss: 0.0119\n",
      "Batch 90, Loss: 0.0033\n",
      "Epoch: 955 | Train loss: 0.05402 | Test loss: 0.04379 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0055\n",
      "Batch 10, Loss: 0.0616\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0238\n",
      "Batch 40, Loss: 0.0076\n",
      "Batch 50, Loss: 0.0028\n",
      "Batch 60, Loss: 0.0704\n",
      "Batch 70, Loss: 0.0012\n",
      "Batch 80, Loss: 0.0133\n",
      "Batch 90, Loss: 0.0154\n",
      "Epoch: 956 | Train loss: 0.03423 | Test loss: 0.05991 | Test accuracy: 0.55928\n",
      "Batch 0, Loss: 0.0789\n",
      "Batch 10, Loss: 0.0418\n",
      "Batch 20, Loss: 0.0159\n",
      "Batch 30, Loss: 0.0006\n",
      "Batch 40, Loss: 0.0543\n",
      "Batch 50, Loss: 0.0229\n",
      "Batch 60, Loss: 0.0121\n",
      "Batch 70, Loss: 0.0408\n",
      "Batch 80, Loss: 0.2074\n",
      "Batch 90, Loss: 0.0337\n",
      "Epoch: 957 | Train loss: 0.04448 | Test loss: 0.04934 | Test accuracy: 0.55155\n",
      "Batch 0, Loss: 0.0452\n",
      "Batch 10, Loss: 0.0131\n",
      "Batch 20, Loss: 0.0163\n",
      "Batch 30, Loss: 0.0495\n",
      "Batch 40, Loss: 0.0110\n",
      "Batch 50, Loss: 0.0063\n",
      "Batch 60, Loss: 0.0221\n",
      "Batch 70, Loss: 0.0052\n",
      "Batch 80, Loss: 0.0127\n",
      "Batch 90, Loss: 0.0210\n",
      "Epoch: 958 | Train loss: 0.03463 | Test loss: 0.56440 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0044\n",
      "Batch 10, Loss: 0.0011\n",
      "Batch 20, Loss: 0.0027\n",
      "Batch 30, Loss: 0.0063\n",
      "Batch 40, Loss: 0.0839\n",
      "Batch 50, Loss: 0.0786\n",
      "Batch 60, Loss: 0.0381\n",
      "Batch 70, Loss: 0.0581\n",
      "Batch 80, Loss: 0.0129\n",
      "Batch 90, Loss: 0.0050\n",
      "Epoch: 959 | Train loss: 0.04109 | Test loss: 0.08071 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0134\n",
      "Batch 10, Loss: 0.0058\n",
      "Batch 20, Loss: 0.4218\n",
      "Batch 30, Loss: 0.0156\n",
      "Batch 40, Loss: 0.0073\n",
      "Batch 50, Loss: 0.0570\n",
      "Batch 60, Loss: 0.1682\n",
      "Batch 70, Loss: 0.0962\n",
      "Batch 80, Loss: 0.0339\n",
      "Batch 90, Loss: 0.0021\n",
      "Epoch: 960 | Train loss: 0.07931 | Test loss: 0.04612 | Test accuracy: 0.54897\n",
      "Batch 0, Loss: 0.0033\n",
      "Batch 10, Loss: 0.0203\n",
      "Batch 20, Loss: 0.0021\n",
      "Batch 30, Loss: 0.0830\n",
      "Batch 40, Loss: 0.0397\n",
      "Batch 50, Loss: 0.0182\n",
      "Batch 60, Loss: 0.1860\n",
      "Batch 70, Loss: 0.0346\n",
      "Batch 80, Loss: 0.0069\n",
      "Batch 90, Loss: 0.0147\n",
      "Epoch: 961 | Train loss: 0.05370 | Test loss: 0.13930 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.1068\n",
      "Batch 10, Loss: 0.0323\n",
      "Batch 20, Loss: 0.0013\n",
      "Batch 30, Loss: 0.0149\n",
      "Batch 40, Loss: 0.0576\n",
      "Batch 50, Loss: 0.0038\n",
      "Batch 60, Loss: 0.1118\n",
      "Batch 70, Loss: 0.0077\n",
      "Batch 80, Loss: 0.0033\n",
      "Batch 90, Loss: 0.0021\n",
      "Epoch: 962 | Train loss: 0.03189 | Test loss: 0.13107 | Test accuracy: 0.47938\n",
      "Batch 0, Loss: 0.0644\n",
      "Batch 10, Loss: 0.0158\n",
      "Batch 20, Loss: 0.0026\n",
      "Batch 30, Loss: 0.0045\n",
      "Batch 40, Loss: 0.0109\n",
      "Batch 50, Loss: 0.2115\n",
      "Batch 60, Loss: 0.0036\n",
      "Batch 70, Loss: 0.0939\n",
      "Batch 80, Loss: 0.0275\n",
      "Batch 90, Loss: 0.0038\n",
      "Epoch: 963 | Train loss: 0.02530 | Test loss: 0.07719 | Test accuracy: 0.53093\n",
      "Batch 0, Loss: 0.0335\n",
      "Batch 10, Loss: 0.0042\n",
      "Batch 20, Loss: 0.0037\n",
      "Batch 30, Loss: 0.0172\n",
      "Batch 40, Loss: 0.0016\n",
      "Batch 50, Loss: 0.0101\n",
      "Batch 60, Loss: 0.0044\n",
      "Batch 70, Loss: 0.0078\n",
      "Batch 80, Loss: 0.1575\n",
      "Batch 90, Loss: 0.0063\n",
      "Epoch: 964 | Train loss: 0.03275 | Test loss: 0.05088 | Test accuracy: 0.54381\n",
      "Batch 0, Loss: 0.0081\n",
      "Batch 10, Loss: 0.0177\n",
      "Batch 20, Loss: 0.0615\n",
      "Batch 30, Loss: 0.1411\n",
      "Batch 40, Loss: 0.0794\n",
      "Batch 50, Loss: 0.0015\n",
      "Batch 60, Loss: 0.0896\n",
      "Batch 70, Loss: 0.0236\n",
      "Batch 80, Loss: 0.0342\n",
      "Batch 90, Loss: 0.0370\n",
      "Epoch: 965 | Train loss: 0.04491 | Test loss: 0.06112 | Test accuracy: 0.52835\n",
      "Batch 0, Loss: 0.0930\n",
      "Batch 10, Loss: 0.0174\n",
      "Batch 20, Loss: 0.0084\n",
      "Batch 30, Loss: 0.0061\n",
      "Batch 40, Loss: 0.0154\n",
      "Batch 50, Loss: 0.0109\n",
      "Batch 60, Loss: 0.0254\n",
      "Batch 70, Loss: 0.0457\n",
      "Batch 80, Loss: 0.0210\n",
      "Batch 90, Loss: 0.0030\n",
      "Epoch: 966 | Train loss: 0.05247 | Test loss: 0.04397 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0814\n",
      "Batch 10, Loss: 0.0026\n",
      "Batch 20, Loss: 0.0171\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0057\n",
      "Batch 50, Loss: 0.0119\n",
      "Batch 60, Loss: 0.0399\n",
      "Batch 70, Loss: 0.0017\n",
      "Batch 80, Loss: 0.0006\n",
      "Batch 90, Loss: 0.0084\n",
      "Epoch: 967 | Train loss: 0.02996 | Test loss: 0.09487 | Test accuracy: 0.53608\n",
      "Batch 0, Loss: 0.0450\n",
      "Batch 10, Loss: 0.1368\n",
      "Batch 20, Loss: 0.0296\n",
      "Batch 30, Loss: 0.1138\n",
      "Batch 40, Loss: 0.0201\n",
      "Batch 50, Loss: 0.0154\n",
      "Batch 60, Loss: 0.1706\n",
      "Batch 70, Loss: 0.1374\n",
      "Batch 80, Loss: 0.1067\n",
      "Batch 90, Loss: 0.0027\n",
      "Epoch: 968 | Train loss: 0.03344 | Test loss: 0.99450 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0009\n",
      "Batch 10, Loss: 0.0025\n",
      "Batch 20, Loss: 0.0147\n",
      "Batch 30, Loss: 0.0075\n",
      "Batch 40, Loss: 0.0010\n",
      "Batch 50, Loss: 0.1058\n",
      "Batch 60, Loss: 0.0041\n",
      "Batch 70, Loss: 0.0037\n",
      "Batch 80, Loss: 0.0049\n",
      "Batch 90, Loss: 0.0035\n",
      "Epoch: 969 | Train loss: 0.04195 | Test loss: 0.11061 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 10, Loss: 0.0278\n",
      "Batch 20, Loss: 0.0009\n",
      "Batch 30, Loss: 0.0057\n",
      "Batch 40, Loss: 0.0358\n",
      "Batch 50, Loss: 0.0186\n",
      "Batch 60, Loss: 0.0115\n",
      "Batch 70, Loss: 0.0018\n",
      "Batch 80, Loss: 0.0385\n",
      "Batch 90, Loss: 0.0079\n",
      "Epoch: 970 | Train loss: 0.05279 | Test loss: 0.14699 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.1152\n",
      "Batch 10, Loss: 0.0064\n",
      "Batch 20, Loss: 0.2090\n",
      "Batch 30, Loss: 0.0257\n",
      "Batch 40, Loss: 0.0306\n",
      "Batch 50, Loss: 0.0005\n",
      "Batch 60, Loss: 0.0211\n",
      "Batch 70, Loss: 0.0043\n",
      "Batch 80, Loss: 0.0108\n",
      "Batch 90, Loss: 0.0180\n",
      "Epoch: 971 | Train loss: 0.06688 | Test loss: 0.09443 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0208\n",
      "Batch 10, Loss: 0.7481\n",
      "Batch 20, Loss: 0.0038\n",
      "Batch 30, Loss: 0.0249\n",
      "Batch 40, Loss: 0.0812\n",
      "Batch 50, Loss: 0.0044\n",
      "Batch 60, Loss: 0.0031\n",
      "Batch 70, Loss: 0.2074\n",
      "Batch 80, Loss: 0.0005\n",
      "Batch 90, Loss: 0.0398\n",
      "Epoch: 972 | Train loss: 0.04039 | Test loss: 0.06512 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0119\n",
      "Batch 10, Loss: 0.0063\n",
      "Batch 20, Loss: 0.0176\n",
      "Batch 30, Loss: 0.0401\n",
      "Batch 40, Loss: 0.0073\n",
      "Batch 50, Loss: 0.1529\n",
      "Batch 60, Loss: 0.0003\n",
      "Batch 70, Loss: 0.0020\n",
      "Batch 80, Loss: 0.0236\n",
      "Batch 90, Loss: 0.7308\n",
      "Epoch: 973 | Train loss: 0.04483 | Test loss: 0.13124 | Test accuracy: 0.45876\n",
      "Batch 0, Loss: 0.0176\n",
      "Batch 10, Loss: 0.0420\n",
      "Batch 20, Loss: 0.0052\n",
      "Batch 30, Loss: 0.0551\n",
      "Batch 40, Loss: 0.0731\n",
      "Batch 50, Loss: 0.1108\n",
      "Batch 60, Loss: 0.0040\n",
      "Batch 70, Loss: 0.0101\n",
      "Batch 80, Loss: 0.0007\n",
      "Batch 90, Loss: 0.0056\n",
      "Epoch: 974 | Train loss: 0.05245 | Test loss: 0.24232 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0343\n",
      "Batch 10, Loss: 0.0050\n",
      "Batch 20, Loss: 0.3997\n",
      "Batch 30, Loss: 0.0318\n",
      "Batch 40, Loss: 0.2284\n",
      "Batch 50, Loss: 0.0074\n",
      "Batch 60, Loss: 0.0291\n",
      "Batch 70, Loss: 0.1623\n",
      "Batch 80, Loss: 0.0754\n",
      "Batch 90, Loss: 0.0340\n",
      "Epoch: 975 | Train loss: 0.06894 | Test loss: 2.11115 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.2529\n",
      "Batch 10, Loss: 0.0194\n",
      "Batch 20, Loss: 0.0226\n",
      "Batch 30, Loss: 0.0173\n",
      "Batch 40, Loss: 0.0032\n",
      "Batch 50, Loss: 0.0051\n",
      "Batch 60, Loss: 0.0008\n",
      "Batch 70, Loss: 0.0034\n",
      "Batch 80, Loss: 0.0099\n",
      "Batch 90, Loss: 0.0319\n",
      "Epoch: 976 | Train loss: 0.03261 | Test loss: 0.05032 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0145\n",
      "Batch 10, Loss: 0.0409\n",
      "Batch 20, Loss: 0.0459\n",
      "Batch 30, Loss: 0.0692\n",
      "Batch 40, Loss: 0.1002\n",
      "Batch 50, Loss: 0.1151\n",
      "Batch 60, Loss: 0.0647\n",
      "Batch 70, Loss: 0.0672\n",
      "Batch 80, Loss: 0.0179\n",
      "Batch 90, Loss: 0.0338\n",
      "Epoch: 977 | Train loss: 0.10640 | Test loss: 0.04842 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0835\n",
      "Batch 10, Loss: 0.0045\n",
      "Batch 20, Loss: 0.0182\n",
      "Batch 30, Loss: 0.0080\n",
      "Batch 40, Loss: 0.0042\n",
      "Batch 50, Loss: 0.0822\n",
      "Batch 60, Loss: 0.0953\n",
      "Batch 70, Loss: 0.0164\n",
      "Batch 80, Loss: 0.1973\n",
      "Batch 90, Loss: 0.0026\n",
      "Epoch: 978 | Train loss: 0.04103 | Test loss: 0.04955 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0349\n",
      "Batch 10, Loss: 0.0014\n",
      "Batch 20, Loss: 0.0003\n",
      "Batch 30, Loss: 0.0043\n",
      "Batch 40, Loss: 0.0362\n",
      "Batch 50, Loss: 0.0041\n",
      "Batch 60, Loss: 0.0498\n",
      "Batch 70, Loss: 0.0088\n",
      "Batch 80, Loss: 0.1108\n",
      "Batch 90, Loss: 0.0043\n",
      "Epoch: 979 | Train loss: 0.03059 | Test loss: 0.07444 | Test accuracy: 0.52577\n",
      "Batch 0, Loss: 0.0008\n",
      "Batch 10, Loss: 0.0163\n",
      "Batch 20, Loss: 0.0158\n",
      "Batch 30, Loss: 0.0011\n",
      "Batch 40, Loss: 0.0017\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 60, Loss: 0.0258\n",
      "Batch 70, Loss: 0.2006\n",
      "Batch 80, Loss: 0.0575\n",
      "Batch 90, Loss: 0.0019\n",
      "Epoch: 980 | Train loss: 0.06978 | Test loss: 0.04858 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0045\n",
      "Batch 10, Loss: 0.0216\n",
      "Batch 20, Loss: 0.0525\n",
      "Batch 30, Loss: 0.0015\n",
      "Batch 40, Loss: 0.0005\n",
      "Batch 50, Loss: 0.0037\n",
      "Batch 60, Loss: 0.0397\n",
      "Batch 70, Loss: 0.0023\n",
      "Batch 80, Loss: 0.0172\n",
      "Batch 90, Loss: 0.0301\n",
      "Epoch: 981 | Train loss: 0.02907 | Test loss: 0.07242 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0797\n",
      "Batch 10, Loss: 0.0027\n",
      "Batch 20, Loss: 0.0076\n",
      "Batch 30, Loss: 0.0112\n",
      "Batch 40, Loss: 0.0100\n",
      "Batch 50, Loss: 0.0256\n",
      "Batch 60, Loss: 0.0064\n",
      "Batch 70, Loss: 0.0758\n",
      "Batch 80, Loss: 0.1173\n",
      "Batch 90, Loss: 0.0005\n",
      "Epoch: 982 | Train loss: 0.03376 | Test loss: 0.04925 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0011\n",
      "Batch 10, Loss: 0.0096\n",
      "Batch 20, Loss: 0.0089\n",
      "Batch 30, Loss: 0.0019\n",
      "Batch 40, Loss: 0.0991\n",
      "Batch 50, Loss: 0.0189\n",
      "Batch 60, Loss: 0.0095\n",
      "Batch 70, Loss: 0.0684\n",
      "Batch 80, Loss: 0.0021\n",
      "Batch 90, Loss: 0.0072\n",
      "Epoch: 983 | Train loss: 0.02239 | Test loss: 0.08427 | Test accuracy: 0.69330\n",
      "Batch 0, Loss: 0.0042\n",
      "Batch 10, Loss: 0.0210\n",
      "Batch 20, Loss: 0.0689\n",
      "Batch 30, Loss: 0.0147\n",
      "Batch 40, Loss: 0.0029\n",
      "Batch 50, Loss: 0.0014\n",
      "Batch 60, Loss: 0.0059\n",
      "Batch 70, Loss: 0.0101\n",
      "Batch 80, Loss: 0.0044\n",
      "Batch 90, Loss: 0.0277\n",
      "Epoch: 984 | Train loss: 0.04503 | Test loss: 0.35861 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0152\n",
      "Batch 10, Loss: 0.0051\n",
      "Batch 20, Loss: 0.0267\n",
      "Batch 30, Loss: 0.0474\n",
      "Batch 40, Loss: 0.0245\n",
      "Batch 50, Loss: 0.2237\n",
      "Batch 60, Loss: 0.0402\n",
      "Batch 70, Loss: 0.3772\n",
      "Batch 80, Loss: 0.1219\n",
      "Batch 90, Loss: 0.0194\n",
      "Epoch: 985 | Train loss: 0.08050 | Test loss: 0.05266 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.1657\n",
      "Batch 10, Loss: 0.0357\n",
      "Batch 20, Loss: 0.0388\n",
      "Batch 30, Loss: 0.0093\n",
      "Batch 40, Loss: 0.0696\n",
      "Batch 50, Loss: 0.0016\n",
      "Batch 60, Loss: 0.0281\n",
      "Batch 70, Loss: 0.0366\n",
      "Batch 80, Loss: 0.0342\n",
      "Batch 90, Loss: 0.0007\n",
      "Epoch: 986 | Train loss: 0.04927 | Test loss: 3.34156 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0156\n",
      "Batch 10, Loss: 0.0104\n",
      "Batch 20, Loss: 0.0142\n",
      "Batch 30, Loss: 0.0745\n",
      "Batch 40, Loss: 0.0104\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0077\n",
      "Batch 70, Loss: 0.0048\n",
      "Batch 80, Loss: 0.0836\n",
      "Batch 90, Loss: 0.0948\n",
      "Epoch: 987 | Train loss: 0.06506 | Test loss: 2.82317 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0285\n",
      "Batch 10, Loss: 0.1319\n",
      "Batch 20, Loss: 0.1257\n",
      "Batch 30, Loss: 0.0114\n",
      "Batch 40, Loss: 0.0131\n",
      "Batch 50, Loss: 0.0390\n",
      "Batch 60, Loss: 0.0714\n",
      "Batch 70, Loss: 0.1590\n",
      "Batch 80, Loss: 0.0014\n",
      "Batch 90, Loss: 0.0159\n",
      "Epoch: 988 | Train loss: 0.05064 | Test loss: 0.04663 | Test accuracy: 0.47423\n",
      "Batch 0, Loss: 0.0018\n",
      "Batch 10, Loss: 0.0024\n",
      "Batch 20, Loss: 0.0643\n",
      "Batch 30, Loss: 0.0005\n",
      "Batch 40, Loss: 0.0116\n",
      "Batch 50, Loss: 0.0012\n",
      "Batch 60, Loss: 0.0024\n",
      "Batch 70, Loss: 0.0003\n",
      "Batch 80, Loss: 0.0082\n",
      "Batch 90, Loss: 0.0071\n",
      "Epoch: 989 | Train loss: 0.02708 | Test loss: 0.04720 | Test accuracy: 0.54124\n",
      "Batch 0, Loss: 0.0209\n",
      "Batch 10, Loss: 0.0047\n",
      "Batch 20, Loss: 0.0073\n",
      "Batch 30, Loss: 0.0258\n",
      "Batch 40, Loss: 0.0225\n",
      "Batch 50, Loss: 0.0076\n",
      "Batch 60, Loss: 0.0127\n",
      "Batch 70, Loss: 0.1217\n",
      "Batch 80, Loss: 0.2923\n",
      "Batch 90, Loss: 0.0457\n",
      "Epoch: 990 | Train loss: 0.06216 | Test loss: 0.33310 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0137\n",
      "Batch 10, Loss: 0.0038\n",
      "Batch 20, Loss: 0.0036\n",
      "Batch 30, Loss: 0.0146\n",
      "Batch 40, Loss: 0.0711\n",
      "Batch 50, Loss: 0.0286\n",
      "Batch 60, Loss: 0.0125\n",
      "Batch 70, Loss: 0.1386\n",
      "Batch 80, Loss: 0.0147\n",
      "Batch 90, Loss: 0.0049\n",
      "Epoch: 991 | Train loss: 0.05025 | Test loss: 0.05099 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0012\n",
      "Batch 10, Loss: 0.0023\n",
      "Batch 20, Loss: 0.0015\n",
      "Batch 30, Loss: 0.0070\n",
      "Batch 40, Loss: 0.0043\n",
      "Batch 50, Loss: 0.0056\n",
      "Batch 60, Loss: 0.0493\n",
      "Batch 70, Loss: 0.0087\n",
      "Batch 80, Loss: 0.0168\n",
      "Batch 90, Loss: 0.0071\n",
      "Epoch: 992 | Train loss: 0.04082 | Test loss: 0.05123 | Test accuracy: 0.48711\n",
      "Batch 0, Loss: 0.0037\n",
      "Batch 10, Loss: 0.0816\n",
      "Batch 20, Loss: 0.0060\n",
      "Batch 30, Loss: 0.0056\n",
      "Batch 40, Loss: 0.0049\n",
      "Batch 50, Loss: 0.0432\n",
      "Batch 60, Loss: 0.0858\n",
      "Batch 70, Loss: 0.0332\n",
      "Batch 80, Loss: 0.0251\n",
      "Batch 90, Loss: 0.0029\n",
      "Epoch: 993 | Train loss: 0.03039 | Test loss: 0.04293 | Test accuracy: 0.64691\n",
      "Batch 0, Loss: 0.0108\n",
      "Batch 10, Loss: 0.1266\n",
      "Batch 20, Loss: 0.0136\n",
      "Batch 30, Loss: 0.0191\n",
      "Batch 40, Loss: 0.0072\n",
      "Batch 50, Loss: 0.0006\n",
      "Batch 60, Loss: 0.0575\n",
      "Batch 70, Loss: 0.0713\n",
      "Batch 80, Loss: 0.0085\n",
      "Batch 90, Loss: 0.0034\n",
      "Epoch: 994 | Train loss: 0.03608 | Test loss: 0.10963 | Test accuracy: 0.55670\n",
      "Batch 0, Loss: 0.0008\n",
      "Batch 10, Loss: 0.0259\n",
      "Batch 20, Loss: 0.0020\n",
      "Batch 30, Loss: 0.0072\n",
      "Batch 40, Loss: 0.0247\n",
      "Batch 50, Loss: 0.0039\n",
      "Batch 60, Loss: 0.1028\n",
      "Batch 70, Loss: 0.1330\n",
      "Batch 80, Loss: 0.0046\n",
      "Batch 90, Loss: 0.0109\n",
      "Epoch: 995 | Train loss: 0.04049 | Test loss: 0.10305 | Test accuracy: 0.48454\n",
      "Batch 0, Loss: 0.0016\n",
      "Batch 10, Loss: 0.0612\n",
      "Batch 20, Loss: 0.0038\n",
      "Batch 30, Loss: 0.0419\n",
      "Batch 40, Loss: 0.0028\n",
      "Batch 50, Loss: 0.0295\n",
      "Batch 60, Loss: 0.0717\n",
      "Batch 70, Loss: 0.0014\n",
      "Batch 80, Loss: 0.0656\n",
      "Batch 90, Loss: 0.0248\n",
      "Epoch: 996 | Train loss: 0.08190 | Test loss: 0.92060 | Test accuracy: 0.51804\n",
      "Batch 0, Loss: 0.0023\n",
      "Batch 10, Loss: 0.0088\n",
      "Batch 20, Loss: 0.0149\n",
      "Batch 30, Loss: 0.1539\n",
      "Batch 40, Loss: 0.0411\n",
      "Batch 50, Loss: 0.0044\n",
      "Batch 60, Loss: 0.0009\n",
      "Batch 70, Loss: 0.1218\n",
      "Batch 80, Loss: 0.0086\n",
      "Batch 90, Loss: 0.0068\n",
      "Epoch: 997 | Train loss: 0.03859 | Test loss: 0.07571 | Test accuracy: 0.52320\n",
      "Batch 0, Loss: 0.0395\n",
      "Batch 10, Loss: 0.0242\n",
      "Batch 20, Loss: 0.0005\n",
      "Batch 30, Loss: 0.0011\n",
      "Batch 40, Loss: 0.0096\n",
      "Batch 50, Loss: 0.0066\n",
      "Batch 60, Loss: 0.0045\n",
      "Batch 70, Loss: 0.0334\n",
      "Batch 80, Loss: 0.0008\n",
      "Batch 90, Loss: 0.0070\n",
      "Epoch: 998 | Train loss: 0.01841 | Test loss: 0.04672 | Test accuracy: 0.46649\n",
      "Batch 0, Loss: 0.0310\n",
      "Batch 10, Loss: 0.0420\n",
      "Batch 20, Loss: 0.0083\n",
      "Batch 30, Loss: 0.0344\n",
      "Batch 40, Loss: 0.0149\n",
      "Batch 50, Loss: 0.0746\n",
      "Batch 60, Loss: 0.0088\n",
      "Batch 70, Loss: 0.0071\n",
      "Batch 80, Loss: 0.0011\n",
      "Batch 90, Loss: 0.0302\n",
      "Epoch: 999 | Train loss: 0.03590 | Test loss: 3.91160 | Test accuracy: 0.48196\n",
      "Batch 0, Loss: 0.0057\n",
      "Batch 10, Loss: 0.0772\n",
      "Batch 20, Loss: 0.0174\n",
      "Batch 30, Loss: 0.0064\n",
      "Batch 40, Loss: 0.0057\n",
      "Batch 50, Loss: 0.0245\n",
      "Batch 60, Loss: 0.0051\n",
      "Batch 70, Loss: 0.0321\n",
      "Batch 80, Loss: 0.0173\n",
      "Batch 90, Loss: 0.0060\n",
      "Epoch: 1000 | Train loss: 0.06064 | Test loss: 0.06733 | Test accuracy: 0.52062\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=66):\n",
    "        super(EEGClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(100)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 100, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)#New shape: (batch_size, channels, time_points)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EEGClassifier(input_channels=66).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "train_loss_results,accuracy_results, loss_results = train_model(model=model,train_dataloader=train_loader,test_dataloader=test_loader,optimizer=optimizer,criterion=criterion,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "proj_folder=Path(\"E:\\\\Penn State Homework\\\\Homework\\\\CMPSC\\\\CMPSC 445 Proj\\\\NNModel.pth\")\n",
    "torch.save(model.state_dict(), proj_folder)\n",
    "best_model_state = deepcopy(model.state_dict())\n",
    "torch.save(model.state_dict(), Path(\"E:\\\\Penn State Homework\\\\Homework\\\\CMPSC\\\\CMPSC 445 Proj\\\\DeepCopyNNModel.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs=[tlr[0] for tlr in train_loss_results]\n",
    "train_loss=[tlr[1] for tlr in train_loss_results]\n",
    "test_accuracy=[acc[1] for acc in accuracy_results]\n",
    "test_loss=[tlr[1] for tlr in loss_results]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "#Train and Test Loss\n",
    "ax1.plot(epochs, train_loss, label='Train Loss', color='b', marker='o')\n",
    "ax1.plot(epochs, test_loss, label='Test Loss', color='r', marker='x')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "#Accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, test_accuracy, label='Test Accuracy', color='g', marker='s')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "#Plot\n",
    "plt.title('Train Loss, Test Loss, and Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics._scorer import _SCORERS\n",
    "import joblib\n",
    "def preprocess_data(X_train, X_test):\n",
    "    #Flatten the data\n",
    "    # X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    # X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    #Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    # X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    # X_test_scaled = scaler.transform(X_test_flat)\n",
    "    X_train_scaled=scaler.fit_transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    joblib.dump(scaler, 'scaler.gz')\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    return lr_model\n",
    "\n",
    "def train_logistic_regression_with_cv(X, y, cv_folds=5):\n",
    "    lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        lr_model, X, y, cv=cv, scoring=['accuracy', 'precision','f1','recall'],\n",
    "        return_train_score=True, return_estimator=True\n",
    "    )\n",
    "    # scores = cross_validate(\n",
    "    #     lr_model, X, y, cv=cv, scoring=_SCORERS,\n",
    "    #     return_train_score=True, return_estimator=True\n",
    "    # )\n",
    "    # print(\"Cross-Validation Results:\")\n",
    "    # for key in scores.keys():\n",
    "    #     print(key,\":\",scores[key])\n",
    "    #Return the model with the best performance on validation data\n",
    "    best_model_index = np.argmax(scores['test_accuracy'])\n",
    "    best_model = scores['estimator'][best_model_index]\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "label_map = {'0': \"Non-Parkinson's\", '1': \"Parkinson's\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_classification_report(y_true,y_pred,beautiful=True):\n",
    "    if (not beautiful):\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    else:\n",
    "        report=classification_report(y_true,y_pred,output_dict=True)\n",
    "        \n",
    "        report_df = pd.DataFrame(report)\n",
    "        report_labels=list(report_df.columns)\n",
    "        report_df=report_df.T\n",
    "        \n",
    "        report_labels[0]=\"Non-Parkinson's\"\n",
    "        report_labels[1]=\"Parkinson's\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(report_df, annot=True, cmap='YlGnBu', fmt='.2f', cbar=True,yticklabels=report_labels)  # Exclude 'accuracy' row and support column if undesired\n",
    "        plt.title('Classification Report Heatmap')\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Classes')\n",
    "        plt.show()\n",
    "        \n",
    "def display_confusion_matrix(y_true,y_pred,beautiful=True):\n",
    "    if (not beautiful):\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "    else:\n",
    "        matrix=confusion_matrix(y_true,y_pred)\n",
    "        \n",
    "        matrix_df = pd.DataFrame(matrix)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(matrix_df, annot=True, cmap='YlGnBu', fmt='.2f', cbar=True,xticklabels=[\"Parkinson's\",\"Non-Parkinson's\"],yticklabels=[\"Parkinson's\",\"Non-Parkinson's\"])  # Exclude 'accuracy' row and support column if undesired\n",
    "        plt.title('Confusion Heatmap')\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Classes')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression with cv...\n",
      "\n",
      "Logistic Regression with cv Results:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_comparison(X_train, X_test, y_train, y_test):\n",
    "    #Logistic Regression\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)\n",
    "    lr_model = train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    y_pred = lr_model.predict(X_test_scaled)\n",
    "    print(\"\\nLogistic Regression Results:\")\n",
    "    display_classification_report(y_true=y_test,y_pred=y_pred,beautiful=False)\n",
    "    display_confusion_matrix(y_true=y_test,y_pred=y_pred,beautiful=False)\n",
    "    \n",
    "    #Logistic Regression with cv\n",
    "    print(\"Training Logistic Regression with cv...\")\n",
    "    X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "    y_combined = np.hstack((y_train, y_test))\n",
    "    best_model= train_logistic_regression_with_cv(X_combined, y_combined, cv_folds=20)\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    print(\"\\nLogistic Regression with cv Results:\")\n",
    "    display_classification_report(y_true=y_test,y_pred=y_pred,beautiful=False)\n",
    "    display_confusion_matrix(y_true=y_test,y_pred=y_pred,beautiful=False)\n",
    "    return lr_model, best_model\n",
    "\n",
    "# lr_model, best_model = run_comparison(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# print(\"Training Logistic Regression...\")\n",
    "# X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)\n",
    "# lr_model = train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "# y_pred = lr_model.predict(X_test_scaled)\n",
    "# print(\"\\nLogistic Regression Results:\")\n",
    "# display_classification_report(y_true=y_test,y_pred=y_pred)\n",
    "# display_confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
    "\n",
    "print(\"Training Logistic Regression with cv...\")\n",
    "X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "best_model= train_logistic_regression_with_cv(X_combined, y_combined, cv_folds=20)\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(\"\\nLogistic Regression with cv Results:\")\n",
    "display_classification_report(y_true=y_test,y_pred=y_pred)\n",
    "display_confusion_matrix(y_true=y_test,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_cv.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "filename = 'lr_cv.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "Prediction: Non-Parkinson's\n",
      "Confidence: 54.25%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Similar to the logic inside the for-loop used for creating the dataset from all the .set files\n",
    "'''\n",
    "def process_single_file(file_path, window_size=2500):\n",
    "\n",
    "    #Preprocessing steps\n",
    "    raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "    raw.set_eeg_reference('average', projection=True)\n",
    "    raw.resample(TARGET_SAMPLING_RATE, npad=\"auto\")\n",
    "    raw.filter(1., 30., fir_design='firwin', n_jobs='cuda')\n",
    "    \n",
    "    ica = mne.preprocessing.ICA(n_components=15, random_state=22, max_iter=1000, method='picard')\n",
    "    ica.fit(raw)\n",
    "    raw = ica.apply(raw)\n",
    "    \n",
    "    data = raw.get_data().T\n",
    "    \n",
    "    #Make multiple windows\n",
    "    windows = []\n",
    "    stride = window_size // 2  #50% overlap\n",
    "    for start in range(0, data.shape[0] - window_size + 1, stride):\n",
    "        window = data[start:start + window_size, :]\n",
    "        windows.append(window)\n",
    "    \n",
    "    return np.stack(windows)\n",
    "\n",
    "#Predict a result for the input file\n",
    "def predict_file(file_path, model, scaler, window_size=2500):\n",
    "    windows = process_single_file(file_path, window_size)\n",
    "    X = windows.reshape(windows.shape[0], -1)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    #Predictions for all windows\n",
    "    predictions = model.predict(X_scaled)\n",
    "    probabilities = model.predict_proba(X_scaled)\n",
    "    \n",
    "    #Majority voting\n",
    "    final_prediction = np.bincount(predictions).argmax()\n",
    "    final_probability = np.mean(probabilities, axis=0)\n",
    "    #file_prediction_prob_avg = np.argmax(final_probability)\n",
    "    return final_prediction, final_probability\n",
    "\n",
    "\n",
    "lr_model = joblib.load(filename)\n",
    "# print(lr_model.n_features_in_)\n",
    "file_path = \"fresh data\\\\16_eyesOpen.set\"\n",
    "scaler=joblib.load('scaler.gz')\n",
    "prediction, probability = predict_file(file_path, best_model, scaler, window_size=window_size)\n",
    "prediction_label=\"Parkinson's\" if prediction == 1 else \"Non-Parkinson's\"\n",
    "print(\"Prediction:\",prediction_label)\n",
    "print(\"Confidence:\",max(probability)*100,\"%\")\n",
    "# print(\"Prob Avg:\",prob_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "display_classification_report(y_true=y_test,y_pred=rf_pred)\n",
    "display_confusion_matrix(y_true=y_test,y_pred=rf_pred)\n",
    "# # SVM\n",
    "# svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
    "# svm_model.fit(X_train_scaled, y_train)\n",
    "# svm_pred = svm_model.predict(X_test_scaled)\n",
    "# print(\"\\nSVM Results:\")\n",
    "# print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Results:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "print(\"\\nSVM Results:\")\n",
    "display_classification_report(y_true=y_test,y_pred=svm_pred)\n",
    "display_confusion_matrix(y_true=y_test,y_pred=svm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rfc.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'rfc.joblib'\n",
    "joblib.dump(rf_model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='svm.joblib'\n",
    "joblib.dump(svm_model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[156  43]\n",
      " [130  59]]\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Using CUDA for FFT FIR filtering\n",
      "Fitting ICA to data using 66 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 0.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (15 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 66 PCA components\n",
      "Prediction: Non-Parkinson's\n",
      "Confidence: 51.37%\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, svm_pred))\n",
    "file_path = \"fresh data\\\\32_eyesOpen.set\"\n",
    "scaler=joblib.load('scaler.gz')\n",
    "prediction, probability = predict_file(file_path, svm_model, scaler, window_size=window_size)\n",
    "prediction_label=\"Parkinson's\" if prediction == 1 else \"Non-Parkinson's\"\n",
    "print(\"Prediction:\",prediction_label)\n",
    "print(\"Confidence:\",max(probability)*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
